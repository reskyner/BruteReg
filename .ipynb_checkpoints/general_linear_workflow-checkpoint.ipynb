{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%reset\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "sys.path.append('./modules')\n",
    "\n",
    "import warnings\n",
    "#import re\n",
    "\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "## import modules to build pipelines\n",
    "import pipemodules as pm\n",
    "import projecthandle as proj\n",
    "import run_grid as rg\n",
    "\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read csv file containing descriptors\n",
    "\n",
    "def set_input(filename):\n",
    "    \"\"\"set_input('filename') \n",
    "    \n",
    "    Import a csv file of the format:\n",
    "    1. first row - should be names of labels, property, then n descriptors:\n",
    "       labels, y, X1-name....Xn-name\n",
    "    \n",
    "    2. following rows - values corresponding to column headers:\n",
    "       label1, y1, X1.....Xn\"\"\"\n",
    "    \n",
    "    descriptors_raw = pd.read_csv(filename)\n",
    "    labels = descriptors_raw.iloc[:,0]\n",
    "    X = descriptors_raw.iloc[:,2:]\n",
    "    y = descriptors_raw.iloc[:,1]\n",
    "\n",
    "    del descriptors_raw\n",
    "    return X, y, labels\n",
    "\n",
    "X, y, labels = set_input('./input_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.094e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.470e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.718e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.660e-03, previous alpha=1.599e-03, with an active set of 28 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.086e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.429e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.522e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=1.441e-03, previous alpha=1.111e-03, with an active set of 22 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.470e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.718e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=2.026e-03, previous alpha=1.933e-03, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.091e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.456e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.455e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.641e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=6.948e-04, previous alpha=6.913e-04, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.116e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.580e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.139e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.009e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.712e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.712e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.527e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.294e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 49 iterations, alpha=1.350e-03, previous alpha=1.203e-03, with an active set of 36 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.089e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.444e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=6.163e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=3.446e-04, previous alpha=3.432e-04, with an active set of 37 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 95 iterations, alpha=3.059e-03, previous alpha=2.318e-04, with an active set of 60 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.674e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.654e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.189e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.731e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=1.324e-03, previous alpha=1.323e-03, with an active set of 43 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=7.644e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.487e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.814e-04, previous alpha=6.677e-04, with an active set of 53 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.445e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.636e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.068e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.676e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.330e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 55 iterations, alpha=3.836e-04, previous alpha=3.584e-04, with an active set of 40 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.092e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.546e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.593e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=4.784e-04, previous alpha=4.752e-04, with an active set of 33 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.841e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.410e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.459e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.452e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=1.829e-03, previous alpha=1.636e-03, with an active set of 26 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 84 iterations, alpha=1.220e-03, previous alpha=1.459e-04, with an active set of 55 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 85 iterations, alpha=3.693e-03, previous alpha=7.821e-05, with an active set of 58 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=4.510e-03, previous alpha=8.756e-05, with an active set of 60 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:43: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/coordinate_descent.py:470: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.415e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.493e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=2.003e-03, previous alpha=1.985e-03, with an active set of 31 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.949e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.465e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.458e-03, previous alpha=2.317e-03, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.712e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=9.045e-03, previous alpha=3.818e-03, with an active set of 54 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.415e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.493e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=2.000e-03, previous alpha=1.928e-03, with an active set of 28 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.465e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.325e-03, previous alpha=2.188e-03, with an active set of 27 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.712e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=6.037e-03, previous alpha=2.879e-03, with an active set of 51 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=1.179e-02, previous alpha=8.339e-05, with an active set of 52 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 83 iterations, alpha=3.121e-03, previous alpha=6.087e-05, with an active set of 56 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=5.762e-05, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=3.130e-02, previous alpha=5.266e-05, with an active set of 58 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=7.343e-05, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=3.843e-03, previous alpha=5.981e-05, with an active set of 52 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.042e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.281e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=3.831e-04, previous alpha=3.564e-04, with an active set of 56 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=7.743e-03, previous alpha=1.106e-04, with an active set of 55 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.505e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.461e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=9.918e-04, previous alpha=9.693e-04, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.311e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.150e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=1.070e-03, previous alpha=1.021e-03, with an active set of 43 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.305e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.116e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=6.066e-03, previous alpha=6.008e-03, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=9.261e-04, previous alpha=1.206e-04, with an active set of 61 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 76 iterations, alpha=2.338e-03, previous alpha=1.425e-04, with an active set of 49 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.475e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.421e-04, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.816e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.698e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=1.707e-04, previous alpha=1.566e-04, with an active set of 56 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 71 iterations, alpha=5.404e+00, previous alpha=1.082e-02, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=3.443e+00, previous alpha=4.276e-03, with an active set of 44 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.631e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.411e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.411e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.031e-03, previous alpha=2.003e-03, with an active set of 27 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 78 iterations, alpha=7.333e+01, previous alpha=2.137e-02, with an active set of 41 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 69 iterations, alpha=1.577e+00, previous alpha=1.990e-02, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 72 iterations, alpha=2.591e+00, previous alpha=5.016e-03, with an active set of 39 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.166e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.449e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.380e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 63 iterations, alpha=2.223e-02, previous alpha=2.200e-02, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=4.657e-02, previous alpha=4.296e-02, with an active set of 36 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=3.226e-02, previous alpha=1.807e-02, with an active set of 43 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 71 iterations, alpha=1.493e-02, previous alpha=1.318e-02, with an active set of 44 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.596e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 74 iterations, alpha=1.974e-02, previous alpha=1.757e-02, with an active set of 43 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.826e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 50 iterations, alpha=9.492e-02, previous alpha=8.713e-02, with an active set of 29 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.374e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=6.401e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=3.253e+01, previous alpha=6.401e-03, with an active set of 50 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.219e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 93 iterations, alpha=3.506e-01, previous alpha=8.354e-03, with an active set of 52 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=5.505e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 78 iterations, alpha=2.431e-02, previous alpha=2.160e-02, with an active set of 43 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=2.341e+01, previous alpha=1.340e-02, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 65 iterations, alpha=7.198e+00, previous alpha=7.814e-03, with an active set of 36 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=6.344e-01, previous alpha=6.344e-01, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=3.517e+00, previous alpha=1.049e-02, with an active set of 31 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.384e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.192e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 44 iterations, alpha=1.261e-01, previous alpha=1.113e-01, with an active set of 25 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pipemodules as pm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## filter results and eliminate poor models\n",
    "for i in range(0,len(results)):\n",
    "    if results.mean_train_score[i] > 0.75 and abs(results.mean_test_score[i] - results.mean_train_score[i]) < 0.15:\n",
    "        continue\n",
    "    else: \n",
    "        results.drop(i, axis=0, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## create analysis set\n",
    "# set arrays for results\n",
    "dev_set_score = []\n",
    "eval_set_score = []\n",
    "dev_evs = []\n",
    "eval_evs = []\n",
    "dev_mae = []\n",
    "eval_mae = []\n",
    "dev_mse = []\n",
    "eval_mse = []\n",
    "dev_medae = []\n",
    "eval_medae = []\n",
    "method_ids = []\n",
    "parameters = []\n",
    "\n",
    "\n",
    "for i in range(0,len(results)):\n",
    "    ## take method_ids and build estimator for current method\n",
    "    string = results.method_ids[i] # retrive method id\n",
    "    setup = eval(string) # convert to iterable array\n",
    "\n",
    "    temp = pm.search_random_forest() #initiate class\n",
    "\n",
    "    # set the estimator type and initiate estimator class\n",
    "    _,clf,_ = temp.set_method(setup[2]) \n",
    "    \n",
    "    # get the development set features\n",
    "    X_dev_temp, _ = pm.get_X(dev_set.matrix_raw, \\\n",
    "                             meth.indvals[setup[0]][setup[1]]) \n",
    "    # get the evaluation set features\n",
    "    X_eval_temp, _ = pm.get_X(eval_set.matrix_raw, \\\n",
    "                              meth.indvals[setup[0]][setup[1]]) \n",
    "\n",
    "    del temp\n",
    "\n",
    "    # retreive hyper-parameters\n",
    "    params = results['params'][i]\n",
    "    # set estimator hyper-parameters\n",
    "    clf.set_params(**params)\n",
    "    \n",
    "    # fit the estimator to the development set\n",
    "    clf.fit(X_dev_temp, dev_set.y_raw)\n",
    "    # predict the evaluation set\n",
    "    eval_predict = clf.predict(X_eval_temp)\n",
    "    # predict the development set - for metrics\n",
    "    dev_predict = clf.predict(X_dev_temp)\n",
    "    \n",
    "    # add calculated metrics, methods, and parameters to lists for results\n",
    "    dev_set_score.append(clf.score(X_dev_temp, dev_set.y_raw))\n",
    "    eval_set_score.append(clf.score(X_eval_temp, eval_set.y_raw))\n",
    "    dev_evs.append(metrics.explained_variance_score(dev_predict, dev_set.y_raw))\n",
    "    eval_evs.append(metrics.explained_variance_score(eval_predict, eval_set.y_raw))\n",
    "    dev_mae.append(metrics.mean_absolute_error(dev_predict, dev_set.y_raw))\n",
    "    eval_mae.append(metrics.mean_absolute_error(eval_predict, eval_set.y_raw))\n",
    "    dev_mse.append(metrics.mean_squared_error(dev_predict, dev_set.y_raw))\n",
    "    eval_mse.append(metrics.mean_squared_error(eval_predict, eval_set.y_raw))\n",
    "    dev_medae.append(metrics.median_absolute_error(dev_predict, dev_set.y_raw))\n",
    "    eval_medae.append(metrics.median_absolute_error(eval_predict, eval_set.y_raw))\n",
    "    method_ids.append(string)\n",
    "    parameters.append(params)\n",
    "    \n",
    "# create dictionary object from results\n",
    "evaluation_results = {'dev_set_score':dev_set_score, 'eval_set_score':eval_set_score, \\\n",
    "                     'method_ids':method_ids, 'parameters':parameters, 'dev_evs':dev_evs, \\\n",
    "                     'eval_evs':eval_evs, 'dev_mae':dev_mae, 'eval_mae':eval_mae, \\\n",
    "                     'dev_mse': dev_mse, 'eval_mse':eval_mse, 'dev_median_ae':dev_medae, \\\n",
    "                     'eval_median_ae':eval_medae}\n",
    "\n",
    "# re-rank and sort filtered methods by test-score (r**2)\n",
    "analysis_set = pd.DataFrame(evaluation_results)\n",
    "array = np.array(analysis_set['eval_set_score'])\n",
    "temp = array.argsort()[::-1]\n",
    "ranks = np.empty(len(array), int)\n",
    "ranks[temp] = np.arange(len(array))\n",
    "analysis_set['rank_test_score'] = ranks\n",
    "analysis_set.sort_values(by='rank_test_score', inplace=True)\n",
    "analysis_set.reset_index(drop=True, inplace=True)\n",
    "analysis_set.to_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.424076            0.999974      0.000405   \n",
      "1             0.99803           0.406243            0.999974      0.002746   \n",
      "2             0.99803           0.397930            0.999974      0.001071   \n",
      "3             0.99803           0.420547            0.999974      0.000255   \n",
      "4             0.99803           0.345349            0.999974      0.001128   \n",
      "5             0.99803           0.405106            0.999974      0.004279   \n",
      "6             0.99803           0.407175            0.999974      0.022929   \n",
      "7             0.99803           0.376038            0.999974      0.028923   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.022064         0.005022         0.429469          0.999327   \n",
      "1       0.057824         0.011764         0.477087          0.999327   \n",
      "2       0.089325         0.019156         0.447721          0.999327   \n",
      "3       0.121582         0.025687         0.465677          0.999327   \n",
      "4       0.050461         0.005172         0.417820          0.999327   \n",
      "5       0.129415         0.011766         0.455314          0.999327   \n",
      "6       0.204490         0.018932         0.445757          0.999327   \n",
      "7       0.280706         0.025735         0.440078          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 1, 2]             mse                 10   \n",
      "1  [0, 1, 2]             mse                 25   \n",
      "2  [0, 1, 2]             mse                 40   \n",
      "3  [0, 1, 2]             mse                 55   \n",
      "4  [0, 1, 2]             mae                 10   \n",
      "5  [0, 1, 2]             mae                 25   \n",
      "6  [0, 1, 2]             mae                 40   \n",
      "7  [0, 1, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.531158            0.999978           0.408375   \n",
      "1           0.503439            0.999978           0.525550   \n",
      "2           0.495257            0.999978           0.473424   \n",
      "3           0.532163            0.999978           0.492380   \n",
      "4           0.479440            0.999978           0.423900   \n",
      "5           0.514665            0.999978           0.466658   \n",
      "6           0.511391            0.999978           0.470341   \n",
      "7           0.503981            0.999978           0.439359   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.347781            0.999974      0.000520   \n",
      "1             0.99803           0.401990            0.999974      0.005516   \n",
      "2             0.99803           0.373970            0.999974      0.002415   \n",
      "3             0.99803           0.371773            0.999974      0.002624   \n",
      "4             0.99803           0.349457            0.999974      0.001997   \n",
      "5             0.99803           0.383980            0.999974      0.000475   \n",
      "6             0.99803           0.354833            0.999974      0.004821   \n",
      "7             0.99803           0.376207            0.999974      0.005013   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000010        0.076391         0.000917  \n",
      "1        0.000275        0.053725         0.000917  \n",
      "2        0.000319        0.052769         0.000917  \n",
      "3        0.000374        0.068188         0.000917  \n",
      "4        0.000168        0.053285         0.000917  \n",
      "5        0.000100        0.053995         0.000917  \n",
      "6        0.000459        0.066278         0.000917  \n",
      "7        0.000165        0.052212         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "0       0.022064         0.005022         0.429469          0.999327   \n",
      "1       0.057824         0.011764         0.477087          0.999327   \n",
      "2       0.089325         0.019156         0.447721          0.999327   \n",
      "3       0.121582         0.025687         0.465677          0.999327   \n",
      "4       0.050461         0.005172         0.417820          0.999327   \n",
      "5       0.129415         0.011766         0.455314          0.999327   \n",
      "6       0.204490         0.018932         0.445757          0.999327   \n",
      "7       0.280706         0.025735         0.440078          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "0  [0, 1, 2]             mse                 10   \n",
      "1  [0, 1, 2]             mse                 25   \n",
      "2  [0, 1, 2]             mse                 40   \n",
      "3  [0, 1, 2]             mse                 55   \n",
      "4  [0, 1, 2]             mae                 10   \n",
      "5  [0, 1, 2]             mae                 25   \n",
      "6  [0, 1, 2]             mae                 40   \n",
      "7  [0, 1, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "0           0.531158            0.999978           0.408375   \n",
      "1           0.503439            0.999978           0.525550   \n",
      "2           0.495257            0.999978           0.473424   \n",
      "3           0.532163            0.999978           0.492380   \n",
      "4           0.479440            0.999978           0.423900   \n",
      "5           0.514665            0.999978           0.466658   \n",
      "6           0.511391            0.999978           0.470341   \n",
      "7           0.503981            0.999978           0.439359   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "0            0.998030           0.347781            0.999974      0.000520   \n",
      "1            0.998030           0.401990            0.999974      0.005516   \n",
      "2            0.998030           0.373970            0.999974      0.002415   \n",
      "3            0.998030           0.371773            0.999974      0.002624   \n",
      "4            0.998030           0.349457            0.999974      0.001997   \n",
      "5            0.998030           0.383980            0.999974      0.000475   \n",
      "6            0.998030           0.354833            0.999974      0.004821   \n",
      "7            0.998030           0.376207            0.999974      0.005013   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "0        0.000010        0.076391         0.000917  \n",
      "1        0.000275        0.053725         0.000917  \n",
      "2        0.000319        0.052769         0.000917  \n",
      "3        0.000374        0.068188         0.000917  \n",
      "4        0.000168        0.053285         0.000917  \n",
      "5        0.000100        0.053995         0.000917  \n",
      "6        0.000459        0.066278         0.000917  \n",
      "7        0.000165        0.052212         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.023749         0.004902         0.377227          0.894722   \n",
      "1       0.061383         0.011770         0.447905          0.912889   \n",
      "2       0.093766         0.018978         0.414147          0.920426   \n",
      "3       0.128777         0.026892         0.423102          0.918386   \n",
      "4       0.049568         0.005000         0.421511          0.883591   \n",
      "5       0.121377         0.012151         0.422526          0.904570   \n",
      "6       0.226937         0.019638         0.434604          0.919032   \n",
      "7       0.268454         0.025964         0.455829          0.913065   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 2, 1]             mse                 10   \n",
      "1  [0, 2, 1]             mse                 25   \n",
      "2  [0, 2, 1]             mse                 40   \n",
      "3  [0, 2, 1]             mse                 55   \n",
      "4  [0, 2, 1]             mae                 10   \n",
      "5  [0, 2, 1]             mae                 25   \n",
      "6  [0, 2, 1]             mae                 40   \n",
      "7  [0, 2, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                7   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.394553            0.908679           0.427265   \n",
      "1           0.506235            0.910769           0.419286   \n",
      "2           0.490336            0.915519           0.356600   \n",
      "3           0.499550            0.915338           0.405926   \n",
      "4           0.530776            0.859296           0.420564   \n",
      "5           0.493540            0.892617           0.427600   \n",
      "6           0.533535            0.910753           0.432182   \n",
      "7           0.520751            0.904628           0.438295   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.880529           0.309677            0.894957      0.000176   \n",
      "1            0.912013           0.417566            0.915883      0.003928   \n",
      "2            0.926442           0.394687            0.919317      0.000166   \n",
      "3            0.923868           0.363007            0.915952      0.000726   \n",
      "4            0.893825           0.312019            0.897651      0.001317   \n",
      "5            0.916280           0.345674            0.904813      0.002863   \n",
      "6            0.924144           0.337031            0.922201      0.030296   \n",
      "7            0.916972           0.407742            0.917597      0.005751   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000139        0.049477         0.011494  \n",
      "1        0.000171        0.041473         0.002178  \n",
      "2        0.000079        0.056342         0.004528  \n",
      "3        0.001883        0.057092         0.003885  \n",
      "4        0.000152        0.089388         0.017250  \n",
      "5        0.000346        0.060525         0.009662  \n",
      "6        0.001403        0.080311         0.005908  \n",
      "7        0.000644        0.047803         0.005972  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "0       0.022064         0.005022         0.429469          0.999327   \n",
      "1       0.057824         0.011764         0.477087          0.999327   \n",
      "2       0.089325         0.019156         0.447721          0.999327   \n",
      "3       0.121582         0.025687         0.465677          0.999327   \n",
      "4       0.050461         0.005172         0.417820          0.999327   \n",
      "5       0.129415         0.011766         0.455314          0.999327   \n",
      "6       0.204490         0.018932         0.445757          0.999327   \n",
      "7       0.280706         0.025735         0.440078          0.999327   \n",
      "0       0.023749         0.004902         0.377227          0.894722   \n",
      "1       0.061383         0.011770         0.447905          0.912889   \n",
      "2       0.093766         0.018978         0.414147          0.920426   \n",
      "3       0.128777         0.026892         0.423102          0.918386   \n",
      "4       0.049568         0.005000         0.421511          0.883591   \n",
      "5       0.121377         0.012151         0.422526          0.904570   \n",
      "6       0.226937         0.019638         0.434604          0.919032   \n",
      "7       0.268454         0.025964         0.455829          0.913065   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "0  [0, 1, 2]             mse                 10   \n",
      "1  [0, 1, 2]             mse                 25   \n",
      "2  [0, 1, 2]             mse                 40   \n",
      "3  [0, 1, 2]             mse                 55   \n",
      "4  [0, 1, 2]             mae                 10   \n",
      "5  [0, 1, 2]             mae                 25   \n",
      "6  [0, 1, 2]             mae                 40   \n",
      "7  [0, 1, 2]             mae                 55   \n",
      "0  [0, 2, 1]             mse                 10   \n",
      "1  [0, 2, 1]             mse                 25   \n",
      "2  [0, 2, 1]             mse                 40   \n",
      "3  [0, 2, 1]             mse                 55   \n",
      "4  [0, 2, 1]             mae                 10   \n",
      "5  [0, 2, 1]             mae                 25   \n",
      "6  [0, 2, 1]             mae                 40   \n",
      "7  [0, 2, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                7   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "0           0.531158            0.999978           0.408375   \n",
      "1           0.503439            0.999978           0.525550   \n",
      "2           0.495257            0.999978           0.473424   \n",
      "3           0.532163            0.999978           0.492380   \n",
      "4           0.479440            0.999978           0.423900   \n",
      "5           0.514665            0.999978           0.466658   \n",
      "6           0.511391            0.999978           0.470341   \n",
      "7           0.503981            0.999978           0.439359   \n",
      "0           0.394553            0.908679           0.427265   \n",
      "1           0.506235            0.910769           0.419286   \n",
      "2           0.490336            0.915519           0.356600   \n",
      "3           0.499550            0.915338           0.405926   \n",
      "4           0.530776            0.859296           0.420564   \n",
      "5           0.493540            0.892617           0.427600   \n",
      "6           0.533535            0.910753           0.432182   \n",
      "7           0.520751            0.904628           0.438295   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "0            0.998030           0.347781            0.999974      0.000520   \n",
      "1            0.998030           0.401990            0.999974      0.005516   \n",
      "2            0.998030           0.373970            0.999974      0.002415   \n",
      "3            0.998030           0.371773            0.999974      0.002624   \n",
      "4            0.998030           0.349457            0.999974      0.001997   \n",
      "5            0.998030           0.383980            0.999974      0.000475   \n",
      "6            0.998030           0.354833            0.999974      0.004821   \n",
      "7            0.998030           0.376207            0.999974      0.005013   \n",
      "0            0.880529           0.309677            0.894957      0.000176   \n",
      "1            0.912013           0.417566            0.915883      0.003928   \n",
      "2            0.926442           0.394687            0.919317      0.000166   \n",
      "3            0.923868           0.363007            0.915952      0.000726   \n",
      "4            0.893825           0.312019            0.897651      0.001317   \n",
      "5            0.916280           0.345674            0.904813      0.002863   \n",
      "6            0.924144           0.337031            0.922201      0.030296   \n",
      "7            0.916972           0.407742            0.917597      0.005751   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "0        0.000010        0.076391         0.000917  \n",
      "1        0.000275        0.053725         0.000917  \n",
      "2        0.000319        0.052769         0.000917  \n",
      "3        0.000374        0.068188         0.000917  \n",
      "4        0.000168        0.053285         0.000917  \n",
      "5        0.000100        0.053995         0.000917  \n",
      "6        0.000459        0.066278         0.000917  \n",
      "7        0.000165        0.052212         0.000917  \n",
      "0        0.000139        0.049477         0.011494  \n",
      "1        0.000171        0.041473         0.002178  \n",
      "2        0.000079        0.056342         0.004528  \n",
      "3        0.001883        0.057092         0.003885  \n",
      "4        0.000152        0.089388         0.017250  \n",
      "5        0.000346        0.060525         0.009662  \n",
      "6        0.001403        0.080311         0.005908  \n",
      "7        0.000644        0.047803         0.005972  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.021641         0.005110         0.397968          0.999327   \n",
      "1       0.057207         0.011799         0.452940          0.999327   \n",
      "2       0.086584         0.018918         0.472299          0.999327   \n",
      "3       0.127629         0.025684         0.464409          0.999327   \n",
      "4       0.056939         0.005046         0.447631          0.999327   \n",
      "5       0.135677         0.012172         0.453036          0.999327   \n",
      "6       0.201224         0.019013         0.460067          0.999327   \n",
      "7       0.272294         0.025723         0.461594          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 2, 2]             mse                 10   \n",
      "1  [0, 2, 2]             mse                 25   \n",
      "2  [0, 2, 2]             mse                 40   \n",
      "3  [0, 2, 2]             mse                 55   \n",
      "4  [0, 2, 2]             mae                 10   \n",
      "5  [0, 2, 2]             mae                 25   \n",
      "6  [0, 2, 2]             mae                 40   \n",
      "7  [0, 2, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                4   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                3   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.448027            0.999978           0.405060   \n",
      "1           0.511811            0.999978           0.463751   \n",
      "2           0.564754            0.999978           0.479838   \n",
      "3           0.523986            0.999978           0.492553   \n",
      "4           0.499696            0.999978           0.443762   \n",
      "5           0.505800            0.999978           0.469945   \n",
      "6           0.531864            0.999978           0.487001   \n",
      "7           0.515662            0.999978           0.474997   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.340278            0.999974      0.000178   \n",
      "1             0.99803           0.382624            0.999974      0.004856   \n",
      "2             0.99803           0.371312            0.999974      0.000149   \n",
      "3             0.99803           0.376048            0.999974      0.012641   \n",
      "4             0.99803           0.398875            0.999974      0.001311   \n",
      "5             0.99803           0.382795            0.999974      0.004922   \n",
      "6             0.99803           0.360563            0.999974      0.007703   \n",
      "7             0.99803           0.393541            0.999974      0.010708   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000249        0.044310         0.000917  \n",
      "1        0.000110        0.053334         0.000917  \n",
      "2        0.000085        0.079220         0.000917  \n",
      "3        0.000315        0.063623         0.000917  \n",
      "4        0.000078        0.041286         0.000917  \n",
      "5        0.000161        0.051656         0.000917  \n",
      "6        0.000131        0.072526         0.000917  \n",
      "7        0.000124        0.050787         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "0       0.022064         0.005022         0.429469          0.999327   \n",
      "1       0.057824         0.011764         0.477087          0.999327   \n",
      "2       0.089325         0.019156         0.447721          0.999327   \n",
      "3       0.121582         0.025687         0.465677          0.999327   \n",
      "4       0.050461         0.005172         0.417820          0.999327   \n",
      "5       0.129415         0.011766         0.455314          0.999327   \n",
      "6       0.204490         0.018932         0.445757          0.999327   \n",
      "7       0.280706         0.025735         0.440078          0.999327   \n",
      "0       0.023749         0.004902         0.377227          0.894722   \n",
      "1       0.061383         0.011770         0.447905          0.912889   \n",
      "2       0.093766         0.018978         0.414147          0.920426   \n",
      "3       0.128777         0.026892         0.423102          0.918386   \n",
      "4       0.049568         0.005000         0.421511          0.883591   \n",
      "5       0.121377         0.012151         0.422526          0.904570   \n",
      "6       0.226937         0.019638         0.434604          0.919032   \n",
      "7       0.268454         0.025964         0.455829          0.913065   \n",
      "0       0.021641         0.005110         0.397968          0.999327   \n",
      "1       0.057207         0.011799         0.452940          0.999327   \n",
      "2       0.086584         0.018918         0.472299          0.999327   \n",
      "3       0.127629         0.025684         0.464409          0.999327   \n",
      "4       0.056939         0.005046         0.447631          0.999327   \n",
      "5       0.135677         0.012172         0.453036          0.999327   \n",
      "6       0.201224         0.019013         0.460067          0.999327   \n",
      "7       0.272294         0.025723         0.461594          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "0  [0, 1, 2]             mse                 10   \n",
      "1  [0, 1, 2]             mse                 25   \n",
      "2  [0, 1, 2]             mse                 40   \n",
      "3  [0, 1, 2]             mse                 55   \n",
      "4  [0, 1, 2]             mae                 10   \n",
      "5  [0, 1, 2]             mae                 25   \n",
      "6  [0, 1, 2]             mae                 40   \n",
      "7  [0, 1, 2]             mae                 55   \n",
      "0  [0, 2, 1]             mse                 10   \n",
      "1  [0, 2, 1]             mse                 25   \n",
      "2  [0, 2, 1]             mse                 40   \n",
      "3  [0, 2, 1]             mse                 55   \n",
      "4  [0, 2, 1]             mae                 10   \n",
      "5  [0, 2, 1]             mae                 25   \n",
      "6  [0, 2, 1]             mae                 40   \n",
      "7  [0, 2, 1]             mae                 55   \n",
      "0  [0, 2, 2]             mse                 10   \n",
      "1  [0, 2, 2]             mse                 25   \n",
      "2  [0, 2, 2]             mse                 40   \n",
      "3  [0, 2, 2]             mse                 55   \n",
      "4  [0, 2, 2]             mae                 10   \n",
      "5  [0, 2, 2]             mae                 25   \n",
      "6  [0, 2, 2]             mae                 40   \n",
      "7  [0, 2, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                7   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                4   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                3   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "0           0.531158            0.999978           0.408375   \n",
      "1           0.503439            0.999978           0.525550   \n",
      "2           0.495257            0.999978           0.473424   \n",
      "3           0.532163            0.999978           0.492380   \n",
      "4           0.479440            0.999978           0.423900   \n",
      "5           0.514665            0.999978           0.466658   \n",
      "6           0.511391            0.999978           0.470341   \n",
      "7           0.503981            0.999978           0.439359   \n",
      "0           0.394553            0.908679           0.427265   \n",
      "1           0.506235            0.910769           0.419286   \n",
      "2           0.490336            0.915519           0.356600   \n",
      "3           0.499550            0.915338           0.405926   \n",
      "4           0.530776            0.859296           0.420564   \n",
      "5           0.493540            0.892617           0.427600   \n",
      "6           0.533535            0.910753           0.432182   \n",
      "7           0.520751            0.904628           0.438295   \n",
      "0           0.448027            0.999978           0.405060   \n",
      "1           0.511811            0.999978           0.463751   \n",
      "2           0.564754            0.999978           0.479838   \n",
      "3           0.523986            0.999978           0.492553   \n",
      "4           0.499696            0.999978           0.443762   \n",
      "5           0.505800            0.999978           0.469945   \n",
      "6           0.531864            0.999978           0.487001   \n",
      "7           0.515662            0.999978           0.474997   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "0            0.998030           0.347781            0.999974      0.000520   \n",
      "1            0.998030           0.401990            0.999974      0.005516   \n",
      "2            0.998030           0.373970            0.999974      0.002415   \n",
      "3            0.998030           0.371773            0.999974      0.002624   \n",
      "4            0.998030           0.349457            0.999974      0.001997   \n",
      "5            0.998030           0.383980            0.999974      0.000475   \n",
      "6            0.998030           0.354833            0.999974      0.004821   \n",
      "7            0.998030           0.376207            0.999974      0.005013   \n",
      "0            0.880529           0.309677            0.894957      0.000176   \n",
      "1            0.912013           0.417566            0.915883      0.003928   \n",
      "2            0.926442           0.394687            0.919317      0.000166   \n",
      "3            0.923868           0.363007            0.915952      0.000726   \n",
      "4            0.893825           0.312019            0.897651      0.001317   \n",
      "5            0.916280           0.345674            0.904813      0.002863   \n",
      "6            0.924144           0.337031            0.922201      0.030296   \n",
      "7            0.916972           0.407742            0.917597      0.005751   \n",
      "0            0.998030           0.340278            0.999974      0.000178   \n",
      "1            0.998030           0.382624            0.999974      0.004856   \n",
      "2            0.998030           0.371312            0.999974      0.000149   \n",
      "3            0.998030           0.376048            0.999974      0.012641   \n",
      "4            0.998030           0.398875            0.999974      0.001311   \n",
      "5            0.998030           0.382795            0.999974      0.004922   \n",
      "6            0.998030           0.360563            0.999974      0.007703   \n",
      "7            0.998030           0.393541            0.999974      0.010708   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "0        0.000010        0.076391         0.000917  \n",
      "1        0.000275        0.053725         0.000917  \n",
      "2        0.000319        0.052769         0.000917  \n",
      "3        0.000374        0.068188         0.000917  \n",
      "4        0.000168        0.053285         0.000917  \n",
      "5        0.000100        0.053995         0.000917  \n",
      "6        0.000459        0.066278         0.000917  \n",
      "7        0.000165        0.052212         0.000917  \n",
      "0        0.000139        0.049477         0.011494  \n",
      "1        0.000171        0.041473         0.002178  \n",
      "2        0.000079        0.056342         0.004528  \n",
      "3        0.001883        0.057092         0.003885  \n",
      "4        0.000152        0.089388         0.017250  \n",
      "5        0.000346        0.060525         0.009662  \n",
      "6        0.001403        0.080311         0.005908  \n",
      "7        0.000644        0.047803         0.005972  \n",
      "0        0.000249        0.044310         0.000917  \n",
      "1        0.000110        0.053334         0.000917  \n",
      "2        0.000085        0.079220         0.000917  \n",
      "3        0.000315        0.063623         0.000917  \n",
      "4        0.000078        0.041286         0.000917  \n",
      "5        0.000161        0.051656         0.000917  \n",
      "6        0.000131        0.072526         0.000917  \n",
      "7        0.000124        0.050787         0.000917  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.026928         0.004926         0.529766          0.914276   \n",
      "1       0.067621         0.012016         0.517010          0.919884   \n",
      "2       0.107500         0.018467         0.529725          0.928579   \n",
      "3       0.150285         0.025328         0.529085          0.929993   \n",
      "4       0.079906         0.005054         0.471788          0.908994   \n",
      "5       0.203138         0.012009         0.522322          0.924236   \n",
      "6       0.340299         0.020804         0.514097          0.925083   \n",
      "7       0.507108         0.029291         0.535624          0.929226   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 0, 1]             mse                 10   \n",
      "1  [1, 0, 1]             mse                 25   \n",
      "2  [1, 0, 1]             mse                 40   \n",
      "3  [1, 0, 1]             mse                 55   \n",
      "4  [1, 0, 1]             mae                 10   \n",
      "5  [1, 0, 1]             mae                 25   \n",
      "6  [1, 0, 1]             mae                 40   \n",
      "7  [1, 0, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.593655            0.899050           0.564145   \n",
      "1           0.576332            0.914199           0.574699   \n",
      "2           0.585921            0.932432           0.571995   \n",
      "3           0.569633            0.923452           0.572766   \n",
      "4           0.556942            0.919819           0.520084   \n",
      "5           0.522936            0.929253           0.575438   \n",
      "6           0.567707            0.919558           0.544228   \n",
      "7           0.562966            0.918211           0.582872   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.916170           0.430812            0.927607      0.000247   \n",
      "1            0.916065           0.399362            0.929387      0.000219   \n",
      "2            0.917596           0.430655            0.935709      0.000166   \n",
      "3            0.928583           0.444420            0.937943      0.003182   \n",
      "4            0.906917           0.337422            0.900244      0.001381   \n",
      "5            0.918809           0.468585            0.924647      0.008198   \n",
      "6            0.925269           0.429778            0.930421      0.034664   \n",
      "7            0.933235           0.460740            0.936233      0.016345   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000115        0.070818         0.011735  \n",
      "1        0.000420        0.082970         0.006763  \n",
      "2        0.000161        0.070097         0.007880  \n",
      "3        0.000340        0.059721         0.005999  \n",
      "4        0.000050        0.095946         0.008125  \n",
      "5        0.000034        0.043547         0.004273  \n",
      "6        0.001539        0.060231         0.004437  \n",
      "7        0.001807        0.053432         0.007885  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024136         0.005008         0.473376          0.883712   \n",
      "1       0.060178         0.011970         0.492589          0.913149   \n",
      "2       0.095356         0.018556         0.501884          0.919258   \n",
      "3       0.131009         0.025559         0.495889          0.920909   \n",
      "4       0.050747         0.005121         0.485578          0.894911   \n",
      "5       0.135085         0.011601         0.484008          0.915978   \n",
      "6       0.215881         0.022047         0.491211          0.916460   \n",
      "7       0.287532         0.025424         0.499552          0.918961   \n",
      "0       0.021495         0.004955         0.455315          0.999327   \n",
      "1       0.055847         0.012153         0.482821          0.999327   \n",
      "2       0.085707         0.020287         0.480219          0.999327   \n",
      "3       0.116703         0.025506         0.482323          0.999327   \n",
      "4       0.051623         0.004975         0.441957          0.999327   \n",
      "5       0.130315         0.011985         0.475367          0.999327   \n",
      "6       0.228170         0.020735         0.485410          0.999327   \n",
      "7       0.326909         0.028547         0.480426          0.999327   \n",
      "0       0.024429         0.005034         0.438331          0.886438   \n",
      "1       0.068368         0.014011         0.417784          0.910506   \n",
      "2       0.120861         0.026287         0.452943          0.910942   \n",
      "3       0.146770         0.026211         0.435130          0.917921   \n",
      "4       0.053327         0.005148         0.424631          0.873036   \n",
      "5       0.132050         0.012251         0.447810          0.904868   \n",
      "6       0.217647         0.023733         0.464478          0.909622   \n",
      "7       0.292883         0.026061         0.443942          0.914943   \n",
      "0       0.022064         0.005022         0.429469          0.999327   \n",
      "1       0.057824         0.011764         0.477087          0.999327   \n",
      "2       0.089325         0.019156         0.447721          0.999327   \n",
      "3       0.121582         0.025687         0.465677          0.999327   \n",
      "4       0.050461         0.005172         0.417820          0.999327   \n",
      "5       0.129415         0.011766         0.455314          0.999327   \n",
      "6       0.204490         0.018932         0.445757          0.999327   \n",
      "7       0.280706         0.025735         0.440078          0.999327   \n",
      "0       0.023749         0.004902         0.377227          0.894722   \n",
      "1       0.061383         0.011770         0.447905          0.912889   \n",
      "2       0.093766         0.018978         0.414147          0.920426   \n",
      "3       0.128777         0.026892         0.423102          0.918386   \n",
      "4       0.049568         0.005000         0.421511          0.883591   \n",
      "5       0.121377         0.012151         0.422526          0.904570   \n",
      "6       0.226937         0.019638         0.434604          0.919032   \n",
      "7       0.268454         0.025964         0.455829          0.913065   \n",
      "0       0.021641         0.005110         0.397968          0.999327   \n",
      "1       0.057207         0.011799         0.452940          0.999327   \n",
      "2       0.086584         0.018918         0.472299          0.999327   \n",
      "3       0.127629         0.025684         0.464409          0.999327   \n",
      "4       0.056939         0.005046         0.447631          0.999327   \n",
      "5       0.135677         0.012172         0.453036          0.999327   \n",
      "6       0.201224         0.019013         0.460067          0.999327   \n",
      "7       0.272294         0.025723         0.461594          0.999327   \n",
      "0       0.026928         0.004926         0.529766          0.914276   \n",
      "1       0.067621         0.012016         0.517010          0.919884   \n",
      "2       0.107500         0.018467         0.529725          0.928579   \n",
      "3       0.150285         0.025328         0.529085          0.929993   \n",
      "4       0.079906         0.005054         0.471788          0.908994   \n",
      "5       0.203138         0.012009         0.522322          0.924236   \n",
      "6       0.340299         0.020804         0.514097          0.925083   \n",
      "7       0.507108         0.029291         0.535624          0.929226   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "0  [0, 1, 1]             mse                 10   \n",
      "1  [0, 1, 1]             mse                 25   \n",
      "2  [0, 1, 1]             mse                 40   \n",
      "3  [0, 1, 1]             mse                 55   \n",
      "4  [0, 1, 1]             mae                 10   \n",
      "5  [0, 1, 1]             mae                 25   \n",
      "6  [0, 1, 1]             mae                 40   \n",
      "7  [0, 1, 1]             mae                 55   \n",
      "0  [0, 1, 2]             mse                 10   \n",
      "1  [0, 1, 2]             mse                 25   \n",
      "2  [0, 1, 2]             mse                 40   \n",
      "3  [0, 1, 2]             mse                 55   \n",
      "4  [0, 1, 2]             mae                 10   \n",
      "5  [0, 1, 2]             mae                 25   \n",
      "6  [0, 1, 2]             mae                 40   \n",
      "7  [0, 1, 2]             mae                 55   \n",
      "0  [0, 2, 1]             mse                 10   \n",
      "1  [0, 2, 1]             mse                 25   \n",
      "2  [0, 2, 1]             mse                 40   \n",
      "3  [0, 2, 1]             mse                 55   \n",
      "4  [0, 2, 1]             mae                 10   \n",
      "5  [0, 2, 1]             mae                 25   \n",
      "6  [0, 2, 1]             mae                 40   \n",
      "7  [0, 2, 1]             mae                 55   \n",
      "0  [0, 2, 2]             mse                 10   \n",
      "1  [0, 2, 2]             mse                 25   \n",
      "2  [0, 2, 2]             mse                 40   \n",
      "3  [0, 2, 2]             mse                 55   \n",
      "4  [0, 2, 2]             mae                 10   \n",
      "5  [0, 2, 2]             mae                 25   \n",
      "6  [0, 2, 2]             mae                 40   \n",
      "7  [0, 2, 2]             mae                 55   \n",
      "0  [1, 0, 1]             mse                 10   \n",
      "1  [1, 0, 1]             mse                 25   \n",
      "2  [1, 0, 1]             mse                 40   \n",
      "3  [1, 0, 1]             mse                 55   \n",
      "4  [1, 0, 1]             mae                 10   \n",
      "5  [1, 0, 1]             mae                 25   \n",
      "6  [1, 0, 1]             mae                 40   \n",
      "7  [1, 0, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                7   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                4   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                3   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.532909            0.875786           0.441208   \n",
      "1           0.539835            0.910320           0.514354   \n",
      "2           0.565894            0.912127           0.505728   \n",
      "3           0.544117            0.922500           0.478462   \n",
      "4           0.558931            0.882973           0.483207   \n",
      "5           0.564253            0.916684           0.445223   \n",
      "6           0.573519            0.910791           0.453887   \n",
      "7           0.585932            0.915393           0.504125   \n",
      "0           0.492315            0.999978           0.449155   \n",
      "1           0.542346            0.999978           0.499233   \n",
      "2           0.531688            0.999978           0.510485   \n",
      "3           0.525395            0.999978           0.500564   \n",
      "4           0.497328            0.999978           0.482597   \n",
      "5           0.553961            0.999978           0.466189   \n",
      "6           0.566515            0.999978           0.481667   \n",
      "7           0.552411            0.999978           0.512055   \n",
      "0           0.469139            0.880897           0.496814   \n",
      "1           0.502791            0.902003           0.424643   \n",
      "2           0.542659            0.903799           0.430453   \n",
      "3           0.495867            0.906287           0.422693   \n",
      "4           0.512529            0.869530           0.414054   \n",
      "5           0.493631            0.894170           0.448127   \n",
      "6           0.525079            0.904297           0.460221   \n",
      "7           0.508563            0.905416           0.466128   \n",
      "0           0.531158            0.999978           0.408375   \n",
      "1           0.503439            0.999978           0.525550   \n",
      "2           0.495257            0.999978           0.473424   \n",
      "3           0.532163            0.999978           0.492380   \n",
      "4           0.479440            0.999978           0.423900   \n",
      "5           0.514665            0.999978           0.466658   \n",
      "6           0.511391            0.999978           0.470341   \n",
      "7           0.503981            0.999978           0.439359   \n",
      "0           0.394553            0.908679           0.427265   \n",
      "1           0.506235            0.910769           0.419286   \n",
      "2           0.490336            0.915519           0.356600   \n",
      "3           0.499550            0.915338           0.405926   \n",
      "4           0.530776            0.859296           0.420564   \n",
      "5           0.493540            0.892617           0.427600   \n",
      "6           0.533535            0.910753           0.432182   \n",
      "7           0.520751            0.904628           0.438295   \n",
      "0           0.448027            0.999978           0.405060   \n",
      "1           0.511811            0.999978           0.463751   \n",
      "2           0.564754            0.999978           0.479838   \n",
      "3           0.523986            0.999978           0.492553   \n",
      "4           0.499696            0.999978           0.443762   \n",
      "5           0.505800            0.999978           0.469945   \n",
      "6           0.531864            0.999978           0.487001   \n",
      "7           0.515662            0.999978           0.474997   \n",
      "0           0.593655            0.899050           0.564145   \n",
      "1           0.576332            0.914199           0.574699   \n",
      "2           0.585921            0.932432           0.571995   \n",
      "3           0.569633            0.923452           0.572766   \n",
      "4           0.556942            0.919819           0.520084   \n",
      "5           0.522936            0.929253           0.575438   \n",
      "6           0.567707            0.919558           0.544228   \n",
      "7           0.562966            0.918211           0.582872   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.877617           0.445372            0.897734      0.000139   \n",
      "1            0.912061           0.423071            0.917066      0.000326   \n",
      "2            0.923810           0.433343            0.921837      0.000400   \n",
      "3            0.923717           0.464570            0.916511      0.000613   \n",
      "4            0.905834           0.413808            0.895927      0.001724   \n",
      "5            0.912121           0.441685            0.919130      0.001826   \n",
      "6            0.920353           0.445343            0.918235      0.007670   \n",
      "7            0.924390           0.407669            0.917102      0.005413   \n",
      "0            0.998030           0.424076            0.999974      0.000405   \n",
      "1            0.998030           0.406243            0.999974      0.002746   \n",
      "2            0.998030           0.397930            0.999974      0.001071   \n",
      "3            0.998030           0.420547            0.999974      0.000255   \n",
      "4            0.998030           0.345349            0.999974      0.001128   \n",
      "5            0.998030           0.405106            0.999974      0.004279   \n",
      "6            0.998030           0.407175            0.999974      0.022929   \n",
      "7            0.998030           0.376038            0.999974      0.028923   \n",
      "0            0.869641           0.348710            0.908778      0.000299   \n",
      "1            0.914159           0.325005            0.915357      0.004393   \n",
      "2            0.911402           0.384754            0.917626      0.004156   \n",
      "3            0.922294           0.386176            0.925181      0.015858   \n",
      "4            0.870999           0.346366            0.878579      0.001481   \n",
      "5            0.915648           0.401179            0.904786      0.002898   \n",
      "6            0.917081           0.407480            0.907487      0.010971   \n",
      "7            0.921047           0.356440            0.918367      0.006066   \n",
      "0            0.998030           0.347781            0.999974      0.000520   \n",
      "1            0.998030           0.401990            0.999974      0.005516   \n",
      "2            0.998030           0.373970            0.999974      0.002415   \n",
      "3            0.998030           0.371773            0.999974      0.002624   \n",
      "4            0.998030           0.349457            0.999974      0.001997   \n",
      "5            0.998030           0.383980            0.999974      0.000475   \n",
      "6            0.998030           0.354833            0.999974      0.004821   \n",
      "7            0.998030           0.376207            0.999974      0.005013   \n",
      "0            0.880529           0.309677            0.894957      0.000176   \n",
      "1            0.912013           0.417566            0.915883      0.003928   \n",
      "2            0.926442           0.394687            0.919317      0.000166   \n",
      "3            0.923868           0.363007            0.915952      0.000726   \n",
      "4            0.893825           0.312019            0.897651      0.001317   \n",
      "5            0.916280           0.345674            0.904813      0.002863   \n",
      "6            0.924144           0.337031            0.922201      0.030296   \n",
      "7            0.916972           0.407742            0.917597      0.005751   \n",
      "0            0.998030           0.340278            0.999974      0.000178   \n",
      "1            0.998030           0.382624            0.999974      0.004856   \n",
      "2            0.998030           0.371312            0.999974      0.000149   \n",
      "3            0.998030           0.376048            0.999974      0.012641   \n",
      "4            0.998030           0.398875            0.999974      0.001311   \n",
      "5            0.998030           0.382795            0.999974      0.004922   \n",
      "6            0.998030           0.360563            0.999974      0.007703   \n",
      "7            0.998030           0.393541            0.999974      0.010708   \n",
      "0            0.916170           0.430812            0.927607      0.000247   \n",
      "1            0.916065           0.399362            0.929387      0.000219   \n",
      "2            0.917596           0.430655            0.935709      0.000166   \n",
      "3            0.928583           0.444420            0.937943      0.003182   \n",
      "4            0.906917           0.337422            0.900244      0.001381   \n",
      "5            0.918809           0.468585            0.924647      0.008198   \n",
      "6            0.925269           0.429778            0.930421      0.034664   \n",
      "7            0.933235           0.460740            0.936233      0.016345   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000128        0.042355         0.009943  \n",
      "1        0.000056        0.050119         0.002860  \n",
      "2        0.000246        0.054229         0.005106  \n",
      "3        0.000061        0.034750         0.003149  \n",
      "4        0.000112        0.059322         0.009361  \n",
      "5        0.000085        0.057065         0.002905  \n",
      "6        0.004956        0.058616         0.004101  \n",
      "7        0.000275        0.072911         0.003901  \n",
      "0        0.000361        0.028219         0.000917  \n",
      "1        0.000495        0.056804         0.000917  \n",
      "2        0.001730        0.058674         0.000917  \n",
      "3        0.000464        0.044731         0.000917  \n",
      "4        0.000103        0.068394         0.000917  \n",
      "5        0.000121        0.061167         0.000917  \n",
      "6        0.001815        0.065161         0.000917  \n",
      "7        0.003829        0.075440         0.000917  \n",
      "0        0.000216        0.064206         0.016451  \n",
      "1        0.001236        0.072806         0.006032  \n",
      "2        0.002018        0.066442         0.005654  \n",
      "3        0.001032        0.045671         0.008310  \n",
      "4        0.000067        0.068304         0.003965  \n",
      "5        0.000056        0.037777         0.008769  \n",
      "6        0.000500        0.048145         0.005433  \n",
      "7        0.000540        0.064097         0.006825  \n",
      "0        0.000010        0.076391         0.000917  \n",
      "1        0.000275        0.053725         0.000917  \n",
      "2        0.000319        0.052769         0.000917  \n",
      "3        0.000374        0.068188         0.000917  \n",
      "4        0.000168        0.053285         0.000917  \n",
      "5        0.000100        0.053995         0.000917  \n",
      "6        0.000459        0.066278         0.000917  \n",
      "7        0.000165        0.052212         0.000917  \n",
      "0        0.000139        0.049477         0.011494  \n",
      "1        0.000171        0.041473         0.002178  \n",
      "2        0.000079        0.056342         0.004528  \n",
      "3        0.001883        0.057092         0.003885  \n",
      "4        0.000152        0.089388         0.017250  \n",
      "5        0.000346        0.060525         0.009662  \n",
      "6        0.001403        0.080311         0.005908  \n",
      "7        0.000644        0.047803         0.005972  \n",
      "0        0.000249        0.044310         0.000917  \n",
      "1        0.000110        0.053334         0.000917  \n",
      "2        0.000085        0.079220         0.000917  \n",
      "3        0.000315        0.063623         0.000917  \n",
      "4        0.000078        0.041286         0.000917  \n",
      "5        0.000161        0.051656         0.000917  \n",
      "6        0.000131        0.072526         0.000917  \n",
      "7        0.000124        0.050787         0.000917  \n",
      "0        0.000115        0.070818         0.011735  \n",
      "1        0.000420        0.082970         0.006763  \n",
      "2        0.000161        0.070097         0.007880  \n",
      "3        0.000340        0.059721         0.005999  \n",
      "4        0.000050        0.095946         0.008125  \n",
      "5        0.000034        0.043547         0.004273  \n",
      "6        0.001539        0.060231         0.004437  \n",
      "7        0.001807        0.053432         0.007885  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.030515         0.008088         0.510087          0.999327   \n",
      "1       0.074109         0.014175         0.532244          0.999327   \n",
      "2       0.108297         0.023410         0.566013          0.999327   \n",
      "3       0.152059         0.029719         0.567674          0.999327   \n",
      "4       0.087587         0.006201         0.524329          0.999327   \n",
      "5       0.234337         0.013297         0.553500          0.999327   \n",
      "6       0.364388         0.020911         0.520476          0.999327   \n",
      "7       0.492291         0.029228         0.546656          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 0, 2]             mse                 10   \n",
      "1  [1, 0, 2]             mse                 25   \n",
      "2  [1, 0, 2]             mse                 40   \n",
      "3  [1, 0, 2]             mse                 55   \n",
      "4  [1, 0, 2]             mae                 10   \n",
      "5  [1, 0, 2]             mae                 25   \n",
      "6  [1, 0, 2]             mae                 40   \n",
      "7  [1, 0, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                5   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.561333            0.999978           0.566889   \n",
      "1           0.613994            0.999978           0.606001   \n",
      "2           0.626742            0.999978           0.612241   \n",
      "3           0.643035            0.999978           0.606921   \n",
      "4           0.595589            0.999978           0.584735   \n",
      "5           0.599834            0.999978           0.619800   \n",
      "6           0.574362            0.999978           0.580725   \n",
      "7           0.610133            0.999978           0.592280   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.401487            0.999974      0.001222   \n",
      "1             0.99803           0.375858            0.999974      0.015076   \n",
      "2             0.99803           0.458402            0.999974      0.012188   \n",
      "3             0.99803           0.452257            0.999974      0.008633   \n",
      "4             0.99803           0.391896            0.999974      0.005510   \n",
      "5             0.99803           0.440368            0.999974      0.008436   \n",
      "6             0.99803           0.405761            0.999974      0.004622   \n",
      "7             0.99803           0.436871            0.999974      0.008887   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.001682        0.076619         0.000917  \n",
      "1        0.000813        0.110334         0.000917  \n",
      "2        0.005118        0.076120         0.000917  \n",
      "3        0.000766        0.082721         0.000917  \n",
      "4        0.001011        0.093499         0.000917  \n",
      "5        0.000826        0.080198         0.000917  \n",
      "6        0.001109        0.080940         0.000917  \n",
      "7        0.001584        0.077765         0.000917  \n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024136         0.005008         0.473376          0.883712   \n",
      "1        0.060178         0.011970         0.492589          0.913149   \n",
      "2        0.095356         0.018556         0.501884          0.919258   \n",
      "3        0.131009         0.025559         0.495889          0.920909   \n",
      "4        0.050747         0.005121         0.485578          0.894911   \n",
      "5        0.135085         0.011601         0.484008          0.915978   \n",
      "6        0.215881         0.022047         0.491211          0.916460   \n",
      "7        0.287532         0.025424         0.499552          0.918961   \n",
      "0        0.021495         0.004955         0.455315          0.999327   \n",
      "1        0.055847         0.012153         0.482821          0.999327   \n",
      "2        0.085707         0.020287         0.480219          0.999327   \n",
      "3        0.116703         0.025506         0.482323          0.999327   \n",
      "4        0.051623         0.004975         0.441957          0.999327   \n",
      "5        0.130315         0.011985         0.475367          0.999327   \n",
      "6        0.228170         0.020735         0.485410          0.999327   \n",
      "7        0.326909         0.028547         0.480426          0.999327   \n",
      "0        0.024429         0.005034         0.438331          0.886438   \n",
      "1        0.068368         0.014011         0.417784          0.910506   \n",
      "2        0.120861         0.026287         0.452943          0.910942   \n",
      "3        0.146770         0.026211         0.435130          0.917921   \n",
      "4        0.053327         0.005148         0.424631          0.873036   \n",
      "5        0.132050         0.012251         0.447810          0.904868   \n",
      "6        0.217647         0.023733         0.464478          0.909622   \n",
      "7        0.292883         0.026061         0.443942          0.914943   \n",
      "0        0.022064         0.005022         0.429469          0.999327   \n",
      "1        0.057824         0.011764         0.477087          0.999327   \n",
      "2        0.089325         0.019156         0.447721          0.999327   \n",
      "3        0.121582         0.025687         0.465677          0.999327   \n",
      "4        0.050461         0.005172         0.417820          0.999327   \n",
      "5        0.129415         0.011766         0.455314          0.999327   \n",
      "..            ...              ...              ...               ...   \n",
      "2        0.093766         0.018978         0.414147          0.920426   \n",
      "3        0.128777         0.026892         0.423102          0.918386   \n",
      "4        0.049568         0.005000         0.421511          0.883591   \n",
      "5        0.121377         0.012151         0.422526          0.904570   \n",
      "6        0.226937         0.019638         0.434604          0.919032   \n",
      "7        0.268454         0.025964         0.455829          0.913065   \n",
      "0        0.021641         0.005110         0.397968          0.999327   \n",
      "1        0.057207         0.011799         0.452940          0.999327   \n",
      "2        0.086584         0.018918         0.472299          0.999327   \n",
      "3        0.127629         0.025684         0.464409          0.999327   \n",
      "4        0.056939         0.005046         0.447631          0.999327   \n",
      "5        0.135677         0.012172         0.453036          0.999327   \n",
      "6        0.201224         0.019013         0.460067          0.999327   \n",
      "7        0.272294         0.025723         0.461594          0.999327   \n",
      "0        0.026928         0.004926         0.529766          0.914276   \n",
      "1        0.067621         0.012016         0.517010          0.919884   \n",
      "2        0.107500         0.018467         0.529725          0.928579   \n",
      "3        0.150285         0.025328         0.529085          0.929993   \n",
      "4        0.079906         0.005054         0.471788          0.908994   \n",
      "5        0.203138         0.012009         0.522322          0.924236   \n",
      "6        0.340299         0.020804         0.514097          0.925083   \n",
      "7        0.507108         0.029291         0.535624          0.929226   \n",
      "0        0.030515         0.008088         0.510087          0.999327   \n",
      "1        0.074109         0.014175         0.532244          0.999327   \n",
      "2        0.108297         0.023410         0.566013          0.999327   \n",
      "3        0.152059         0.029719         0.567674          0.999327   \n",
      "4        0.087587         0.006201         0.524329          0.999327   \n",
      "5        0.234337         0.013297         0.553500          0.999327   \n",
      "6        0.364388         0.020911         0.520476          0.999327   \n",
      "7        0.492291         0.029228         0.546656          0.999327   \n",
      "\n",
      "   method_ids param_criterion param_n_estimators  \\\n",
      "0   [0, 0, 1]             mse                 10   \n",
      "1   [0, 0, 1]             mse                 25   \n",
      "2   [0, 0, 1]             mse                 40   \n",
      "3   [0, 0, 1]             mse                 55   \n",
      "4   [0, 0, 1]             mae                 10   \n",
      "5   [0, 0, 1]             mae                 25   \n",
      "6   [0, 0, 1]             mae                 40   \n",
      "7   [0, 0, 1]             mae                 55   \n",
      "0   [0, 0, 2]             mse                 10   \n",
      "1   [0, 0, 2]             mse                 25   \n",
      "2   [0, 0, 2]             mse                 40   \n",
      "3   [0, 0, 2]             mse                 55   \n",
      "4   [0, 0, 2]             mae                 10   \n",
      "5   [0, 0, 2]             mae                 25   \n",
      "6   [0, 0, 2]             mae                 40   \n",
      "7   [0, 0, 2]             mae                 55   \n",
      "0   [0, 1, 1]             mse                 10   \n",
      "1   [0, 1, 1]             mse                 25   \n",
      "2   [0, 1, 1]             mse                 40   \n",
      "3   [0, 1, 1]             mse                 55   \n",
      "4   [0, 1, 1]             mae                 10   \n",
      "5   [0, 1, 1]             mae                 25   \n",
      "6   [0, 1, 1]             mae                 40   \n",
      "7   [0, 1, 1]             mae                 55   \n",
      "0   [0, 1, 2]             mse                 10   \n",
      "1   [0, 1, 2]             mse                 25   \n",
      "2   [0, 1, 2]             mse                 40   \n",
      "3   [0, 1, 2]             mse                 55   \n",
      "4   [0, 1, 2]             mae                 10   \n",
      "5   [0, 1, 2]             mae                 25   \n",
      "..        ...             ...                ...   \n",
      "2   [0, 2, 1]             mse                 40   \n",
      "3   [0, 2, 1]             mse                 55   \n",
      "4   [0, 2, 1]             mae                 10   \n",
      "5   [0, 2, 1]             mae                 25   \n",
      "6   [0, 2, 1]             mae                 40   \n",
      "7   [0, 2, 1]             mae                 55   \n",
      "0   [0, 2, 2]             mse                 10   \n",
      "1   [0, 2, 2]             mse                 25   \n",
      "2   [0, 2, 2]             mse                 40   \n",
      "3   [0, 2, 2]             mse                 55   \n",
      "4   [0, 2, 2]             mae                 10   \n",
      "5   [0, 2, 2]             mae                 25   \n",
      "6   [0, 2, 2]             mae                 40   \n",
      "7   [0, 2, 2]             mae                 55   \n",
      "0   [1, 0, 1]             mse                 10   \n",
      "1   [1, 0, 1]             mse                 25   \n",
      "2   [1, 0, 1]             mse                 40   \n",
      "3   [1, 0, 1]             mse                 55   \n",
      "4   [1, 0, 1]             mae                 10   \n",
      "5   [1, 0, 1]             mae                 25   \n",
      "6   [1, 0, 1]             mae                 40   \n",
      "7   [1, 0, 1]             mae                 55   \n",
      "0   [1, 0, 2]             mse                 10   \n",
      "1   [1, 0, 2]             mse                 25   \n",
      "2   [1, 0, 2]             mse                 40   \n",
      "3   [1, 0, 2]             mse                 55   \n",
      "4   [1, 0, 2]             mae                 10   \n",
      "5   [1, 0, 2]             mae                 25   \n",
      "6   [1, 0, 2]             mae                 40   \n",
      "7   [1, 0, 2]             mae                 55   \n",
      "\n",
      "                                         params  rank_test_score  \\\n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "..                                          ...              ...   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                7   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                4   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                3   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                5   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.532909            0.875786           0.441208   \n",
      "1            0.539835            0.910320           0.514354   \n",
      "2            0.565894            0.912127           0.505728   \n",
      "3            0.544117            0.922500           0.478462   \n",
      "4            0.558931            0.882973           0.483207   \n",
      "5            0.564253            0.916684           0.445223   \n",
      "6            0.573519            0.910791           0.453887   \n",
      "7            0.585932            0.915393           0.504125   \n",
      "0            0.492315            0.999978           0.449155   \n",
      "1            0.542346            0.999978           0.499233   \n",
      "2            0.531688            0.999978           0.510485   \n",
      "3            0.525395            0.999978           0.500564   \n",
      "4            0.497328            0.999978           0.482597   \n",
      "5            0.553961            0.999978           0.466189   \n",
      "6            0.566515            0.999978           0.481667   \n",
      "7            0.552411            0.999978           0.512055   \n",
      "0            0.469139            0.880897           0.496814   \n",
      "1            0.502791            0.902003           0.424643   \n",
      "2            0.542659            0.903799           0.430453   \n",
      "3            0.495867            0.906287           0.422693   \n",
      "4            0.512529            0.869530           0.414054   \n",
      "5            0.493631            0.894170           0.448127   \n",
      "6            0.525079            0.904297           0.460221   \n",
      "7            0.508563            0.905416           0.466128   \n",
      "0            0.531158            0.999978           0.408375   \n",
      "1            0.503439            0.999978           0.525550   \n",
      "2            0.495257            0.999978           0.473424   \n",
      "3            0.532163            0.999978           0.492380   \n",
      "4            0.479440            0.999978           0.423900   \n",
      "5            0.514665            0.999978           0.466658   \n",
      "..                ...                 ...                ...   \n",
      "2            0.490336            0.915519           0.356600   \n",
      "3            0.499550            0.915338           0.405926   \n",
      "4            0.530776            0.859296           0.420564   \n",
      "5            0.493540            0.892617           0.427600   \n",
      "6            0.533535            0.910753           0.432182   \n",
      "7            0.520751            0.904628           0.438295   \n",
      "0            0.448027            0.999978           0.405060   \n",
      "1            0.511811            0.999978           0.463751   \n",
      "2            0.564754            0.999978           0.479838   \n",
      "3            0.523986            0.999978           0.492553   \n",
      "4            0.499696            0.999978           0.443762   \n",
      "5            0.505800            0.999978           0.469945   \n",
      "6            0.531864            0.999978           0.487001   \n",
      "7            0.515662            0.999978           0.474997   \n",
      "0            0.593655            0.899050           0.564145   \n",
      "1            0.576332            0.914199           0.574699   \n",
      "2            0.585921            0.932432           0.571995   \n",
      "3            0.569633            0.923452           0.572766   \n",
      "4            0.556942            0.919819           0.520084   \n",
      "5            0.522936            0.929253           0.575438   \n",
      "6            0.567707            0.919558           0.544228   \n",
      "7            0.562966            0.918211           0.582872   \n",
      "0            0.561333            0.999978           0.566889   \n",
      "1            0.613994            0.999978           0.606001   \n",
      "2            0.626742            0.999978           0.612241   \n",
      "3            0.643035            0.999978           0.606921   \n",
      "4            0.595589            0.999978           0.584735   \n",
      "5            0.599834            0.999978           0.619800   \n",
      "6            0.574362            0.999978           0.580725   \n",
      "7            0.610133            0.999978           0.592280   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.877617           0.445372            0.897734      0.000139   \n",
      "1             0.912061           0.423071            0.917066      0.000326   \n",
      "2             0.923810           0.433343            0.921837      0.000400   \n",
      "3             0.923717           0.464570            0.916511      0.000613   \n",
      "4             0.905834           0.413808            0.895927      0.001724   \n",
      "5             0.912121           0.441685            0.919130      0.001826   \n",
      "6             0.920353           0.445343            0.918235      0.007670   \n",
      "7             0.924390           0.407669            0.917102      0.005413   \n",
      "0             0.998030           0.424076            0.999974      0.000405   \n",
      "1             0.998030           0.406243            0.999974      0.002746   \n",
      "2             0.998030           0.397930            0.999974      0.001071   \n",
      "3             0.998030           0.420547            0.999974      0.000255   \n",
      "4             0.998030           0.345349            0.999974      0.001128   \n",
      "5             0.998030           0.405106            0.999974      0.004279   \n",
      "6             0.998030           0.407175            0.999974      0.022929   \n",
      "7             0.998030           0.376038            0.999974      0.028923   \n",
      "0             0.869641           0.348710            0.908778      0.000299   \n",
      "1             0.914159           0.325005            0.915357      0.004393   \n",
      "2             0.911402           0.384754            0.917626      0.004156   \n",
      "3             0.922294           0.386176            0.925181      0.015858   \n",
      "4             0.870999           0.346366            0.878579      0.001481   \n",
      "5             0.915648           0.401179            0.904786      0.002898   \n",
      "6             0.917081           0.407480            0.907487      0.010971   \n",
      "7             0.921047           0.356440            0.918367      0.006066   \n",
      "0             0.998030           0.347781            0.999974      0.000520   \n",
      "1             0.998030           0.401990            0.999974      0.005516   \n",
      "2             0.998030           0.373970            0.999974      0.002415   \n",
      "3             0.998030           0.371773            0.999974      0.002624   \n",
      "4             0.998030           0.349457            0.999974      0.001997   \n",
      "5             0.998030           0.383980            0.999974      0.000475   \n",
      "..                 ...                ...                 ...           ...   \n",
      "2             0.926442           0.394687            0.919317      0.000166   \n",
      "3             0.923868           0.363007            0.915952      0.000726   \n",
      "4             0.893825           0.312019            0.897651      0.001317   \n",
      "5             0.916280           0.345674            0.904813      0.002863   \n",
      "6             0.924144           0.337031            0.922201      0.030296   \n",
      "7             0.916972           0.407742            0.917597      0.005751   \n",
      "0             0.998030           0.340278            0.999974      0.000178   \n",
      "1             0.998030           0.382624            0.999974      0.004856   \n",
      "2             0.998030           0.371312            0.999974      0.000149   \n",
      "3             0.998030           0.376048            0.999974      0.012641   \n",
      "4             0.998030           0.398875            0.999974      0.001311   \n",
      "5             0.998030           0.382795            0.999974      0.004922   \n",
      "6             0.998030           0.360563            0.999974      0.007703   \n",
      "7             0.998030           0.393541            0.999974      0.010708   \n",
      "0             0.916170           0.430812            0.927607      0.000247   \n",
      "1             0.916065           0.399362            0.929387      0.000219   \n",
      "2             0.917596           0.430655            0.935709      0.000166   \n",
      "3             0.928583           0.444420            0.937943      0.003182   \n",
      "4             0.906917           0.337422            0.900244      0.001381   \n",
      "5             0.918809           0.468585            0.924647      0.008198   \n",
      "6             0.925269           0.429778            0.930421      0.034664   \n",
      "7             0.933235           0.460740            0.936233      0.016345   \n",
      "0             0.998030           0.401487            0.999974      0.001222   \n",
      "1             0.998030           0.375858            0.999974      0.015076   \n",
      "2             0.998030           0.458402            0.999974      0.012188   \n",
      "3             0.998030           0.452257            0.999974      0.008633   \n",
      "4             0.998030           0.391896            0.999974      0.005510   \n",
      "5             0.998030           0.440368            0.999974      0.008436   \n",
      "6             0.998030           0.405761            0.999974      0.004622   \n",
      "7             0.998030           0.436871            0.999974      0.008887   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.000128        0.042355         0.009943  \n",
      "1         0.000056        0.050119         0.002860  \n",
      "2         0.000246        0.054229         0.005106  \n",
      "3         0.000061        0.034750         0.003149  \n",
      "4         0.000112        0.059322         0.009361  \n",
      "5         0.000085        0.057065         0.002905  \n",
      "6         0.004956        0.058616         0.004101  \n",
      "7         0.000275        0.072911         0.003901  \n",
      "0         0.000361        0.028219         0.000917  \n",
      "1         0.000495        0.056804         0.000917  \n",
      "2         0.001730        0.058674         0.000917  \n",
      "3         0.000464        0.044731         0.000917  \n",
      "4         0.000103        0.068394         0.000917  \n",
      "5         0.000121        0.061167         0.000917  \n",
      "6         0.001815        0.065161         0.000917  \n",
      "7         0.003829        0.075440         0.000917  \n",
      "0         0.000216        0.064206         0.016451  \n",
      "1         0.001236        0.072806         0.006032  \n",
      "2         0.002018        0.066442         0.005654  \n",
      "3         0.001032        0.045671         0.008310  \n",
      "4         0.000067        0.068304         0.003965  \n",
      "5         0.000056        0.037777         0.008769  \n",
      "6         0.000500        0.048145         0.005433  \n",
      "7         0.000540        0.064097         0.006825  \n",
      "0         0.000010        0.076391         0.000917  \n",
      "1         0.000275        0.053725         0.000917  \n",
      "2         0.000319        0.052769         0.000917  \n",
      "3         0.000374        0.068188         0.000917  \n",
      "4         0.000168        0.053285         0.000917  \n",
      "5         0.000100        0.053995         0.000917  \n",
      "..             ...             ...              ...  \n",
      "2         0.000079        0.056342         0.004528  \n",
      "3         0.001883        0.057092         0.003885  \n",
      "4         0.000152        0.089388         0.017250  \n",
      "5         0.000346        0.060525         0.009662  \n",
      "6         0.001403        0.080311         0.005908  \n",
      "7         0.000644        0.047803         0.005972  \n",
      "0         0.000249        0.044310         0.000917  \n",
      "1         0.000110        0.053334         0.000917  \n",
      "2         0.000085        0.079220         0.000917  \n",
      "3         0.000315        0.063623         0.000917  \n",
      "4         0.000078        0.041286         0.000917  \n",
      "5         0.000161        0.051656         0.000917  \n",
      "6         0.000131        0.072526         0.000917  \n",
      "7         0.000124        0.050787         0.000917  \n",
      "0         0.000115        0.070818         0.011735  \n",
      "1         0.000420        0.082970         0.006763  \n",
      "2         0.000161        0.070097         0.007880  \n",
      "3         0.000340        0.059721         0.005999  \n",
      "4         0.000050        0.095946         0.008125  \n",
      "5         0.000034        0.043547         0.004273  \n",
      "6         0.001539        0.060231         0.004437  \n",
      "7         0.001807        0.053432         0.007885  \n",
      "0         0.001682        0.076619         0.000917  \n",
      "1         0.000813        0.110334         0.000917  \n",
      "2         0.005118        0.076120         0.000917  \n",
      "3         0.000766        0.082721         0.000917  \n",
      "4         0.001011        0.093499         0.000917  \n",
      "5         0.000826        0.080198         0.000917  \n",
      "6         0.001109        0.080940         0.000917  \n",
      "7         0.001584        0.077765         0.000917  \n",
      "\n",
      "[64 rows x 19 columns]\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.030743         0.005673         0.472661          0.904200   \n",
      "1       0.079181         0.014082         0.434305          0.923994   \n",
      "2       0.119693         0.022323         0.465765          0.922783   \n",
      "3       0.159249         0.027426         0.478616          0.925977   \n",
      "4       0.091102         0.005276         0.384041          0.904600   \n",
      "5       0.237513         0.013088         0.471034          0.918397   \n",
      "6       0.383138         0.022711         0.452645          0.926690   \n",
      "7       0.513650         0.028162         0.437374          0.923543   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 1, 1]             mse                 10   \n",
      "1  [1, 1, 1]             mse                 25   \n",
      "2  [1, 1, 1]             mse                 40   \n",
      "3  [1, 1, 1]             mse                 55   \n",
      "4  [1, 1, 1]             mae                 10   \n",
      "5  [1, 1, 1]             mae                 25   \n",
      "6  [1, 1, 1]             mae                 40   \n",
      "7  [1, 1, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                7   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.529877            0.913407           0.474296   \n",
      "1           0.495826            0.919807           0.436976   \n",
      "2           0.491664            0.918327           0.526921   \n",
      "3           0.521132            0.929118           0.535756   \n",
      "4           0.346534            0.896983           0.493151   \n",
      "5           0.518611            0.925493           0.514109   \n",
      "6           0.470176            0.928646           0.515855   \n",
      "7           0.435470            0.929999           0.527282   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.874649           0.413195            0.924545      0.000490   \n",
      "1            0.920527           0.369452            0.931648      0.002282   \n",
      "2            0.919070           0.378431            0.930952      0.002355   \n",
      "3            0.921036           0.378503            0.927776      0.007452   \n",
      "4            0.896086           0.312841            0.920732      0.000950   \n",
      "5            0.903653           0.379872            0.926045      0.013425   \n",
      "6            0.920022           0.371715            0.931401      0.034239   \n",
      "7            0.914613           0.349390            0.926017      0.011770   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000421        0.047691         0.021385  \n",
      "1        0.001913        0.051672         0.005420  \n",
      "2        0.001147        0.063251         0.005785  \n",
      "3        0.001678        0.070853         0.003536  \n",
      "4        0.000251        0.078168         0.011413  \n",
      "5        0.000608        0.064316         0.010428  \n",
      "6        0.001851        0.060047         0.004847  \n",
      "7        0.001139        0.072507         0.006520  \n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024136         0.005008         0.473376          0.883712   \n",
      "1        0.060178         0.011970         0.492589          0.913149   \n",
      "2        0.095356         0.018556         0.501884          0.919258   \n",
      "3        0.131009         0.025559         0.495889          0.920909   \n",
      "4        0.050747         0.005121         0.485578          0.894911   \n",
      "5        0.135085         0.011601         0.484008          0.915978   \n",
      "6        0.215881         0.022047         0.491211          0.916460   \n",
      "7        0.287532         0.025424         0.499552          0.918961   \n",
      "0        0.021495         0.004955         0.455315          0.999327   \n",
      "1        0.055847         0.012153         0.482821          0.999327   \n",
      "2        0.085707         0.020287         0.480219          0.999327   \n",
      "3        0.116703         0.025506         0.482323          0.999327   \n",
      "4        0.051623         0.004975         0.441957          0.999327   \n",
      "5        0.130315         0.011985         0.475367          0.999327   \n",
      "6        0.228170         0.020735         0.485410          0.999327   \n",
      "7        0.326909         0.028547         0.480426          0.999327   \n",
      "0        0.024429         0.005034         0.438331          0.886438   \n",
      "1        0.068368         0.014011         0.417784          0.910506   \n",
      "2        0.120861         0.026287         0.452943          0.910942   \n",
      "3        0.146770         0.026211         0.435130          0.917921   \n",
      "4        0.053327         0.005148         0.424631          0.873036   \n",
      "5        0.132050         0.012251         0.447810          0.904868   \n",
      "6        0.217647         0.023733         0.464478          0.909622   \n",
      "7        0.292883         0.026061         0.443942          0.914943   \n",
      "0        0.022064         0.005022         0.429469          0.999327   \n",
      "1        0.057824         0.011764         0.477087          0.999327   \n",
      "2        0.089325         0.019156         0.447721          0.999327   \n",
      "3        0.121582         0.025687         0.465677          0.999327   \n",
      "4        0.050461         0.005172         0.417820          0.999327   \n",
      "5        0.129415         0.011766         0.455314          0.999327   \n",
      "..            ...              ...              ...               ...   \n",
      "2        0.086584         0.018918         0.472299          0.999327   \n",
      "3        0.127629         0.025684         0.464409          0.999327   \n",
      "4        0.056939         0.005046         0.447631          0.999327   \n",
      "5        0.135677         0.012172         0.453036          0.999327   \n",
      "6        0.201224         0.019013         0.460067          0.999327   \n",
      "7        0.272294         0.025723         0.461594          0.999327   \n",
      "0        0.026928         0.004926         0.529766          0.914276   \n",
      "1        0.067621         0.012016         0.517010          0.919884   \n",
      "2        0.107500         0.018467         0.529725          0.928579   \n",
      "3        0.150285         0.025328         0.529085          0.929993   \n",
      "4        0.079906         0.005054         0.471788          0.908994   \n",
      "5        0.203138         0.012009         0.522322          0.924236   \n",
      "6        0.340299         0.020804         0.514097          0.925083   \n",
      "7        0.507108         0.029291         0.535624          0.929226   \n",
      "0        0.030515         0.008088         0.510087          0.999327   \n",
      "1        0.074109         0.014175         0.532244          0.999327   \n",
      "2        0.108297         0.023410         0.566013          0.999327   \n",
      "3        0.152059         0.029719         0.567674          0.999327   \n",
      "4        0.087587         0.006201         0.524329          0.999327   \n",
      "5        0.234337         0.013297         0.553500          0.999327   \n",
      "6        0.364388         0.020911         0.520476          0.999327   \n",
      "7        0.492291         0.029228         0.546656          0.999327   \n",
      "0        0.030743         0.005673         0.472661          0.904200   \n",
      "1        0.079181         0.014082         0.434305          0.923994   \n",
      "2        0.119693         0.022323         0.465765          0.922783   \n",
      "3        0.159249         0.027426         0.478616          0.925977   \n",
      "4        0.091102         0.005276         0.384041          0.904600   \n",
      "5        0.237513         0.013088         0.471034          0.918397   \n",
      "6        0.383138         0.022711         0.452645          0.926690   \n",
      "7        0.513650         0.028162         0.437374          0.923543   \n",
      "\n",
      "   method_ids param_criterion param_n_estimators  \\\n",
      "0   [0, 0, 1]             mse                 10   \n",
      "1   [0, 0, 1]             mse                 25   \n",
      "2   [0, 0, 1]             mse                 40   \n",
      "3   [0, 0, 1]             mse                 55   \n",
      "4   [0, 0, 1]             mae                 10   \n",
      "5   [0, 0, 1]             mae                 25   \n",
      "6   [0, 0, 1]             mae                 40   \n",
      "7   [0, 0, 1]             mae                 55   \n",
      "0   [0, 0, 2]             mse                 10   \n",
      "1   [0, 0, 2]             mse                 25   \n",
      "2   [0, 0, 2]             mse                 40   \n",
      "3   [0, 0, 2]             mse                 55   \n",
      "4   [0, 0, 2]             mae                 10   \n",
      "5   [0, 0, 2]             mae                 25   \n",
      "6   [0, 0, 2]             mae                 40   \n",
      "7   [0, 0, 2]             mae                 55   \n",
      "0   [0, 1, 1]             mse                 10   \n",
      "1   [0, 1, 1]             mse                 25   \n",
      "2   [0, 1, 1]             mse                 40   \n",
      "3   [0, 1, 1]             mse                 55   \n",
      "4   [0, 1, 1]             mae                 10   \n",
      "5   [0, 1, 1]             mae                 25   \n",
      "6   [0, 1, 1]             mae                 40   \n",
      "7   [0, 1, 1]             mae                 55   \n",
      "0   [0, 1, 2]             mse                 10   \n",
      "1   [0, 1, 2]             mse                 25   \n",
      "2   [0, 1, 2]             mse                 40   \n",
      "3   [0, 1, 2]             mse                 55   \n",
      "4   [0, 1, 2]             mae                 10   \n",
      "5   [0, 1, 2]             mae                 25   \n",
      "..        ...             ...                ...   \n",
      "2   [0, 2, 2]             mse                 40   \n",
      "3   [0, 2, 2]             mse                 55   \n",
      "4   [0, 2, 2]             mae                 10   \n",
      "5   [0, 2, 2]             mae                 25   \n",
      "6   [0, 2, 2]             mae                 40   \n",
      "7   [0, 2, 2]             mae                 55   \n",
      "0   [1, 0, 1]             mse                 10   \n",
      "1   [1, 0, 1]             mse                 25   \n",
      "2   [1, 0, 1]             mse                 40   \n",
      "3   [1, 0, 1]             mse                 55   \n",
      "4   [1, 0, 1]             mae                 10   \n",
      "5   [1, 0, 1]             mae                 25   \n",
      "6   [1, 0, 1]             mae                 40   \n",
      "7   [1, 0, 1]             mae                 55   \n",
      "0   [1, 0, 2]             mse                 10   \n",
      "1   [1, 0, 2]             mse                 25   \n",
      "2   [1, 0, 2]             mse                 40   \n",
      "3   [1, 0, 2]             mse                 55   \n",
      "4   [1, 0, 2]             mae                 10   \n",
      "5   [1, 0, 2]             mae                 25   \n",
      "6   [1, 0, 2]             mae                 40   \n",
      "7   [1, 0, 2]             mae                 55   \n",
      "0   [1, 1, 1]             mse                 10   \n",
      "1   [1, 1, 1]             mse                 25   \n",
      "2   [1, 1, 1]             mse                 40   \n",
      "3   [1, 1, 1]             mse                 55   \n",
      "4   [1, 1, 1]             mae                 10   \n",
      "5   [1, 1, 1]             mae                 25   \n",
      "6   [1, 1, 1]             mae                 40   \n",
      "7   [1, 1, 1]             mae                 55   \n",
      "\n",
      "                                         params  rank_test_score  \\\n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "..                                          ...              ...   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                4   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                3   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                5   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                7   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.532909            0.875786           0.441208   \n",
      "1            0.539835            0.910320           0.514354   \n",
      "2            0.565894            0.912127           0.505728   \n",
      "3            0.544117            0.922500           0.478462   \n",
      "4            0.558931            0.882973           0.483207   \n",
      "5            0.564253            0.916684           0.445223   \n",
      "6            0.573519            0.910791           0.453887   \n",
      "7            0.585932            0.915393           0.504125   \n",
      "0            0.492315            0.999978           0.449155   \n",
      "1            0.542346            0.999978           0.499233   \n",
      "2            0.531688            0.999978           0.510485   \n",
      "3            0.525395            0.999978           0.500564   \n",
      "4            0.497328            0.999978           0.482597   \n",
      "5            0.553961            0.999978           0.466189   \n",
      "6            0.566515            0.999978           0.481667   \n",
      "7            0.552411            0.999978           0.512055   \n",
      "0            0.469139            0.880897           0.496814   \n",
      "1            0.502791            0.902003           0.424643   \n",
      "2            0.542659            0.903799           0.430453   \n",
      "3            0.495867            0.906287           0.422693   \n",
      "4            0.512529            0.869530           0.414054   \n",
      "5            0.493631            0.894170           0.448127   \n",
      "6            0.525079            0.904297           0.460221   \n",
      "7            0.508563            0.905416           0.466128   \n",
      "0            0.531158            0.999978           0.408375   \n",
      "1            0.503439            0.999978           0.525550   \n",
      "2            0.495257            0.999978           0.473424   \n",
      "3            0.532163            0.999978           0.492380   \n",
      "4            0.479440            0.999978           0.423900   \n",
      "5            0.514665            0.999978           0.466658   \n",
      "..                ...                 ...                ...   \n",
      "2            0.564754            0.999978           0.479838   \n",
      "3            0.523986            0.999978           0.492553   \n",
      "4            0.499696            0.999978           0.443762   \n",
      "5            0.505800            0.999978           0.469945   \n",
      "6            0.531864            0.999978           0.487001   \n",
      "7            0.515662            0.999978           0.474997   \n",
      "0            0.593655            0.899050           0.564145   \n",
      "1            0.576332            0.914199           0.574699   \n",
      "2            0.585921            0.932432           0.571995   \n",
      "3            0.569633            0.923452           0.572766   \n",
      "4            0.556942            0.919819           0.520084   \n",
      "5            0.522936            0.929253           0.575438   \n",
      "6            0.567707            0.919558           0.544228   \n",
      "7            0.562966            0.918211           0.582872   \n",
      "0            0.561333            0.999978           0.566889   \n",
      "1            0.613994            0.999978           0.606001   \n",
      "2            0.626742            0.999978           0.612241   \n",
      "3            0.643035            0.999978           0.606921   \n",
      "4            0.595589            0.999978           0.584735   \n",
      "5            0.599834            0.999978           0.619800   \n",
      "6            0.574362            0.999978           0.580725   \n",
      "7            0.610133            0.999978           0.592280   \n",
      "0            0.529877            0.913407           0.474296   \n",
      "1            0.495826            0.919807           0.436976   \n",
      "2            0.491664            0.918327           0.526921   \n",
      "3            0.521132            0.929118           0.535756   \n",
      "4            0.346534            0.896983           0.493151   \n",
      "5            0.518611            0.925493           0.514109   \n",
      "6            0.470176            0.928646           0.515855   \n",
      "7            0.435470            0.929999           0.527282   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.877617           0.445372            0.897734      0.000139   \n",
      "1             0.912061           0.423071            0.917066      0.000326   \n",
      "2             0.923810           0.433343            0.921837      0.000400   \n",
      "3             0.923717           0.464570            0.916511      0.000613   \n",
      "4             0.905834           0.413808            0.895927      0.001724   \n",
      "5             0.912121           0.441685            0.919130      0.001826   \n",
      "6             0.920353           0.445343            0.918235      0.007670   \n",
      "7             0.924390           0.407669            0.917102      0.005413   \n",
      "0             0.998030           0.424076            0.999974      0.000405   \n",
      "1             0.998030           0.406243            0.999974      0.002746   \n",
      "2             0.998030           0.397930            0.999974      0.001071   \n",
      "3             0.998030           0.420547            0.999974      0.000255   \n",
      "4             0.998030           0.345349            0.999974      0.001128   \n",
      "5             0.998030           0.405106            0.999974      0.004279   \n",
      "6             0.998030           0.407175            0.999974      0.022929   \n",
      "7             0.998030           0.376038            0.999974      0.028923   \n",
      "0             0.869641           0.348710            0.908778      0.000299   \n",
      "1             0.914159           0.325005            0.915357      0.004393   \n",
      "2             0.911402           0.384754            0.917626      0.004156   \n",
      "3             0.922294           0.386176            0.925181      0.015858   \n",
      "4             0.870999           0.346366            0.878579      0.001481   \n",
      "5             0.915648           0.401179            0.904786      0.002898   \n",
      "6             0.917081           0.407480            0.907487      0.010971   \n",
      "7             0.921047           0.356440            0.918367      0.006066   \n",
      "0             0.998030           0.347781            0.999974      0.000520   \n",
      "1             0.998030           0.401990            0.999974      0.005516   \n",
      "2             0.998030           0.373970            0.999974      0.002415   \n",
      "3             0.998030           0.371773            0.999974      0.002624   \n",
      "4             0.998030           0.349457            0.999974      0.001997   \n",
      "5             0.998030           0.383980            0.999974      0.000475   \n",
      "..                 ...                ...                 ...           ...   \n",
      "2             0.998030           0.371312            0.999974      0.000149   \n",
      "3             0.998030           0.376048            0.999974      0.012641   \n",
      "4             0.998030           0.398875            0.999974      0.001311   \n",
      "5             0.998030           0.382795            0.999974      0.004922   \n",
      "6             0.998030           0.360563            0.999974      0.007703   \n",
      "7             0.998030           0.393541            0.999974      0.010708   \n",
      "0             0.916170           0.430812            0.927607      0.000247   \n",
      "1             0.916065           0.399362            0.929387      0.000219   \n",
      "2             0.917596           0.430655            0.935709      0.000166   \n",
      "3             0.928583           0.444420            0.937943      0.003182   \n",
      "4             0.906917           0.337422            0.900244      0.001381   \n",
      "5             0.918809           0.468585            0.924647      0.008198   \n",
      "6             0.925269           0.429778            0.930421      0.034664   \n",
      "7             0.933235           0.460740            0.936233      0.016345   \n",
      "0             0.998030           0.401487            0.999974      0.001222   \n",
      "1             0.998030           0.375858            0.999974      0.015076   \n",
      "2             0.998030           0.458402            0.999974      0.012188   \n",
      "3             0.998030           0.452257            0.999974      0.008633   \n",
      "4             0.998030           0.391896            0.999974      0.005510   \n",
      "5             0.998030           0.440368            0.999974      0.008436   \n",
      "6             0.998030           0.405761            0.999974      0.004622   \n",
      "7             0.998030           0.436871            0.999974      0.008887   \n",
      "0             0.874649           0.413195            0.924545      0.000490   \n",
      "1             0.920527           0.369452            0.931648      0.002282   \n",
      "2             0.919070           0.378431            0.930952      0.002355   \n",
      "3             0.921036           0.378503            0.927776      0.007452   \n",
      "4             0.896086           0.312841            0.920732      0.000950   \n",
      "5             0.903653           0.379872            0.926045      0.013425   \n",
      "6             0.920022           0.371715            0.931401      0.034239   \n",
      "7             0.914613           0.349390            0.926017      0.011770   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.000128        0.042355         0.009943  \n",
      "1         0.000056        0.050119         0.002860  \n",
      "2         0.000246        0.054229         0.005106  \n",
      "3         0.000061        0.034750         0.003149  \n",
      "4         0.000112        0.059322         0.009361  \n",
      "5         0.000085        0.057065         0.002905  \n",
      "6         0.004956        0.058616         0.004101  \n",
      "7         0.000275        0.072911         0.003901  \n",
      "0         0.000361        0.028219         0.000917  \n",
      "1         0.000495        0.056804         0.000917  \n",
      "2         0.001730        0.058674         0.000917  \n",
      "3         0.000464        0.044731         0.000917  \n",
      "4         0.000103        0.068394         0.000917  \n",
      "5         0.000121        0.061167         0.000917  \n",
      "6         0.001815        0.065161         0.000917  \n",
      "7         0.003829        0.075440         0.000917  \n",
      "0         0.000216        0.064206         0.016451  \n",
      "1         0.001236        0.072806         0.006032  \n",
      "2         0.002018        0.066442         0.005654  \n",
      "3         0.001032        0.045671         0.008310  \n",
      "4         0.000067        0.068304         0.003965  \n",
      "5         0.000056        0.037777         0.008769  \n",
      "6         0.000500        0.048145         0.005433  \n",
      "7         0.000540        0.064097         0.006825  \n",
      "0         0.000010        0.076391         0.000917  \n",
      "1         0.000275        0.053725         0.000917  \n",
      "2         0.000319        0.052769         0.000917  \n",
      "3         0.000374        0.068188         0.000917  \n",
      "4         0.000168        0.053285         0.000917  \n",
      "5         0.000100        0.053995         0.000917  \n",
      "..             ...             ...              ...  \n",
      "2         0.000085        0.079220         0.000917  \n",
      "3         0.000315        0.063623         0.000917  \n",
      "4         0.000078        0.041286         0.000917  \n",
      "5         0.000161        0.051656         0.000917  \n",
      "6         0.000131        0.072526         0.000917  \n",
      "7         0.000124        0.050787         0.000917  \n",
      "0         0.000115        0.070818         0.011735  \n",
      "1         0.000420        0.082970         0.006763  \n",
      "2         0.000161        0.070097         0.007880  \n",
      "3         0.000340        0.059721         0.005999  \n",
      "4         0.000050        0.095946         0.008125  \n",
      "5         0.000034        0.043547         0.004273  \n",
      "6         0.001539        0.060231         0.004437  \n",
      "7         0.001807        0.053432         0.007885  \n",
      "0         0.001682        0.076619         0.000917  \n",
      "1         0.000813        0.110334         0.000917  \n",
      "2         0.005118        0.076120         0.000917  \n",
      "3         0.000766        0.082721         0.000917  \n",
      "4         0.001011        0.093499         0.000917  \n",
      "5         0.000826        0.080198         0.000917  \n",
      "6         0.001109        0.080940         0.000917  \n",
      "7         0.001584        0.077765         0.000917  \n",
      "0         0.000421        0.047691         0.021385  \n",
      "1         0.001913        0.051672         0.005420  \n",
      "2         0.001147        0.063251         0.005785  \n",
      "3         0.001678        0.070853         0.003536  \n",
      "4         0.000251        0.078168         0.011413  \n",
      "5         0.000608        0.064316         0.010428  \n",
      "6         0.001851        0.060047         0.004847  \n",
      "7         0.001139        0.072507         0.006520  \n",
      "\n",
      "[72 rows x 19 columns]\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.027020         0.005336         0.480873          0.999327   \n",
      "1       0.068788         0.012659         0.486692          0.999327   \n",
      "2       0.107790         0.023480         0.510219          0.999327   \n",
      "3       0.145789         0.027460         0.474844          0.999327   \n",
      "4       0.093866         0.006053         0.443603          0.999327   \n",
      "5       0.241093         0.014588         0.477887          0.999327   \n",
      "6       0.384652         0.022765         0.491559          0.999327   \n",
      "7       0.524458         0.037809         0.466708          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 1, 2]             mse                 10   \n",
      "1  [1, 1, 2]             mse                 25   \n",
      "2  [1, 1, 2]             mse                 40   \n",
      "3  [1, 1, 2]             mse                 55   \n",
      "4  [1, 1, 2]             mae                 10   \n",
      "5  [1, 1, 2]             mae                 25   \n",
      "6  [1, 1, 2]             mae                 40   \n",
      "7  [1, 1, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                4   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                3   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                7   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.551075            0.999978           0.558273   \n",
      "1           0.492013            0.999978           0.553150   \n",
      "2           0.538011            0.999978           0.576530   \n",
      "3           0.525228            0.999978           0.567881   \n",
      "4           0.459749            0.999978           0.535967   \n",
      "5           0.497923            0.999978           0.539135   \n",
      "6           0.499860            0.999978           0.603550   \n",
      "7           0.470177            0.999978           0.577105   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.332517            0.999974      0.001776   \n",
      "1             0.99803           0.414854            0.999974      0.003099   \n",
      "2             0.99803           0.415819            0.999974      0.000128   \n",
      "3             0.99803           0.330883            0.999974      0.002054   \n",
      "4             0.99803           0.334918            0.999974      0.003281   \n",
      "5             0.99803           0.396388            0.999974      0.009429   \n",
      "6             0.99803           0.371179            0.999974      0.015930   \n",
      "7             0.99803           0.352804            0.999974      0.025848   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000272        0.104664         0.000917  \n",
      "1        0.000308        0.056485         0.000917  \n",
      "2        0.001063        0.068408         0.000917  \n",
      "3        0.000370        0.103009         0.000917  \n",
      "4        0.000542        0.082731         0.000917  \n",
      "5        0.001277        0.059890         0.000917  \n",
      "6        0.001539        0.094879         0.000917  \n",
      "7        0.006585        0.091440         0.000917  \n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024136         0.005008         0.473376          0.883712   \n",
      "1        0.060178         0.011970         0.492589          0.913149   \n",
      "2        0.095356         0.018556         0.501884          0.919258   \n",
      "3        0.131009         0.025559         0.495889          0.920909   \n",
      "4        0.050747         0.005121         0.485578          0.894911   \n",
      "5        0.135085         0.011601         0.484008          0.915978   \n",
      "6        0.215881         0.022047         0.491211          0.916460   \n",
      "7        0.287532         0.025424         0.499552          0.918961   \n",
      "0        0.021495         0.004955         0.455315          0.999327   \n",
      "1        0.055847         0.012153         0.482821          0.999327   \n",
      "2        0.085707         0.020287         0.480219          0.999327   \n",
      "3        0.116703         0.025506         0.482323          0.999327   \n",
      "4        0.051623         0.004975         0.441957          0.999327   \n",
      "5        0.130315         0.011985         0.475367          0.999327   \n",
      "6        0.228170         0.020735         0.485410          0.999327   \n",
      "7        0.326909         0.028547         0.480426          0.999327   \n",
      "0        0.024429         0.005034         0.438331          0.886438   \n",
      "1        0.068368         0.014011         0.417784          0.910506   \n",
      "2        0.120861         0.026287         0.452943          0.910942   \n",
      "3        0.146770         0.026211         0.435130          0.917921   \n",
      "4        0.053327         0.005148         0.424631          0.873036   \n",
      "5        0.132050         0.012251         0.447810          0.904868   \n",
      "6        0.217647         0.023733         0.464478          0.909622   \n",
      "7        0.292883         0.026061         0.443942          0.914943   \n",
      "0        0.022064         0.005022         0.429469          0.999327   \n",
      "1        0.057824         0.011764         0.477087          0.999327   \n",
      "2        0.089325         0.019156         0.447721          0.999327   \n",
      "3        0.121582         0.025687         0.465677          0.999327   \n",
      "4        0.050461         0.005172         0.417820          0.999327   \n",
      "5        0.129415         0.011766         0.455314          0.999327   \n",
      "..            ...              ...              ...               ...   \n",
      "2        0.107500         0.018467         0.529725          0.928579   \n",
      "3        0.150285         0.025328         0.529085          0.929993   \n",
      "4        0.079906         0.005054         0.471788          0.908994   \n",
      "5        0.203138         0.012009         0.522322          0.924236   \n",
      "6        0.340299         0.020804         0.514097          0.925083   \n",
      "7        0.507108         0.029291         0.535624          0.929226   \n",
      "0        0.030515         0.008088         0.510087          0.999327   \n",
      "1        0.074109         0.014175         0.532244          0.999327   \n",
      "2        0.108297         0.023410         0.566013          0.999327   \n",
      "3        0.152059         0.029719         0.567674          0.999327   \n",
      "4        0.087587         0.006201         0.524329          0.999327   \n",
      "5        0.234337         0.013297         0.553500          0.999327   \n",
      "6        0.364388         0.020911         0.520476          0.999327   \n",
      "7        0.492291         0.029228         0.546656          0.999327   \n",
      "0        0.030743         0.005673         0.472661          0.904200   \n",
      "1        0.079181         0.014082         0.434305          0.923994   \n",
      "2        0.119693         0.022323         0.465765          0.922783   \n",
      "3        0.159249         0.027426         0.478616          0.925977   \n",
      "4        0.091102         0.005276         0.384041          0.904600   \n",
      "5        0.237513         0.013088         0.471034          0.918397   \n",
      "6        0.383138         0.022711         0.452645          0.926690   \n",
      "7        0.513650         0.028162         0.437374          0.923543   \n",
      "0        0.027020         0.005336         0.480873          0.999327   \n",
      "1        0.068788         0.012659         0.486692          0.999327   \n",
      "2        0.107790         0.023480         0.510219          0.999327   \n",
      "3        0.145789         0.027460         0.474844          0.999327   \n",
      "4        0.093866         0.006053         0.443603          0.999327   \n",
      "5        0.241093         0.014588         0.477887          0.999327   \n",
      "6        0.384652         0.022765         0.491559          0.999327   \n",
      "7        0.524458         0.037809         0.466708          0.999327   \n",
      "\n",
      "   method_ids param_criterion param_n_estimators  \\\n",
      "0   [0, 0, 1]             mse                 10   \n",
      "1   [0, 0, 1]             mse                 25   \n",
      "2   [0, 0, 1]             mse                 40   \n",
      "3   [0, 0, 1]             mse                 55   \n",
      "4   [0, 0, 1]             mae                 10   \n",
      "5   [0, 0, 1]             mae                 25   \n",
      "6   [0, 0, 1]             mae                 40   \n",
      "7   [0, 0, 1]             mae                 55   \n",
      "0   [0, 0, 2]             mse                 10   \n",
      "1   [0, 0, 2]             mse                 25   \n",
      "2   [0, 0, 2]             mse                 40   \n",
      "3   [0, 0, 2]             mse                 55   \n",
      "4   [0, 0, 2]             mae                 10   \n",
      "5   [0, 0, 2]             mae                 25   \n",
      "6   [0, 0, 2]             mae                 40   \n",
      "7   [0, 0, 2]             mae                 55   \n",
      "0   [0, 1, 1]             mse                 10   \n",
      "1   [0, 1, 1]             mse                 25   \n",
      "2   [0, 1, 1]             mse                 40   \n",
      "3   [0, 1, 1]             mse                 55   \n",
      "4   [0, 1, 1]             mae                 10   \n",
      "5   [0, 1, 1]             mae                 25   \n",
      "6   [0, 1, 1]             mae                 40   \n",
      "7   [0, 1, 1]             mae                 55   \n",
      "0   [0, 1, 2]             mse                 10   \n",
      "1   [0, 1, 2]             mse                 25   \n",
      "2   [0, 1, 2]             mse                 40   \n",
      "3   [0, 1, 2]             mse                 55   \n",
      "4   [0, 1, 2]             mae                 10   \n",
      "5   [0, 1, 2]             mae                 25   \n",
      "..        ...             ...                ...   \n",
      "2   [1, 0, 1]             mse                 40   \n",
      "3   [1, 0, 1]             mse                 55   \n",
      "4   [1, 0, 1]             mae                 10   \n",
      "5   [1, 0, 1]             mae                 25   \n",
      "6   [1, 0, 1]             mae                 40   \n",
      "7   [1, 0, 1]             mae                 55   \n",
      "0   [1, 0, 2]             mse                 10   \n",
      "1   [1, 0, 2]             mse                 25   \n",
      "2   [1, 0, 2]             mse                 40   \n",
      "3   [1, 0, 2]             mse                 55   \n",
      "4   [1, 0, 2]             mae                 10   \n",
      "5   [1, 0, 2]             mae                 25   \n",
      "6   [1, 0, 2]             mae                 40   \n",
      "7   [1, 0, 2]             mae                 55   \n",
      "0   [1, 1, 1]             mse                 10   \n",
      "1   [1, 1, 1]             mse                 25   \n",
      "2   [1, 1, 1]             mse                 40   \n",
      "3   [1, 1, 1]             mse                 55   \n",
      "4   [1, 1, 1]             mae                 10   \n",
      "5   [1, 1, 1]             mae                 25   \n",
      "6   [1, 1, 1]             mae                 40   \n",
      "7   [1, 1, 1]             mae                 55   \n",
      "0   [1, 1, 2]             mse                 10   \n",
      "1   [1, 1, 2]             mse                 25   \n",
      "2   [1, 1, 2]             mse                 40   \n",
      "3   [1, 1, 2]             mse                 55   \n",
      "4   [1, 1, 2]             mae                 10   \n",
      "5   [1, 1, 2]             mae                 25   \n",
      "6   [1, 1, 2]             mae                 40   \n",
      "7   [1, 1, 2]             mae                 55   \n",
      "\n",
      "                                         params  rank_test_score  \\\n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "..                                          ...              ...   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                4   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                5   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                7   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                4   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                3   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                7   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.532909            0.875786           0.441208   \n",
      "1            0.539835            0.910320           0.514354   \n",
      "2            0.565894            0.912127           0.505728   \n",
      "3            0.544117            0.922500           0.478462   \n",
      "4            0.558931            0.882973           0.483207   \n",
      "5            0.564253            0.916684           0.445223   \n",
      "6            0.573519            0.910791           0.453887   \n",
      "7            0.585932            0.915393           0.504125   \n",
      "0            0.492315            0.999978           0.449155   \n",
      "1            0.542346            0.999978           0.499233   \n",
      "2            0.531688            0.999978           0.510485   \n",
      "3            0.525395            0.999978           0.500564   \n",
      "4            0.497328            0.999978           0.482597   \n",
      "5            0.553961            0.999978           0.466189   \n",
      "6            0.566515            0.999978           0.481667   \n",
      "7            0.552411            0.999978           0.512055   \n",
      "0            0.469139            0.880897           0.496814   \n",
      "1            0.502791            0.902003           0.424643   \n",
      "2            0.542659            0.903799           0.430453   \n",
      "3            0.495867            0.906287           0.422693   \n",
      "4            0.512529            0.869530           0.414054   \n",
      "5            0.493631            0.894170           0.448127   \n",
      "6            0.525079            0.904297           0.460221   \n",
      "7            0.508563            0.905416           0.466128   \n",
      "0            0.531158            0.999978           0.408375   \n",
      "1            0.503439            0.999978           0.525550   \n",
      "2            0.495257            0.999978           0.473424   \n",
      "3            0.532163            0.999978           0.492380   \n",
      "4            0.479440            0.999978           0.423900   \n",
      "5            0.514665            0.999978           0.466658   \n",
      "..                ...                 ...                ...   \n",
      "2            0.585921            0.932432           0.571995   \n",
      "3            0.569633            0.923452           0.572766   \n",
      "4            0.556942            0.919819           0.520084   \n",
      "5            0.522936            0.929253           0.575438   \n",
      "6            0.567707            0.919558           0.544228   \n",
      "7            0.562966            0.918211           0.582872   \n",
      "0            0.561333            0.999978           0.566889   \n",
      "1            0.613994            0.999978           0.606001   \n",
      "2            0.626742            0.999978           0.612241   \n",
      "3            0.643035            0.999978           0.606921   \n",
      "4            0.595589            0.999978           0.584735   \n",
      "5            0.599834            0.999978           0.619800   \n",
      "6            0.574362            0.999978           0.580725   \n",
      "7            0.610133            0.999978           0.592280   \n",
      "0            0.529877            0.913407           0.474296   \n",
      "1            0.495826            0.919807           0.436976   \n",
      "2            0.491664            0.918327           0.526921   \n",
      "3            0.521132            0.929118           0.535756   \n",
      "4            0.346534            0.896983           0.493151   \n",
      "5            0.518611            0.925493           0.514109   \n",
      "6            0.470176            0.928646           0.515855   \n",
      "7            0.435470            0.929999           0.527282   \n",
      "0            0.551075            0.999978           0.558273   \n",
      "1            0.492013            0.999978           0.553150   \n",
      "2            0.538011            0.999978           0.576530   \n",
      "3            0.525228            0.999978           0.567881   \n",
      "4            0.459749            0.999978           0.535967   \n",
      "5            0.497923            0.999978           0.539135   \n",
      "6            0.499860            0.999978           0.603550   \n",
      "7            0.470177            0.999978           0.577105   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.877617           0.445372            0.897734      0.000139   \n",
      "1             0.912061           0.423071            0.917066      0.000326   \n",
      "2             0.923810           0.433343            0.921837      0.000400   \n",
      "3             0.923717           0.464570            0.916511      0.000613   \n",
      "4             0.905834           0.413808            0.895927      0.001724   \n",
      "5             0.912121           0.441685            0.919130      0.001826   \n",
      "6             0.920353           0.445343            0.918235      0.007670   \n",
      "7             0.924390           0.407669            0.917102      0.005413   \n",
      "0             0.998030           0.424076            0.999974      0.000405   \n",
      "1             0.998030           0.406243            0.999974      0.002746   \n",
      "2             0.998030           0.397930            0.999974      0.001071   \n",
      "3             0.998030           0.420547            0.999974      0.000255   \n",
      "4             0.998030           0.345349            0.999974      0.001128   \n",
      "5             0.998030           0.405106            0.999974      0.004279   \n",
      "6             0.998030           0.407175            0.999974      0.022929   \n",
      "7             0.998030           0.376038            0.999974      0.028923   \n",
      "0             0.869641           0.348710            0.908778      0.000299   \n",
      "1             0.914159           0.325005            0.915357      0.004393   \n",
      "2             0.911402           0.384754            0.917626      0.004156   \n",
      "3             0.922294           0.386176            0.925181      0.015858   \n",
      "4             0.870999           0.346366            0.878579      0.001481   \n",
      "5             0.915648           0.401179            0.904786      0.002898   \n",
      "6             0.917081           0.407480            0.907487      0.010971   \n",
      "7             0.921047           0.356440            0.918367      0.006066   \n",
      "0             0.998030           0.347781            0.999974      0.000520   \n",
      "1             0.998030           0.401990            0.999974      0.005516   \n",
      "2             0.998030           0.373970            0.999974      0.002415   \n",
      "3             0.998030           0.371773            0.999974      0.002624   \n",
      "4             0.998030           0.349457            0.999974      0.001997   \n",
      "5             0.998030           0.383980            0.999974      0.000475   \n",
      "..                 ...                ...                 ...           ...   \n",
      "2             0.917596           0.430655            0.935709      0.000166   \n",
      "3             0.928583           0.444420            0.937943      0.003182   \n",
      "4             0.906917           0.337422            0.900244      0.001381   \n",
      "5             0.918809           0.468585            0.924647      0.008198   \n",
      "6             0.925269           0.429778            0.930421      0.034664   \n",
      "7             0.933235           0.460740            0.936233      0.016345   \n",
      "0             0.998030           0.401487            0.999974      0.001222   \n",
      "1             0.998030           0.375858            0.999974      0.015076   \n",
      "2             0.998030           0.458402            0.999974      0.012188   \n",
      "3             0.998030           0.452257            0.999974      0.008633   \n",
      "4             0.998030           0.391896            0.999974      0.005510   \n",
      "5             0.998030           0.440368            0.999974      0.008436   \n",
      "6             0.998030           0.405761            0.999974      0.004622   \n",
      "7             0.998030           0.436871            0.999974      0.008887   \n",
      "0             0.874649           0.413195            0.924545      0.000490   \n",
      "1             0.920527           0.369452            0.931648      0.002282   \n",
      "2             0.919070           0.378431            0.930952      0.002355   \n",
      "3             0.921036           0.378503            0.927776      0.007452   \n",
      "4             0.896086           0.312841            0.920732      0.000950   \n",
      "5             0.903653           0.379872            0.926045      0.013425   \n",
      "6             0.920022           0.371715            0.931401      0.034239   \n",
      "7             0.914613           0.349390            0.926017      0.011770   \n",
      "0             0.998030           0.332517            0.999974      0.001776   \n",
      "1             0.998030           0.414854            0.999974      0.003099   \n",
      "2             0.998030           0.415819            0.999974      0.000128   \n",
      "3             0.998030           0.330883            0.999974      0.002054   \n",
      "4             0.998030           0.334918            0.999974      0.003281   \n",
      "5             0.998030           0.396388            0.999974      0.009429   \n",
      "6             0.998030           0.371179            0.999974      0.015930   \n",
      "7             0.998030           0.352804            0.999974      0.025848   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.000128        0.042355         0.009943  \n",
      "1         0.000056        0.050119         0.002860  \n",
      "2         0.000246        0.054229         0.005106  \n",
      "3         0.000061        0.034750         0.003149  \n",
      "4         0.000112        0.059322         0.009361  \n",
      "5         0.000085        0.057065         0.002905  \n",
      "6         0.004956        0.058616         0.004101  \n",
      "7         0.000275        0.072911         0.003901  \n",
      "0         0.000361        0.028219         0.000917  \n",
      "1         0.000495        0.056804         0.000917  \n",
      "2         0.001730        0.058674         0.000917  \n",
      "3         0.000464        0.044731         0.000917  \n",
      "4         0.000103        0.068394         0.000917  \n",
      "5         0.000121        0.061167         0.000917  \n",
      "6         0.001815        0.065161         0.000917  \n",
      "7         0.003829        0.075440         0.000917  \n",
      "0         0.000216        0.064206         0.016451  \n",
      "1         0.001236        0.072806         0.006032  \n",
      "2         0.002018        0.066442         0.005654  \n",
      "3         0.001032        0.045671         0.008310  \n",
      "4         0.000067        0.068304         0.003965  \n",
      "5         0.000056        0.037777         0.008769  \n",
      "6         0.000500        0.048145         0.005433  \n",
      "7         0.000540        0.064097         0.006825  \n",
      "0         0.000010        0.076391         0.000917  \n",
      "1         0.000275        0.053725         0.000917  \n",
      "2         0.000319        0.052769         0.000917  \n",
      "3         0.000374        0.068188         0.000917  \n",
      "4         0.000168        0.053285         0.000917  \n",
      "5         0.000100        0.053995         0.000917  \n",
      "..             ...             ...              ...  \n",
      "2         0.000161        0.070097         0.007880  \n",
      "3         0.000340        0.059721         0.005999  \n",
      "4         0.000050        0.095946         0.008125  \n",
      "5         0.000034        0.043547         0.004273  \n",
      "6         0.001539        0.060231         0.004437  \n",
      "7         0.001807        0.053432         0.007885  \n",
      "0         0.001682        0.076619         0.000917  \n",
      "1         0.000813        0.110334         0.000917  \n",
      "2         0.005118        0.076120         0.000917  \n",
      "3         0.000766        0.082721         0.000917  \n",
      "4         0.001011        0.093499         0.000917  \n",
      "5         0.000826        0.080198         0.000917  \n",
      "6         0.001109        0.080940         0.000917  \n",
      "7         0.001584        0.077765         0.000917  \n",
      "0         0.000421        0.047691         0.021385  \n",
      "1         0.001913        0.051672         0.005420  \n",
      "2         0.001147        0.063251         0.005785  \n",
      "3         0.001678        0.070853         0.003536  \n",
      "4         0.000251        0.078168         0.011413  \n",
      "5         0.000608        0.064316         0.010428  \n",
      "6         0.001851        0.060047         0.004847  \n",
      "7         0.001139        0.072507         0.006520  \n",
      "0         0.000272        0.104664         0.000917  \n",
      "1         0.000308        0.056485         0.000917  \n",
      "2         0.001063        0.068408         0.000917  \n",
      "3         0.000370        0.103009         0.000917  \n",
      "4         0.000542        0.082731         0.000917  \n",
      "5         0.001277        0.059890         0.000917  \n",
      "6         0.001539        0.094879         0.000917  \n",
      "7         0.006585        0.091440         0.000917  \n",
      "\n",
      "[80 rows x 19 columns]\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.027670         0.005471         0.572118          0.930895   \n",
      "1       0.087699         0.014203         0.586321          0.934560   \n",
      "2       0.123745         0.021479         0.615974          0.941097   \n",
      "3       0.156221         0.028420         0.617331          0.942951   \n",
      "4       0.076248         0.005222         0.563950          0.923866   \n",
      "5       0.199907         0.014017         0.608719          0.934413   \n",
      "6       0.300280         0.020285         0.588369          0.936139   \n",
      "7       0.439856         0.027308         0.618587          0.940927   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 2, 1]             mse                 10   \n",
      "1  [1, 2, 1]             mse                 25   \n",
      "2  [1, 2, 1]             mse                 40   \n",
      "3  [1, 2, 1]             mse                 55   \n",
      "4  [1, 2, 1]             mae                 10   \n",
      "5  [1, 2, 1]             mae                 25   \n",
      "6  [1, 2, 1]             mae                 40   \n",
      "7  [1, 2, 1]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                4   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.641078            0.922661           0.549063   \n",
      "1           0.696969            0.929801           0.558220   \n",
      "2           0.693874            0.929307           0.626593   \n",
      "3           0.715424            0.934945           0.579221   \n",
      "4           0.582810            0.903686           0.594502   \n",
      "5           0.635584            0.933998           0.660207   \n",
      "6           0.655826            0.923715           0.571048   \n",
      "7           0.662526            0.932603           0.615139   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.937908           0.525471            0.932116      0.001447   \n",
      "1            0.924635           0.502585            0.949243      0.003171   \n",
      "2            0.941446           0.526619            0.952537      0.006429   \n",
      "3            0.947193           0.556294            0.946716      0.006242   \n",
      "4            0.928826           0.514336            0.939085      0.005195   \n",
      "5            0.920911           0.530077            0.948331      0.012959   \n",
      "6            0.933144           0.537507            0.951558      0.012898   \n",
      "7            0.945497           0.577623            0.944681      0.019597   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000494        0.049958         0.006284  \n",
      "1        0.000462        0.081862         0.010595  \n",
      "2        0.003365        0.068751         0.009487  \n",
      "3        0.001802        0.070357         0.005665  \n",
      "4        0.000098        0.035313         0.014871  \n",
      "5        0.000620        0.056365         0.011198  \n",
      "6        0.001610        0.049865         0.011562  \n",
      "7        0.000795        0.034777         0.005895  \n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024136         0.005008         0.473376          0.883712   \n",
      "1        0.060178         0.011970         0.492589          0.913149   \n",
      "2        0.095356         0.018556         0.501884          0.919258   \n",
      "3        0.131009         0.025559         0.495889          0.920909   \n",
      "4        0.050747         0.005121         0.485578          0.894911   \n",
      "5        0.135085         0.011601         0.484008          0.915978   \n",
      "6        0.215881         0.022047         0.491211          0.916460   \n",
      "7        0.287532         0.025424         0.499552          0.918961   \n",
      "0        0.021495         0.004955         0.455315          0.999327   \n",
      "1        0.055847         0.012153         0.482821          0.999327   \n",
      "2        0.085707         0.020287         0.480219          0.999327   \n",
      "3        0.116703         0.025506         0.482323          0.999327   \n",
      "4        0.051623         0.004975         0.441957          0.999327   \n",
      "5        0.130315         0.011985         0.475367          0.999327   \n",
      "6        0.228170         0.020735         0.485410          0.999327   \n",
      "7        0.326909         0.028547         0.480426          0.999327   \n",
      "0        0.024429         0.005034         0.438331          0.886438   \n",
      "1        0.068368         0.014011         0.417784          0.910506   \n",
      "2        0.120861         0.026287         0.452943          0.910942   \n",
      "3        0.146770         0.026211         0.435130          0.917921   \n",
      "4        0.053327         0.005148         0.424631          0.873036   \n",
      "5        0.132050         0.012251         0.447810          0.904868   \n",
      "6        0.217647         0.023733         0.464478          0.909622   \n",
      "7        0.292883         0.026061         0.443942          0.914943   \n",
      "0        0.022064         0.005022         0.429469          0.999327   \n",
      "1        0.057824         0.011764         0.477087          0.999327   \n",
      "2        0.089325         0.019156         0.447721          0.999327   \n",
      "3        0.121582         0.025687         0.465677          0.999327   \n",
      "4        0.050461         0.005172         0.417820          0.999327   \n",
      "5        0.129415         0.011766         0.455314          0.999327   \n",
      "..            ...              ...              ...               ...   \n",
      "2        0.108297         0.023410         0.566013          0.999327   \n",
      "3        0.152059         0.029719         0.567674          0.999327   \n",
      "4        0.087587         0.006201         0.524329          0.999327   \n",
      "5        0.234337         0.013297         0.553500          0.999327   \n",
      "6        0.364388         0.020911         0.520476          0.999327   \n",
      "7        0.492291         0.029228         0.546656          0.999327   \n",
      "0        0.030743         0.005673         0.472661          0.904200   \n",
      "1        0.079181         0.014082         0.434305          0.923994   \n",
      "2        0.119693         0.022323         0.465765          0.922783   \n",
      "3        0.159249         0.027426         0.478616          0.925977   \n",
      "4        0.091102         0.005276         0.384041          0.904600   \n",
      "5        0.237513         0.013088         0.471034          0.918397   \n",
      "6        0.383138         0.022711         0.452645          0.926690   \n",
      "7        0.513650         0.028162         0.437374          0.923543   \n",
      "0        0.027020         0.005336         0.480873          0.999327   \n",
      "1        0.068788         0.012659         0.486692          0.999327   \n",
      "2        0.107790         0.023480         0.510219          0.999327   \n",
      "3        0.145789         0.027460         0.474844          0.999327   \n",
      "4        0.093866         0.006053         0.443603          0.999327   \n",
      "5        0.241093         0.014588         0.477887          0.999327   \n",
      "6        0.384652         0.022765         0.491559          0.999327   \n",
      "7        0.524458         0.037809         0.466708          0.999327   \n",
      "0        0.027670         0.005471         0.572118          0.930895   \n",
      "1        0.087699         0.014203         0.586321          0.934560   \n",
      "2        0.123745         0.021479         0.615974          0.941097   \n",
      "3        0.156221         0.028420         0.617331          0.942951   \n",
      "4        0.076248         0.005222         0.563950          0.923866   \n",
      "5        0.199907         0.014017         0.608719          0.934413   \n",
      "6        0.300280         0.020285         0.588369          0.936139   \n",
      "7        0.439856         0.027308         0.618587          0.940927   \n",
      "\n",
      "   method_ids param_criterion param_n_estimators  \\\n",
      "0   [0, 0, 1]             mse                 10   \n",
      "1   [0, 0, 1]             mse                 25   \n",
      "2   [0, 0, 1]             mse                 40   \n",
      "3   [0, 0, 1]             mse                 55   \n",
      "4   [0, 0, 1]             mae                 10   \n",
      "5   [0, 0, 1]             mae                 25   \n",
      "6   [0, 0, 1]             mae                 40   \n",
      "7   [0, 0, 1]             mae                 55   \n",
      "0   [0, 0, 2]             mse                 10   \n",
      "1   [0, 0, 2]             mse                 25   \n",
      "2   [0, 0, 2]             mse                 40   \n",
      "3   [0, 0, 2]             mse                 55   \n",
      "4   [0, 0, 2]             mae                 10   \n",
      "5   [0, 0, 2]             mae                 25   \n",
      "6   [0, 0, 2]             mae                 40   \n",
      "7   [0, 0, 2]             mae                 55   \n",
      "0   [0, 1, 1]             mse                 10   \n",
      "1   [0, 1, 1]             mse                 25   \n",
      "2   [0, 1, 1]             mse                 40   \n",
      "3   [0, 1, 1]             mse                 55   \n",
      "4   [0, 1, 1]             mae                 10   \n",
      "5   [0, 1, 1]             mae                 25   \n",
      "6   [0, 1, 1]             mae                 40   \n",
      "7   [0, 1, 1]             mae                 55   \n",
      "0   [0, 1, 2]             mse                 10   \n",
      "1   [0, 1, 2]             mse                 25   \n",
      "2   [0, 1, 2]             mse                 40   \n",
      "3   [0, 1, 2]             mse                 55   \n",
      "4   [0, 1, 2]             mae                 10   \n",
      "5   [0, 1, 2]             mae                 25   \n",
      "..        ...             ...                ...   \n",
      "2   [1, 0, 2]             mse                 40   \n",
      "3   [1, 0, 2]             mse                 55   \n",
      "4   [1, 0, 2]             mae                 10   \n",
      "5   [1, 0, 2]             mae                 25   \n",
      "6   [1, 0, 2]             mae                 40   \n",
      "7   [1, 0, 2]             mae                 55   \n",
      "0   [1, 1, 1]             mse                 10   \n",
      "1   [1, 1, 1]             mse                 25   \n",
      "2   [1, 1, 1]             mse                 40   \n",
      "3   [1, 1, 1]             mse                 55   \n",
      "4   [1, 1, 1]             mae                 10   \n",
      "5   [1, 1, 1]             mae                 25   \n",
      "6   [1, 1, 1]             mae                 40   \n",
      "7   [1, 1, 1]             mae                 55   \n",
      "0   [1, 1, 2]             mse                 10   \n",
      "1   [1, 1, 2]             mse                 25   \n",
      "2   [1, 1, 2]             mse                 40   \n",
      "3   [1, 1, 2]             mse                 55   \n",
      "4   [1, 1, 2]             mae                 10   \n",
      "5   [1, 1, 2]             mae                 25   \n",
      "6   [1, 1, 2]             mae                 40   \n",
      "7   [1, 1, 2]             mae                 55   \n",
      "0   [1, 2, 1]             mse                 10   \n",
      "1   [1, 2, 1]             mse                 25   \n",
      "2   [1, 2, 1]             mse                 40   \n",
      "3   [1, 2, 1]             mse                 55   \n",
      "4   [1, 2, 1]             mae                 10   \n",
      "5   [1, 2, 1]             mae                 25   \n",
      "6   [1, 2, 1]             mae                 40   \n",
      "7   [1, 2, 1]             mae                 55   \n",
      "\n",
      "                                         params  rank_test_score  \\\n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "..                                          ...              ...   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                7   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                2   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                7   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                4   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                3   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                7   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                4   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.532909            0.875786           0.441208   \n",
      "1            0.539835            0.910320           0.514354   \n",
      "2            0.565894            0.912127           0.505728   \n",
      "3            0.544117            0.922500           0.478462   \n",
      "4            0.558931            0.882973           0.483207   \n",
      "5            0.564253            0.916684           0.445223   \n",
      "6            0.573519            0.910791           0.453887   \n",
      "7            0.585932            0.915393           0.504125   \n",
      "0            0.492315            0.999978           0.449155   \n",
      "1            0.542346            0.999978           0.499233   \n",
      "2            0.531688            0.999978           0.510485   \n",
      "3            0.525395            0.999978           0.500564   \n",
      "4            0.497328            0.999978           0.482597   \n",
      "5            0.553961            0.999978           0.466189   \n",
      "6            0.566515            0.999978           0.481667   \n",
      "7            0.552411            0.999978           0.512055   \n",
      "0            0.469139            0.880897           0.496814   \n",
      "1            0.502791            0.902003           0.424643   \n",
      "2            0.542659            0.903799           0.430453   \n",
      "3            0.495867            0.906287           0.422693   \n",
      "4            0.512529            0.869530           0.414054   \n",
      "5            0.493631            0.894170           0.448127   \n",
      "6            0.525079            0.904297           0.460221   \n",
      "7            0.508563            0.905416           0.466128   \n",
      "0            0.531158            0.999978           0.408375   \n",
      "1            0.503439            0.999978           0.525550   \n",
      "2            0.495257            0.999978           0.473424   \n",
      "3            0.532163            0.999978           0.492380   \n",
      "4            0.479440            0.999978           0.423900   \n",
      "5            0.514665            0.999978           0.466658   \n",
      "..                ...                 ...                ...   \n",
      "2            0.626742            0.999978           0.612241   \n",
      "3            0.643035            0.999978           0.606921   \n",
      "4            0.595589            0.999978           0.584735   \n",
      "5            0.599834            0.999978           0.619800   \n",
      "6            0.574362            0.999978           0.580725   \n",
      "7            0.610133            0.999978           0.592280   \n",
      "0            0.529877            0.913407           0.474296   \n",
      "1            0.495826            0.919807           0.436976   \n",
      "2            0.491664            0.918327           0.526921   \n",
      "3            0.521132            0.929118           0.535756   \n",
      "4            0.346534            0.896983           0.493151   \n",
      "5            0.518611            0.925493           0.514109   \n",
      "6            0.470176            0.928646           0.515855   \n",
      "7            0.435470            0.929999           0.527282   \n",
      "0            0.551075            0.999978           0.558273   \n",
      "1            0.492013            0.999978           0.553150   \n",
      "2            0.538011            0.999978           0.576530   \n",
      "3            0.525228            0.999978           0.567881   \n",
      "4            0.459749            0.999978           0.535967   \n",
      "5            0.497923            0.999978           0.539135   \n",
      "6            0.499860            0.999978           0.603550   \n",
      "7            0.470177            0.999978           0.577105   \n",
      "0            0.641078            0.922661           0.549063   \n",
      "1            0.696969            0.929801           0.558220   \n",
      "2            0.693874            0.929307           0.626593   \n",
      "3            0.715424            0.934945           0.579221   \n",
      "4            0.582810            0.903686           0.594502   \n",
      "5            0.635584            0.933998           0.660207   \n",
      "6            0.655826            0.923715           0.571048   \n",
      "7            0.662526            0.932603           0.615139   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.877617           0.445372            0.897734      0.000139   \n",
      "1             0.912061           0.423071            0.917066      0.000326   \n",
      "2             0.923810           0.433343            0.921837      0.000400   \n",
      "3             0.923717           0.464570            0.916511      0.000613   \n",
      "4             0.905834           0.413808            0.895927      0.001724   \n",
      "5             0.912121           0.441685            0.919130      0.001826   \n",
      "6             0.920353           0.445343            0.918235      0.007670   \n",
      "7             0.924390           0.407669            0.917102      0.005413   \n",
      "0             0.998030           0.424076            0.999974      0.000405   \n",
      "1             0.998030           0.406243            0.999974      0.002746   \n",
      "2             0.998030           0.397930            0.999974      0.001071   \n",
      "3             0.998030           0.420547            0.999974      0.000255   \n",
      "4             0.998030           0.345349            0.999974      0.001128   \n",
      "5             0.998030           0.405106            0.999974      0.004279   \n",
      "6             0.998030           0.407175            0.999974      0.022929   \n",
      "7             0.998030           0.376038            0.999974      0.028923   \n",
      "0             0.869641           0.348710            0.908778      0.000299   \n",
      "1             0.914159           0.325005            0.915357      0.004393   \n",
      "2             0.911402           0.384754            0.917626      0.004156   \n",
      "3             0.922294           0.386176            0.925181      0.015858   \n",
      "4             0.870999           0.346366            0.878579      0.001481   \n",
      "5             0.915648           0.401179            0.904786      0.002898   \n",
      "6             0.917081           0.407480            0.907487      0.010971   \n",
      "7             0.921047           0.356440            0.918367      0.006066   \n",
      "0             0.998030           0.347781            0.999974      0.000520   \n",
      "1             0.998030           0.401990            0.999974      0.005516   \n",
      "2             0.998030           0.373970            0.999974      0.002415   \n",
      "3             0.998030           0.371773            0.999974      0.002624   \n",
      "4             0.998030           0.349457            0.999974      0.001997   \n",
      "5             0.998030           0.383980            0.999974      0.000475   \n",
      "..                 ...                ...                 ...           ...   \n",
      "2             0.998030           0.458402            0.999974      0.012188   \n",
      "3             0.998030           0.452257            0.999974      0.008633   \n",
      "4             0.998030           0.391896            0.999974      0.005510   \n",
      "5             0.998030           0.440368            0.999974      0.008436   \n",
      "6             0.998030           0.405761            0.999974      0.004622   \n",
      "7             0.998030           0.436871            0.999974      0.008887   \n",
      "0             0.874649           0.413195            0.924545      0.000490   \n",
      "1             0.920527           0.369452            0.931648      0.002282   \n",
      "2             0.919070           0.378431            0.930952      0.002355   \n",
      "3             0.921036           0.378503            0.927776      0.007452   \n",
      "4             0.896086           0.312841            0.920732      0.000950   \n",
      "5             0.903653           0.379872            0.926045      0.013425   \n",
      "6             0.920022           0.371715            0.931401      0.034239   \n",
      "7             0.914613           0.349390            0.926017      0.011770   \n",
      "0             0.998030           0.332517            0.999974      0.001776   \n",
      "1             0.998030           0.414854            0.999974      0.003099   \n",
      "2             0.998030           0.415819            0.999974      0.000128   \n",
      "3             0.998030           0.330883            0.999974      0.002054   \n",
      "4             0.998030           0.334918            0.999974      0.003281   \n",
      "5             0.998030           0.396388            0.999974      0.009429   \n",
      "6             0.998030           0.371179            0.999974      0.015930   \n",
      "7             0.998030           0.352804            0.999974      0.025848   \n",
      "0             0.937908           0.525471            0.932116      0.001447   \n",
      "1             0.924635           0.502585            0.949243      0.003171   \n",
      "2             0.941446           0.526619            0.952537      0.006429   \n",
      "3             0.947193           0.556294            0.946716      0.006242   \n",
      "4             0.928826           0.514336            0.939085      0.005195   \n",
      "5             0.920911           0.530077            0.948331      0.012959   \n",
      "6             0.933144           0.537507            0.951558      0.012898   \n",
      "7             0.945497           0.577623            0.944681      0.019597   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.000128        0.042355         0.009943  \n",
      "1         0.000056        0.050119         0.002860  \n",
      "2         0.000246        0.054229         0.005106  \n",
      "3         0.000061        0.034750         0.003149  \n",
      "4         0.000112        0.059322         0.009361  \n",
      "5         0.000085        0.057065         0.002905  \n",
      "6         0.004956        0.058616         0.004101  \n",
      "7         0.000275        0.072911         0.003901  \n",
      "0         0.000361        0.028219         0.000917  \n",
      "1         0.000495        0.056804         0.000917  \n",
      "2         0.001730        0.058674         0.000917  \n",
      "3         0.000464        0.044731         0.000917  \n",
      "4         0.000103        0.068394         0.000917  \n",
      "5         0.000121        0.061167         0.000917  \n",
      "6         0.001815        0.065161         0.000917  \n",
      "7         0.003829        0.075440         0.000917  \n",
      "0         0.000216        0.064206         0.016451  \n",
      "1         0.001236        0.072806         0.006032  \n",
      "2         0.002018        0.066442         0.005654  \n",
      "3         0.001032        0.045671         0.008310  \n",
      "4         0.000067        0.068304         0.003965  \n",
      "5         0.000056        0.037777         0.008769  \n",
      "6         0.000500        0.048145         0.005433  \n",
      "7         0.000540        0.064097         0.006825  \n",
      "0         0.000010        0.076391         0.000917  \n",
      "1         0.000275        0.053725         0.000917  \n",
      "2         0.000319        0.052769         0.000917  \n",
      "3         0.000374        0.068188         0.000917  \n",
      "4         0.000168        0.053285         0.000917  \n",
      "5         0.000100        0.053995         0.000917  \n",
      "..             ...             ...              ...  \n",
      "2         0.005118        0.076120         0.000917  \n",
      "3         0.000766        0.082721         0.000917  \n",
      "4         0.001011        0.093499         0.000917  \n",
      "5         0.000826        0.080198         0.000917  \n",
      "6         0.001109        0.080940         0.000917  \n",
      "7         0.001584        0.077765         0.000917  \n",
      "0         0.000421        0.047691         0.021385  \n",
      "1         0.001913        0.051672         0.005420  \n",
      "2         0.001147        0.063251         0.005785  \n",
      "3         0.001678        0.070853         0.003536  \n",
      "4         0.000251        0.078168         0.011413  \n",
      "5         0.000608        0.064316         0.010428  \n",
      "6         0.001851        0.060047         0.004847  \n",
      "7         0.001139        0.072507         0.006520  \n",
      "0         0.000272        0.104664         0.000917  \n",
      "1         0.000308        0.056485         0.000917  \n",
      "2         0.001063        0.068408         0.000917  \n",
      "3         0.000370        0.103009         0.000917  \n",
      "4         0.000542        0.082731         0.000917  \n",
      "5         0.001277        0.059890         0.000917  \n",
      "6         0.001539        0.094879         0.000917  \n",
      "7         0.006585        0.091440         0.000917  \n",
      "0         0.000494        0.049958         0.006284  \n",
      "1         0.000462        0.081862         0.010595  \n",
      "2         0.003365        0.068751         0.009487  \n",
      "3         0.001802        0.070357         0.005665  \n",
      "4         0.000098        0.035313         0.014871  \n",
      "5         0.000620        0.056365         0.011198  \n",
      "6         0.001610        0.049865         0.011562  \n",
      "7         0.000795        0.034777         0.005895  \n",
      "\n",
      "[88 rows x 19 columns]\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.027211         0.006327         0.563069          0.999327   \n",
      "1       0.068220         0.012536         0.593994          0.999327   \n",
      "2       0.103084         0.021112         0.610592          0.999327   \n",
      "3       0.139993         0.027920         0.619020          0.999327   \n",
      "4       0.084314         0.005619         0.598731          0.999327   \n",
      "5       0.210423         0.014739         0.577511          0.999327   \n",
      "6       0.335544         0.019676         0.621732          0.999327   \n",
      "7       0.459420         0.029228         0.636866          0.999327   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [1, 2, 2]             mse                 10   \n",
      "1  [1, 2, 2]             mse                 25   \n",
      "2  [1, 2, 2]             mse                 40   \n",
      "3  [1, 2, 2]             mse                 55   \n",
      "4  [1, 2, 2]             mae                 10   \n",
      "5  [1, 2, 2]             mae                 25   \n",
      "6  [1, 2, 2]             mae                 40   \n",
      "7  [1, 2, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                5   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.629988            0.999978           0.576435   \n",
      "1           0.674517            0.999978           0.600672   \n",
      "2           0.675135            0.999978           0.610547   \n",
      "3           0.668569            0.999978           0.638537   \n",
      "4           0.639206            0.999978           0.601776   \n",
      "5           0.671600            0.999978           0.525742   \n",
      "6           0.667524            0.999978           0.648800   \n",
      "7           0.677133            0.999978           0.666931   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.99803           0.482063            0.999974      0.001330   \n",
      "1             0.99803           0.505927            0.999974      0.001923   \n",
      "2             0.99803           0.545401            0.999974      0.003110   \n",
      "3             0.99803           0.549420            0.999974      0.004264   \n",
      "4             0.99803           0.554777            0.999974      0.003891   \n",
      "5             0.99803           0.534180            0.999974      0.006413   \n",
      "6             0.99803           0.548378            0.999974      0.010716   \n",
      "7             0.99803           0.566100            0.999974      0.016674   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.001287        0.061174         0.000917  \n",
      "1        0.000309        0.069048         0.000917  \n",
      "2        0.000726        0.053010         0.000917  \n",
      "3        0.001138        0.050593         0.000917  \n",
      "4        0.000318        0.034565         0.000917  \n",
      "5        0.000752        0.066976         0.000917  \n",
      "6        0.000828        0.052292         0.000917  \n",
      "7        0.000592        0.050079         0.000917  \n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024136         0.005008         0.473376          0.883712   \n",
      "1        0.060178         0.011970         0.492589          0.913149   \n",
      "2        0.095356         0.018556         0.501884          0.919258   \n",
      "3        0.131009         0.025559         0.495889          0.920909   \n",
      "4        0.050747         0.005121         0.485578          0.894911   \n",
      "5        0.135085         0.011601         0.484008          0.915978   \n",
      "6        0.215881         0.022047         0.491211          0.916460   \n",
      "7        0.287532         0.025424         0.499552          0.918961   \n",
      "0        0.021495         0.004955         0.455315          0.999327   \n",
      "1        0.055847         0.012153         0.482821          0.999327   \n",
      "2        0.085707         0.020287         0.480219          0.999327   \n",
      "3        0.116703         0.025506         0.482323          0.999327   \n",
      "4        0.051623         0.004975         0.441957          0.999327   \n",
      "5        0.130315         0.011985         0.475367          0.999327   \n",
      "6        0.228170         0.020735         0.485410          0.999327   \n",
      "7        0.326909         0.028547         0.480426          0.999327   \n",
      "0        0.024429         0.005034         0.438331          0.886438   \n",
      "1        0.068368         0.014011         0.417784          0.910506   \n",
      "2        0.120861         0.026287         0.452943          0.910942   \n",
      "3        0.146770         0.026211         0.435130          0.917921   \n",
      "4        0.053327         0.005148         0.424631          0.873036   \n",
      "5        0.132050         0.012251         0.447810          0.904868   \n",
      "6        0.217647         0.023733         0.464478          0.909622   \n",
      "7        0.292883         0.026061         0.443942          0.914943   \n",
      "0        0.022064         0.005022         0.429469          0.999327   \n",
      "1        0.057824         0.011764         0.477087          0.999327   \n",
      "2        0.089325         0.019156         0.447721          0.999327   \n",
      "3        0.121582         0.025687         0.465677          0.999327   \n",
      "4        0.050461         0.005172         0.417820          0.999327   \n",
      "5        0.129415         0.011766         0.455314          0.999327   \n",
      "..            ...              ...              ...               ...   \n",
      "2        0.119693         0.022323         0.465765          0.922783   \n",
      "3        0.159249         0.027426         0.478616          0.925977   \n",
      "4        0.091102         0.005276         0.384041          0.904600   \n",
      "5        0.237513         0.013088         0.471034          0.918397   \n",
      "6        0.383138         0.022711         0.452645          0.926690   \n",
      "7        0.513650         0.028162         0.437374          0.923543   \n",
      "0        0.027020         0.005336         0.480873          0.999327   \n",
      "1        0.068788         0.012659         0.486692          0.999327   \n",
      "2        0.107790         0.023480         0.510219          0.999327   \n",
      "3        0.145789         0.027460         0.474844          0.999327   \n",
      "4        0.093866         0.006053         0.443603          0.999327   \n",
      "5        0.241093         0.014588         0.477887          0.999327   \n",
      "6        0.384652         0.022765         0.491559          0.999327   \n",
      "7        0.524458         0.037809         0.466708          0.999327   \n",
      "0        0.027670         0.005471         0.572118          0.930895   \n",
      "1        0.087699         0.014203         0.586321          0.934560   \n",
      "2        0.123745         0.021479         0.615974          0.941097   \n",
      "3        0.156221         0.028420         0.617331          0.942951   \n",
      "4        0.076248         0.005222         0.563950          0.923866   \n",
      "5        0.199907         0.014017         0.608719          0.934413   \n",
      "6        0.300280         0.020285         0.588369          0.936139   \n",
      "7        0.439856         0.027308         0.618587          0.940927   \n",
      "0        0.027211         0.006327         0.563069          0.999327   \n",
      "1        0.068220         0.012536         0.593994          0.999327   \n",
      "2        0.103084         0.021112         0.610592          0.999327   \n",
      "3        0.139993         0.027920         0.619020          0.999327   \n",
      "4        0.084314         0.005619         0.598731          0.999327   \n",
      "5        0.210423         0.014739         0.577511          0.999327   \n",
      "6        0.335544         0.019676         0.621732          0.999327   \n",
      "7        0.459420         0.029228         0.636866          0.999327   \n",
      "\n",
      "   method_ids param_criterion param_n_estimators  \\\n",
      "0   [0, 0, 1]             mse                 10   \n",
      "1   [0, 0, 1]             mse                 25   \n",
      "2   [0, 0, 1]             mse                 40   \n",
      "3   [0, 0, 1]             mse                 55   \n",
      "4   [0, 0, 1]             mae                 10   \n",
      "5   [0, 0, 1]             mae                 25   \n",
      "6   [0, 0, 1]             mae                 40   \n",
      "7   [0, 0, 1]             mae                 55   \n",
      "0   [0, 0, 2]             mse                 10   \n",
      "1   [0, 0, 2]             mse                 25   \n",
      "2   [0, 0, 2]             mse                 40   \n",
      "3   [0, 0, 2]             mse                 55   \n",
      "4   [0, 0, 2]             mae                 10   \n",
      "5   [0, 0, 2]             mae                 25   \n",
      "6   [0, 0, 2]             mae                 40   \n",
      "7   [0, 0, 2]             mae                 55   \n",
      "0   [0, 1, 1]             mse                 10   \n",
      "1   [0, 1, 1]             mse                 25   \n",
      "2   [0, 1, 1]             mse                 40   \n",
      "3   [0, 1, 1]             mse                 55   \n",
      "4   [0, 1, 1]             mae                 10   \n",
      "5   [0, 1, 1]             mae                 25   \n",
      "6   [0, 1, 1]             mae                 40   \n",
      "7   [0, 1, 1]             mae                 55   \n",
      "0   [0, 1, 2]             mse                 10   \n",
      "1   [0, 1, 2]             mse                 25   \n",
      "2   [0, 1, 2]             mse                 40   \n",
      "3   [0, 1, 2]             mse                 55   \n",
      "4   [0, 1, 2]             mae                 10   \n",
      "5   [0, 1, 2]             mae                 25   \n",
      "..        ...             ...                ...   \n",
      "2   [1, 1, 1]             mse                 40   \n",
      "3   [1, 1, 1]             mse                 55   \n",
      "4   [1, 1, 1]             mae                 10   \n",
      "5   [1, 1, 1]             mae                 25   \n",
      "6   [1, 1, 1]             mae                 40   \n",
      "7   [1, 1, 1]             mae                 55   \n",
      "0   [1, 1, 2]             mse                 10   \n",
      "1   [1, 1, 2]             mse                 25   \n",
      "2   [1, 1, 2]             mse                 40   \n",
      "3   [1, 1, 2]             mse                 55   \n",
      "4   [1, 1, 2]             mae                 10   \n",
      "5   [1, 1, 2]             mae                 25   \n",
      "6   [1, 1, 2]             mae                 40   \n",
      "7   [1, 1, 2]             mae                 55   \n",
      "0   [1, 2, 1]             mse                 10   \n",
      "1   [1, 2, 1]             mse                 25   \n",
      "2   [1, 2, 1]             mse                 40   \n",
      "3   [1, 2, 1]             mse                 55   \n",
      "4   [1, 2, 1]             mae                 10   \n",
      "5   [1, 2, 1]             mae                 25   \n",
      "6   [1, 2, 1]             mae                 40   \n",
      "7   [1, 2, 1]             mae                 55   \n",
      "0   [1, 2, 2]             mse                 10   \n",
      "1   [1, 2, 2]             mse                 25   \n",
      "2   [1, 2, 2]             mse                 40   \n",
      "3   [1, 2, 2]             mse                 55   \n",
      "4   [1, 2, 2]             mae                 10   \n",
      "5   [1, 2, 2]             mae                 25   \n",
      "6   [1, 2, 2]             mae                 40   \n",
      "7   [1, 2, 2]             mae                 55   \n",
      "\n",
      "                                         params  rank_test_score  \\\n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                2   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                5   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                8   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                2   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                1   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                1   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "..                                          ...              ...   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                1   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                3   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                6   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                4   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                3   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                6   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                5   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                7   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                7   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                3   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                2   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                8   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                4   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                5   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "0   {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1   {u'n_estimators': 25, u'criterion': u'mse'}                6   \n",
      "2   {u'n_estimators': 40, u'criterion': u'mse'}                4   \n",
      "3   {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4   {u'n_estimators': 10, u'criterion': u'mae'}                5   \n",
      "5   {u'n_estimators': 25, u'criterion': u'mae'}                7   \n",
      "6   {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7   {u'n_estimators': 55, u'criterion': u'mae'}                1   \n",
      "\n",
      "    split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0            0.532909            0.875786           0.441208   \n",
      "1            0.539835            0.910320           0.514354   \n",
      "2            0.565894            0.912127           0.505728   \n",
      "3            0.544117            0.922500           0.478462   \n",
      "4            0.558931            0.882973           0.483207   \n",
      "5            0.564253            0.916684           0.445223   \n",
      "6            0.573519            0.910791           0.453887   \n",
      "7            0.585932            0.915393           0.504125   \n",
      "0            0.492315            0.999978           0.449155   \n",
      "1            0.542346            0.999978           0.499233   \n",
      "2            0.531688            0.999978           0.510485   \n",
      "3            0.525395            0.999978           0.500564   \n",
      "4            0.497328            0.999978           0.482597   \n",
      "5            0.553961            0.999978           0.466189   \n",
      "6            0.566515            0.999978           0.481667   \n",
      "7            0.552411            0.999978           0.512055   \n",
      "0            0.469139            0.880897           0.496814   \n",
      "1            0.502791            0.902003           0.424643   \n",
      "2            0.542659            0.903799           0.430453   \n",
      "3            0.495867            0.906287           0.422693   \n",
      "4            0.512529            0.869530           0.414054   \n",
      "5            0.493631            0.894170           0.448127   \n",
      "6            0.525079            0.904297           0.460221   \n",
      "7            0.508563            0.905416           0.466128   \n",
      "0            0.531158            0.999978           0.408375   \n",
      "1            0.503439            0.999978           0.525550   \n",
      "2            0.495257            0.999978           0.473424   \n",
      "3            0.532163            0.999978           0.492380   \n",
      "4            0.479440            0.999978           0.423900   \n",
      "5            0.514665            0.999978           0.466658   \n",
      "..                ...                 ...                ...   \n",
      "2            0.491664            0.918327           0.526921   \n",
      "3            0.521132            0.929118           0.535756   \n",
      "4            0.346534            0.896983           0.493151   \n",
      "5            0.518611            0.925493           0.514109   \n",
      "6            0.470176            0.928646           0.515855   \n",
      "7            0.435470            0.929999           0.527282   \n",
      "0            0.551075            0.999978           0.558273   \n",
      "1            0.492013            0.999978           0.553150   \n",
      "2            0.538011            0.999978           0.576530   \n",
      "3            0.525228            0.999978           0.567881   \n",
      "4            0.459749            0.999978           0.535967   \n",
      "5            0.497923            0.999978           0.539135   \n",
      "6            0.499860            0.999978           0.603550   \n",
      "7            0.470177            0.999978           0.577105   \n",
      "0            0.641078            0.922661           0.549063   \n",
      "1            0.696969            0.929801           0.558220   \n",
      "2            0.693874            0.929307           0.626593   \n",
      "3            0.715424            0.934945           0.579221   \n",
      "4            0.582810            0.903686           0.594502   \n",
      "5            0.635584            0.933998           0.660207   \n",
      "6            0.655826            0.923715           0.571048   \n",
      "7            0.662526            0.932603           0.615139   \n",
      "0            0.629988            0.999978           0.576435   \n",
      "1            0.674517            0.999978           0.600672   \n",
      "2            0.675135            0.999978           0.610547   \n",
      "3            0.668569            0.999978           0.638537   \n",
      "4            0.639206            0.999978           0.601776   \n",
      "5            0.671600            0.999978           0.525742   \n",
      "6            0.667524            0.999978           0.648800   \n",
      "7            0.677133            0.999978           0.666931   \n",
      "\n",
      "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0             0.877617           0.445372            0.897734      0.000139   \n",
      "1             0.912061           0.423071            0.917066      0.000326   \n",
      "2             0.923810           0.433343            0.921837      0.000400   \n",
      "3             0.923717           0.464570            0.916511      0.000613   \n",
      "4             0.905834           0.413808            0.895927      0.001724   \n",
      "5             0.912121           0.441685            0.919130      0.001826   \n",
      "6             0.920353           0.445343            0.918235      0.007670   \n",
      "7             0.924390           0.407669            0.917102      0.005413   \n",
      "0             0.998030           0.424076            0.999974      0.000405   \n",
      "1             0.998030           0.406243            0.999974      0.002746   \n",
      "2             0.998030           0.397930            0.999974      0.001071   \n",
      "3             0.998030           0.420547            0.999974      0.000255   \n",
      "4             0.998030           0.345349            0.999974      0.001128   \n",
      "5             0.998030           0.405106            0.999974      0.004279   \n",
      "6             0.998030           0.407175            0.999974      0.022929   \n",
      "7             0.998030           0.376038            0.999974      0.028923   \n",
      "0             0.869641           0.348710            0.908778      0.000299   \n",
      "1             0.914159           0.325005            0.915357      0.004393   \n",
      "2             0.911402           0.384754            0.917626      0.004156   \n",
      "3             0.922294           0.386176            0.925181      0.015858   \n",
      "4             0.870999           0.346366            0.878579      0.001481   \n",
      "5             0.915648           0.401179            0.904786      0.002898   \n",
      "6             0.917081           0.407480            0.907487      0.010971   \n",
      "7             0.921047           0.356440            0.918367      0.006066   \n",
      "0             0.998030           0.347781            0.999974      0.000520   \n",
      "1             0.998030           0.401990            0.999974      0.005516   \n",
      "2             0.998030           0.373970            0.999974      0.002415   \n",
      "3             0.998030           0.371773            0.999974      0.002624   \n",
      "4             0.998030           0.349457            0.999974      0.001997   \n",
      "5             0.998030           0.383980            0.999974      0.000475   \n",
      "..                 ...                ...                 ...           ...   \n",
      "2             0.919070           0.378431            0.930952      0.002355   \n",
      "3             0.921036           0.378503            0.927776      0.007452   \n",
      "4             0.896086           0.312841            0.920732      0.000950   \n",
      "5             0.903653           0.379872            0.926045      0.013425   \n",
      "6             0.920022           0.371715            0.931401      0.034239   \n",
      "7             0.914613           0.349390            0.926017      0.011770   \n",
      "0             0.998030           0.332517            0.999974      0.001776   \n",
      "1             0.998030           0.414854            0.999974      0.003099   \n",
      "2             0.998030           0.415819            0.999974      0.000128   \n",
      "3             0.998030           0.330883            0.999974      0.002054   \n",
      "4             0.998030           0.334918            0.999974      0.003281   \n",
      "5             0.998030           0.396388            0.999974      0.009429   \n",
      "6             0.998030           0.371179            0.999974      0.015930   \n",
      "7             0.998030           0.352804            0.999974      0.025848   \n",
      "0             0.937908           0.525471            0.932116      0.001447   \n",
      "1             0.924635           0.502585            0.949243      0.003171   \n",
      "2             0.941446           0.526619            0.952537      0.006429   \n",
      "3             0.947193           0.556294            0.946716      0.006242   \n",
      "4             0.928826           0.514336            0.939085      0.005195   \n",
      "5             0.920911           0.530077            0.948331      0.012959   \n",
      "6             0.933144           0.537507            0.951558      0.012898   \n",
      "7             0.945497           0.577623            0.944681      0.019597   \n",
      "0             0.998030           0.482063            0.999974      0.001330   \n",
      "1             0.998030           0.505927            0.999974      0.001923   \n",
      "2             0.998030           0.545401            0.999974      0.003110   \n",
      "3             0.998030           0.549420            0.999974      0.004264   \n",
      "4             0.998030           0.554777            0.999974      0.003891   \n",
      "5             0.998030           0.534180            0.999974      0.006413   \n",
      "6             0.998030           0.548378            0.999974      0.010716   \n",
      "7             0.998030           0.566100            0.999974      0.016674   \n",
      "\n",
      "    std_score_time  std_test_score  std_train_score  \n",
      "0         0.000128        0.042355         0.009943  \n",
      "1         0.000056        0.050119         0.002860  \n",
      "2         0.000246        0.054229         0.005106  \n",
      "3         0.000061        0.034750         0.003149  \n",
      "4         0.000112        0.059322         0.009361  \n",
      "5         0.000085        0.057065         0.002905  \n",
      "6         0.004956        0.058616         0.004101  \n",
      "7         0.000275        0.072911         0.003901  \n",
      "0         0.000361        0.028219         0.000917  \n",
      "1         0.000495        0.056804         0.000917  \n",
      "2         0.001730        0.058674         0.000917  \n",
      "3         0.000464        0.044731         0.000917  \n",
      "4         0.000103        0.068394         0.000917  \n",
      "5         0.000121        0.061167         0.000917  \n",
      "6         0.001815        0.065161         0.000917  \n",
      "7         0.003829        0.075440         0.000917  \n",
      "0         0.000216        0.064206         0.016451  \n",
      "1         0.001236        0.072806         0.006032  \n",
      "2         0.002018        0.066442         0.005654  \n",
      "3         0.001032        0.045671         0.008310  \n",
      "4         0.000067        0.068304         0.003965  \n",
      "5         0.000056        0.037777         0.008769  \n",
      "6         0.000500        0.048145         0.005433  \n",
      "7         0.000540        0.064097         0.006825  \n",
      "0         0.000010        0.076391         0.000917  \n",
      "1         0.000275        0.053725         0.000917  \n",
      "2         0.000319        0.052769         0.000917  \n",
      "3         0.000374        0.068188         0.000917  \n",
      "4         0.000168        0.053285         0.000917  \n",
      "5         0.000100        0.053995         0.000917  \n",
      "..             ...             ...              ...  \n",
      "2         0.001147        0.063251         0.005785  \n",
      "3         0.001678        0.070853         0.003536  \n",
      "4         0.000251        0.078168         0.011413  \n",
      "5         0.000608        0.064316         0.010428  \n",
      "6         0.001851        0.060047         0.004847  \n",
      "7         0.001139        0.072507         0.006520  \n",
      "0         0.000272        0.104664         0.000917  \n",
      "1         0.000308        0.056485         0.000917  \n",
      "2         0.001063        0.068408         0.000917  \n",
      "3         0.000370        0.103009         0.000917  \n",
      "4         0.000542        0.082731         0.000917  \n",
      "5         0.001277        0.059890         0.000917  \n",
      "6         0.001539        0.094879         0.000917  \n",
      "7         0.006585        0.091440         0.000917  \n",
      "0         0.000494        0.049958         0.006284  \n",
      "1         0.000462        0.081862         0.010595  \n",
      "2         0.003365        0.068751         0.009487  \n",
      "3         0.001802        0.070357         0.005665  \n",
      "4         0.000098        0.035313         0.014871  \n",
      "5         0.000620        0.056365         0.011198  \n",
      "6         0.001610        0.049865         0.011562  \n",
      "7         0.000795        0.034777         0.005895  \n",
      "0         0.001287        0.061174         0.000917  \n",
      "1         0.000309        0.069048         0.000917  \n",
      "2         0.000726        0.053010         0.000917  \n",
      "3         0.001138        0.050593         0.000917  \n",
      "4         0.000318        0.034565         0.000917  \n",
      "5         0.000752        0.066976         0.000917  \n",
      "6         0.000828        0.052292         0.000917  \n",
      "7         0.000592        0.050079         0.000917  \n",
      "\n",
      "[96 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "eval_set = rg.auto_grid(X,y,labels, ks=[10,20], opts=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "currind = 0\n",
    "to_drop = []\n",
    "for i in range(0, len(evaluation_results)-1):\n",
    "    if abs(evaluation_results.eval_set_score[currind] - evaluation_results.eval_set_score[i+1]) < 0.01 \\\n",
    "        and abs(evaluation_results.dev_set_score[i] - evaluation_results.dev_set_score[i+1]) < 0.01:\n",
    "        to_drop.append(i+1)\n",
    "    else:\n",
    "        currind = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in to_drop:\n",
    "    evaluation_results.drop(i, inplace=True)\n",
    "    \n",
    "evaluation_results.sort_values(by='rank_test_score', inplace=True)\n",
    "evaluation_results.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for i in range(0, len(evaluation_results)-1):\n",
    "    if str(evaluation_results.method_ids[i])==str(evaluation_results.method_ids[i+1]):\n",
    "        to_drop.append(i+1)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for i in to_drop:\n",
    "    evaluation_results.drop(i, inplace=True)\n",
    "\n",
    "evaluation_results.sort_values(by='rank_test_score', inplace=True)\n",
    "evaluation_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#array = np.array(evaluation_results['eval_set_score'])\n",
    "#temp = array.argsort()[::-1]\n",
    "#ranks = np.empty(len(array), int)\n",
    "#ranks[temp] = np.arange(len(array))\n",
    "#evaluation_results['rank_test_score'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file contains an evaluation and analysis set\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Trying to write to wrong type of file, or analysis set already saved! Check the file...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-4f22333fe248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./results.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-142-23a30a858144>\u001b[0m in \u001b[0;36msave_analysis\u001b[0;34m(analysis_results, filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying to write to wrong type of file, or analysis set already saved! Check the file...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mfile_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Trying to write to wrong type of file, or analysis set already saved! Check the file..."
     ]
    }
   ],
   "source": [
    "save_analysis(evaluation_results, './results.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y,labels = set_input('./input_files/rdkit_descriptors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ZZZIVG03\n",
       "1      PYRDNO19\n",
       "2        TAQYUG\n",
       "3      SOFVUG02\n",
       "4        SOFWIV\n",
       "5      DBRMET11\n",
       "6      NTROMA01\n",
       "7        TIVWOL\n",
       "8      ZZZMWK02\n",
       "9        YAGTOQ\n",
       "10       GAFLEF\n",
       "11       BUWZAU\n",
       "12     ACRLAC02\n",
       "13     ZZZTKA02\n",
       "14       PEZHEH\n",
       "15       FUFSOP\n",
       "16       FUFTIK\n",
       "17     ALYTUR01\n",
       "18       SECSIE\n",
       "19       VACCEH\n",
       "20       RUVQII\n",
       "21     QQQCIS01\n",
       "22     PYRENE02\n",
       "23       HEYCIX\n",
       "24     DIBENZ14\n",
       "25     AZOBEN12\n",
       "26       RAFFIO\n",
       "27     RAFFIO01\n",
       "28     BZDMAZ16\n",
       "29     CRBZOL01\n",
       "         ...   \n",
       "422    CAPLAC01\n",
       "423    PARBAC11\n",
       "424      SCCHRN\n",
       "425      ROKPIQ\n",
       "426      SOPYOM\n",
       "427    PHBALD11\n",
       "428      FOMZUD\n",
       "429      BUYZAW\n",
       "430      BIVYAG\n",
       "431      CYPHAM\n",
       "432      VABVEZ\n",
       "433    YUHHUE01\n",
       "434      IVUQOF\n",
       "435      SILGOK\n",
       "436    FAMDIG01\n",
       "437    FAMDIG02\n",
       "438    CLPHOL12\n",
       "439    NAPHOB03\n",
       "440      QOVRUP\n",
       "441      QAMVEG\n",
       "442    RESORA03\n",
       "443    QQQEJV06\n",
       "444    PHENOL11\n",
       "445    XYLTOL01\n",
       "446      IJUXEQ\n",
       "447    GAKNAH02\n",
       "448    COKBIN01\n",
       "449      QATTIO\n",
       "450      NOZKES\n",
       "451      TEPSAM\n",
       "Name: refcode, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dev_evs</th>\n",
       "      <th>dev_mae</th>\n",
       "      <th>dev_median_ae</th>\n",
       "      <th>dev_mse</th>\n",
       "      <th>dev_set_score</th>\n",
       "      <th>eval_evs</th>\n",
       "      <th>eval_mae</th>\n",
       "      <th>eval_median_ae</th>\n",
       "      <th>eval_mse</th>\n",
       "      <th>eval_set_score</th>\n",
       "      <th>method_ids</th>\n",
       "      <th>parameters</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.739102</td>\n",
       "      <td>0.623838</td>\n",
       "      <td>0.501034</td>\n",
       "      <td>0.639926</td>\n",
       "      <td>0.819417</td>\n",
       "      <td>0.694692</td>\n",
       "      <td>0.616625</td>\n",
       "      <td>0.441952</td>\n",
       "      <td>0.720915</td>\n",
       "      <td>0.787241</td>\n",
       "      <td>[7, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.744669</td>\n",
       "      <td>0.617112</td>\n",
       "      <td>0.483138</td>\n",
       "      <td>0.626944</td>\n",
       "      <td>0.823081</td>\n",
       "      <td>0.699515</td>\n",
       "      <td>0.612576</td>\n",
       "      <td>0.466975</td>\n",
       "      <td>0.721839</td>\n",
       "      <td>0.786968</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.733978</td>\n",
       "      <td>0.636807</td>\n",
       "      <td>0.511334</td>\n",
       "      <td>0.669066</td>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.710737</td>\n",
       "      <td>0.615025</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.721846</td>\n",
       "      <td>0.786966</td>\n",
       "      <td>[5, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.745762</td>\n",
       "      <td>0.616421</td>\n",
       "      <td>0.480062</td>\n",
       "      <td>0.625489</td>\n",
       "      <td>0.823491</td>\n",
       "      <td>0.700215</td>\n",
       "      <td>0.612471</td>\n",
       "      <td>0.465889</td>\n",
       "      <td>0.721890</td>\n",
       "      <td>0.786953</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.733186</td>\n",
       "      <td>0.637282</td>\n",
       "      <td>0.511426</td>\n",
       "      <td>0.669930</td>\n",
       "      <td>0.810950</td>\n",
       "      <td>0.710194</td>\n",
       "      <td>0.615207</td>\n",
       "      <td>0.435337</td>\n",
       "      <td>0.721912</td>\n",
       "      <td>0.786947</td>\n",
       "      <td>[5, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.748417</td>\n",
       "      <td>0.614667</td>\n",
       "      <td>0.475049</td>\n",
       "      <td>0.621882</td>\n",
       "      <td>0.824509</td>\n",
       "      <td>0.701933</td>\n",
       "      <td>0.612238</td>\n",
       "      <td>0.459401</td>\n",
       "      <td>0.721978</td>\n",
       "      <td>0.786927</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.732047</td>\n",
       "      <td>0.637962</td>\n",
       "      <td>0.511549</td>\n",
       "      <td>0.671181</td>\n",
       "      <td>0.810597</td>\n",
       "      <td>0.709410</td>\n",
       "      <td>0.615465</td>\n",
       "      <td>0.436924</td>\n",
       "      <td>0.722020</td>\n",
       "      <td>0.786915</td>\n",
       "      <td>[5, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.694322</td>\n",
       "      <td>0.657794</td>\n",
       "      <td>0.518539</td>\n",
       "      <td>0.714293</td>\n",
       "      <td>0.798431</td>\n",
       "      <td>0.684489</td>\n",
       "      <td>0.627613</td>\n",
       "      <td>0.460684</td>\n",
       "      <td>0.736574</td>\n",
       "      <td>0.782620</td>\n",
       "      <td>[3, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.752357</td>\n",
       "      <td>0.632101</td>\n",
       "      <td>0.494662</td>\n",
       "      <td>0.648956</td>\n",
       "      <td>0.816869</td>\n",
       "      <td>0.711772</td>\n",
       "      <td>0.618592</td>\n",
       "      <td>0.453992</td>\n",
       "      <td>0.738298</td>\n",
       "      <td>0.782111</td>\n",
       "      <td>[5, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.4, u'n_alp...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.732740</td>\n",
       "      <td>0.638443</td>\n",
       "      <td>0.486457</td>\n",
       "      <td>0.673809</td>\n",
       "      <td>0.809855</td>\n",
       "      <td>0.706970</td>\n",
       "      <td>0.624094</td>\n",
       "      <td>0.438201</td>\n",
       "      <td>0.738805</td>\n",
       "      <td>0.781961</td>\n",
       "      <td>[4, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.712333</td>\n",
       "      <td>0.643176</td>\n",
       "      <td>0.498564</td>\n",
       "      <td>0.684339</td>\n",
       "      <td>0.806884</td>\n",
       "      <td>0.673801</td>\n",
       "      <td>0.628156</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>0.781210</td>\n",
       "      <td>[6, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.760021</td>\n",
       "      <td>0.618858</td>\n",
       "      <td>0.497022</td>\n",
       "      <td>0.623746</td>\n",
       "      <td>0.823983</td>\n",
       "      <td>0.708178</td>\n",
       "      <td>0.621593</td>\n",
       "      <td>0.435541</td>\n",
       "      <td>0.746938</td>\n",
       "      <td>0.779561</td>\n",
       "      <td>[6, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.715423</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.518645</td>\n",
       "      <td>0.667201</td>\n",
       "      <td>0.811720</td>\n",
       "      <td>0.668582</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.495734</td>\n",
       "      <td>0.747283</td>\n",
       "      <td>0.779459</td>\n",
       "      <td>[8, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.761455</td>\n",
       "      <td>0.617882</td>\n",
       "      <td>0.493286</td>\n",
       "      <td>0.622160</td>\n",
       "      <td>0.824431</td>\n",
       "      <td>0.709157</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.437635</td>\n",
       "      <td>0.747285</td>\n",
       "      <td>0.779459</td>\n",
       "      <td>[6, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.715333</td>\n",
       "      <td>0.645763</td>\n",
       "      <td>0.518736</td>\n",
       "      <td>0.667319</td>\n",
       "      <td>0.811687</td>\n",
       "      <td>0.668516</td>\n",
       "      <td>0.625743</td>\n",
       "      <td>0.495719</td>\n",
       "      <td>0.747314</td>\n",
       "      <td>0.779450</td>\n",
       "      <td>[8, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.761590</td>\n",
       "      <td>0.617789</td>\n",
       "      <td>0.492898</td>\n",
       "      <td>0.622011</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.709249</td>\n",
       "      <td>0.621457</td>\n",
       "      <td>0.437951</td>\n",
       "      <td>0.747320</td>\n",
       "      <td>0.779448</td>\n",
       "      <td>[6, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.753805</td>\n",
       "      <td>0.612044</td>\n",
       "      <td>0.464145</td>\n",
       "      <td>0.613458</td>\n",
       "      <td>0.826886</td>\n",
       "      <td>0.688871</td>\n",
       "      <td>0.623567</td>\n",
       "      <td>0.465434</td>\n",
       "      <td>0.748569</td>\n",
       "      <td>0.779080</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.727966</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>0.502775</td>\n",
       "      <td>0.655139</td>\n",
       "      <td>0.815124</td>\n",
       "      <td>0.670222</td>\n",
       "      <td>0.630329</td>\n",
       "      <td>0.460665</td>\n",
       "      <td>0.748615</td>\n",
       "      <td>0.779066</td>\n",
       "      <td>[7, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.753141</td>\n",
       "      <td>0.612524</td>\n",
       "      <td>0.463647</td>\n",
       "      <td>0.614427</td>\n",
       "      <td>0.826613</td>\n",
       "      <td>0.688424</td>\n",
       "      <td>0.623663</td>\n",
       "      <td>0.467851</td>\n",
       "      <td>0.748642</td>\n",
       "      <td>0.779058</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.727571</td>\n",
       "      <td>0.635538</td>\n",
       "      <td>0.502972</td>\n",
       "      <td>0.655631</td>\n",
       "      <td>0.814985</td>\n",
       "      <td>0.669901</td>\n",
       "      <td>0.630404</td>\n",
       "      <td>0.461174</td>\n",
       "      <td>0.748741</td>\n",
       "      <td>0.779029</td>\n",
       "      <td>[7, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.748437</td>\n",
       "      <td>0.615994</td>\n",
       "      <td>0.466295</td>\n",
       "      <td>0.621203</td>\n",
       "      <td>0.824700</td>\n",
       "      <td>0.685186</td>\n",
       "      <td>0.624397</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.749268</td>\n",
       "      <td>0.778873</td>\n",
       "      <td>[8, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.709217</td>\n",
       "      <td>0.648809</td>\n",
       "      <td>0.456101</td>\n",
       "      <td>0.717894</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>0.706953</td>\n",
       "      <td>0.641784</td>\n",
       "      <td>0.466925</td>\n",
       "      <td>0.760394</td>\n",
       "      <td>0.775590</td>\n",
       "      <td>[3, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 1.0, u'n_alp...</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.621763</td>\n",
       "      <td>0.505157</td>\n",
       "      <td>0.636988</td>\n",
       "      <td>0.820246</td>\n",
       "      <td>0.685149</td>\n",
       "      <td>0.634194</td>\n",
       "      <td>0.470239</td>\n",
       "      <td>0.760898</td>\n",
       "      <td>0.775441</td>\n",
       "      <td>[6, 0, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.714051</td>\n",
       "      <td>0.645477</td>\n",
       "      <td>0.454366</td>\n",
       "      <td>0.711626</td>\n",
       "      <td>0.799184</td>\n",
       "      <td>0.707913</td>\n",
       "      <td>0.640131</td>\n",
       "      <td>0.460990</td>\n",
       "      <td>0.760953</td>\n",
       "      <td>0.775425</td>\n",
       "      <td>[4, 2, 9]</td>\n",
       "      <td>{u'normalize': True, u'positive': False, u'cri...</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.707464</td>\n",
       "      <td>0.653229</td>\n",
       "      <td>0.528076</td>\n",
       "      <td>0.680128</td>\n",
       "      <td>0.808072</td>\n",
       "      <td>0.663530</td>\n",
       "      <td>0.635869</td>\n",
       "      <td>0.458075</td>\n",
       "      <td>0.761125</td>\n",
       "      <td>0.775374</td>\n",
       "      <td>[7, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.707902</td>\n",
       "      <td>0.649701</td>\n",
       "      <td>0.456368</td>\n",
       "      <td>0.719311</td>\n",
       "      <td>0.797015</td>\n",
       "      <td>0.706007</td>\n",
       "      <td>0.642356</td>\n",
       "      <td>0.468621</td>\n",
       "      <td>0.761136</td>\n",
       "      <td>0.775371</td>\n",
       "      <td>[3, 2, 7]</td>\n",
       "      <td>{u'normalize': True, u'n_alphas': 260, u'fit_i...</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.707269</td>\n",
       "      <td>0.653343</td>\n",
       "      <td>0.528153</td>\n",
       "      <td>0.680364</td>\n",
       "      <td>0.808006</td>\n",
       "      <td>0.663397</td>\n",
       "      <td>0.635913</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.761176</td>\n",
       "      <td>0.775359</td>\n",
       "      <td>[7, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.707560</td>\n",
       "      <td>0.649931</td>\n",
       "      <td>0.456474</td>\n",
       "      <td>0.719676</td>\n",
       "      <td>0.796912</td>\n",
       "      <td>0.705765</td>\n",
       "      <td>0.642497</td>\n",
       "      <td>0.468358</td>\n",
       "      <td>0.761319</td>\n",
       "      <td>0.775317</td>\n",
       "      <td>[3, 2, 7]</td>\n",
       "      <td>{u'normalize': True, u'n_alphas': 310, u'fit_i...</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.706498</td>\n",
       "      <td>0.653795</td>\n",
       "      <td>0.528483</td>\n",
       "      <td>0.681305</td>\n",
       "      <td>0.807740</td>\n",
       "      <td>0.662874</td>\n",
       "      <td>0.636078</td>\n",
       "      <td>0.458020</td>\n",
       "      <td>0.761378</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>[7, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.707318</td>\n",
       "      <td>0.650090</td>\n",
       "      <td>0.456602</td>\n",
       "      <td>0.719927</td>\n",
       "      <td>0.796841</td>\n",
       "      <td>0.705601</td>\n",
       "      <td>0.642587</td>\n",
       "      <td>0.468873</td>\n",
       "      <td>0.761425</td>\n",
       "      <td>0.775285</td>\n",
       "      <td>[3, 2, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 1.0, u'n_alp...</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.550590</td>\n",
       "      <td>0.712954</td>\n",
       "      <td>0.562368</td>\n",
       "      <td>0.912872</td>\n",
       "      <td>0.742393</td>\n",
       "      <td>0.580783</td>\n",
       "      <td>0.675239</td>\n",
       "      <td>0.505763</td>\n",
       "      <td>0.837002</td>\n",
       "      <td>0.752981</td>\n",
       "      <td>[5, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.603733</td>\n",
       "      <td>0.720595</td>\n",
       "      <td>0.565269</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.760588</td>\n",
       "      <td>0.622767</td>\n",
       "      <td>0.708632</td>\n",
       "      <td>0.570301</td>\n",
       "      <td>0.841065</td>\n",
       "      <td>0.751782</td>\n",
       "      <td>[2, 2, 9]</td>\n",
       "      <td>{u'normalize': True, u'positive': False, u'cri...</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.806119</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.492557</td>\n",
       "      <td>0.564538</td>\n",
       "      <td>0.840691</td>\n",
       "      <td>0.726606</td>\n",
       "      <td>0.658499</td>\n",
       "      <td>0.512103</td>\n",
       "      <td>0.843655</td>\n",
       "      <td>0.751017</td>\n",
       "      <td>[6, 2, 9]</td>\n",
       "      <td>{u'normalize': False, u'positive': False, u'cr...</td>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.770162</td>\n",
       "      <td>0.626217</td>\n",
       "      <td>0.535170</td>\n",
       "      <td>0.645725</td>\n",
       "      <td>0.817781</td>\n",
       "      <td>0.716766</td>\n",
       "      <td>0.666046</td>\n",
       "      <td>0.481189</td>\n",
       "      <td>0.845424</td>\n",
       "      <td>0.750495</td>\n",
       "      <td>[3, 2, 10]</td>\n",
       "      <td>{u'normalize': True, u'alpha': 0, u'l1_ratio':...</td>\n",
       "      <td>1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.533353</td>\n",
       "      <td>0.731591</td>\n",
       "      <td>0.612059</td>\n",
       "      <td>0.921334</td>\n",
       "      <td>0.740005</td>\n",
       "      <td>0.546803</td>\n",
       "      <td>0.675835</td>\n",
       "      <td>0.501723</td>\n",
       "      <td>0.849062</td>\n",
       "      <td>0.749422</td>\n",
       "      <td>[8, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.504935</td>\n",
       "      <td>0.731396</td>\n",
       "      <td>0.610117</td>\n",
       "      <td>0.967370</td>\n",
       "      <td>0.727014</td>\n",
       "      <td>0.594080</td>\n",
       "      <td>0.700044</td>\n",
       "      <td>0.541446</td>\n",
       "      <td>0.850542</td>\n",
       "      <td>0.748985</td>\n",
       "      <td>[8, 1, 7]</td>\n",
       "      <td>{u'normalize': True, u'n_alphas': 160, u'fit_i...</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.530106</td>\n",
       "      <td>0.732692</td>\n",
       "      <td>0.611526</td>\n",
       "      <td>0.924453</td>\n",
       "      <td>0.739125</td>\n",
       "      <td>0.544051</td>\n",
       "      <td>0.676669</td>\n",
       "      <td>0.500438</td>\n",
       "      <td>0.850770</td>\n",
       "      <td>0.748918</td>\n",
       "      <td>[8, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.734525</td>\n",
       "      <td>0.602428</td>\n",
       "      <td>0.975076</td>\n",
       "      <td>0.724840</td>\n",
       "      <td>0.588283</td>\n",
       "      <td>0.702526</td>\n",
       "      <td>0.544397</td>\n",
       "      <td>0.855595</td>\n",
       "      <td>0.747494</td>\n",
       "      <td>[8, 1, 7]</td>\n",
       "      <td>{u'normalize': True, u'n_alphas': 210, u'fit_i...</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.569702</td>\n",
       "      <td>0.722565</td>\n",
       "      <td>0.631466</td>\n",
       "      <td>0.841763</td>\n",
       "      <td>0.762460</td>\n",
       "      <td>0.540087</td>\n",
       "      <td>0.682678</td>\n",
       "      <td>0.485593</td>\n",
       "      <td>0.859571</td>\n",
       "      <td>0.746320</td>\n",
       "      <td>[8, 2, 4]</td>\n",
       "      <td>{u'normalize': True, u'fit_intercept': True}</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.451734</td>\n",
       "      <td>0.749088</td>\n",
       "      <td>0.608370</td>\n",
       "      <td>0.987187</td>\n",
       "      <td>0.721422</td>\n",
       "      <td>0.501453</td>\n",
       "      <td>0.684844</td>\n",
       "      <td>0.497613</td>\n",
       "      <td>0.863123</td>\n",
       "      <td>0.745272</td>\n",
       "      <td>[8, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.776571</td>\n",
       "      <td>0.619039</td>\n",
       "      <td>0.513424</td>\n",
       "      <td>0.630290</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>0.707807</td>\n",
       "      <td>0.669490</td>\n",
       "      <td>0.470071</td>\n",
       "      <td>0.864982</td>\n",
       "      <td>0.744724</td>\n",
       "      <td>[4, 2, 10]</td>\n",
       "      <td>{u'normalize': False, u'alpha': 0, u'l1_ratio'...</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.447570</td>\n",
       "      <td>0.750434</td>\n",
       "      <td>0.611115</td>\n",
       "      <td>0.990570</td>\n",
       "      <td>0.720467</td>\n",
       "      <td>0.497802</td>\n",
       "      <td>0.686017</td>\n",
       "      <td>0.499541</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>[8, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.534083</td>\n",
       "      <td>0.734079</td>\n",
       "      <td>0.596311</td>\n",
       "      <td>0.925011</td>\n",
       "      <td>0.738968</td>\n",
       "      <td>0.532096</td>\n",
       "      <td>0.685192</td>\n",
       "      <td>0.506913</td>\n",
       "      <td>0.868250</td>\n",
       "      <td>0.743759</td>\n",
       "      <td>[6, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.4, u'n_alp...</td>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.729616</td>\n",
       "      <td>0.665953</td>\n",
       "      <td>0.524935</td>\n",
       "      <td>0.709743</td>\n",
       "      <td>0.799715</td>\n",
       "      <td>0.668276</td>\n",
       "      <td>0.697973</td>\n",
       "      <td>0.546733</td>\n",
       "      <td>0.878031</td>\n",
       "      <td>0.740872</td>\n",
       "      <td>[2, 2, 10]</td>\n",
       "      <td>{u'normalize': False, u'alpha': 0, u'l1_ratio'...</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.541978</td>\n",
       "      <td>0.737704</td>\n",
       "      <td>0.652997</td>\n",
       "      <td>0.874577</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>0.520419</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.512165</td>\n",
       "      <td>0.879264</td>\n",
       "      <td>0.740509</td>\n",
       "      <td>[7, 2, 4]</td>\n",
       "      <td>{u'normalize': True, u'fit_intercept': True}</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.422882</td>\n",
       "      <td>0.757920</td>\n",
       "      <td>0.615778</td>\n",
       "      <td>1.009943</td>\n",
       "      <td>0.715001</td>\n",
       "      <td>0.477688</td>\n",
       "      <td>0.693306</td>\n",
       "      <td>0.509715</td>\n",
       "      <td>0.879486</td>\n",
       "      <td>0.740443</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.735618</td>\n",
       "      <td>0.665637</td>\n",
       "      <td>0.522273</td>\n",
       "      <td>0.701986</td>\n",
       "      <td>0.801904</td>\n",
       "      <td>0.674743</td>\n",
       "      <td>0.698704</td>\n",
       "      <td>0.555863</td>\n",
       "      <td>0.880352</td>\n",
       "      <td>0.740187</td>\n",
       "      <td>[2, 2, 3]</td>\n",
       "      <td>{u'normalize': True, u'fit_intercept': False}</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.421128</td>\n",
       "      <td>0.758463</td>\n",
       "      <td>0.614793</td>\n",
       "      <td>1.011325</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.476152</td>\n",
       "      <td>0.693819</td>\n",
       "      <td>0.510340</td>\n",
       "      <td>0.880572</td>\n",
       "      <td>0.740123</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.8, u'n_alp...</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.725819</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>0.535260</td>\n",
       "      <td>0.709767</td>\n",
       "      <td>0.799708</td>\n",
       "      <td>0.667863</td>\n",
       "      <td>0.697610</td>\n",
       "      <td>0.525565</td>\n",
       "      <td>0.884596</td>\n",
       "      <td>0.738935</td>\n",
       "      <td>[2, 2, 4]</td>\n",
       "      <td>{u'normalize': False, u'fit_intercept': False}</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.506101</td>\n",
       "      <td>0.746404</td>\n",
       "      <td>0.645662</td>\n",
       "      <td>0.950568</td>\n",
       "      <td>0.731756</td>\n",
       "      <td>0.511875</td>\n",
       "      <td>0.692870</td>\n",
       "      <td>0.473515</td>\n",
       "      <td>0.885048</td>\n",
       "      <td>0.738801</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.4, u'n_alp...</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.525478</td>\n",
       "      <td>0.743138</td>\n",
       "      <td>0.650591</td>\n",
       "      <td>0.896135</td>\n",
       "      <td>0.747117</td>\n",
       "      <td>0.504712</td>\n",
       "      <td>0.700073</td>\n",
       "      <td>0.535032</td>\n",
       "      <td>0.895630</td>\n",
       "      <td>0.735679</td>\n",
       "      <td>[6, 2, 4]</td>\n",
       "      <td>{u'normalize': True, u'fit_intercept': True}</td>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.759827</td>\n",
       "      <td>0.592218</td>\n",
       "      <td>0.998727</td>\n",
       "      <td>0.718166</td>\n",
       "      <td>0.470838</td>\n",
       "      <td>0.700127</td>\n",
       "      <td>0.486327</td>\n",
       "      <td>0.896699</td>\n",
       "      <td>0.735363</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.675823</td>\n",
       "      <td>0.663516</td>\n",
       "      <td>0.552594</td>\n",
       "      <td>0.787680</td>\n",
       "      <td>0.777722</td>\n",
       "      <td>0.653145</td>\n",
       "      <td>0.682258</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.901762</td>\n",
       "      <td>0.733869</td>\n",
       "      <td>[5, 1, 9]</td>\n",
       "      <td>{u'normalize': True, u'positive': False, u'cri...</td>\n",
       "      <td>1419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.435712</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.598814</td>\n",
       "      <td>1.006578</td>\n",
       "      <td>0.715950</td>\n",
       "      <td>0.462404</td>\n",
       "      <td>0.702793</td>\n",
       "      <td>0.488876</td>\n",
       "      <td>0.902035</td>\n",
       "      <td>0.733788</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.736167</td>\n",
       "      <td>0.637874</td>\n",
       "      <td>0.495982</td>\n",
       "      <td>0.724036</td>\n",
       "      <td>0.795682</td>\n",
       "      <td>0.695098</td>\n",
       "      <td>0.671311</td>\n",
       "      <td>0.486387</td>\n",
       "      <td>0.903170</td>\n",
       "      <td>0.733453</td>\n",
       "      <td>[5, 1, 9]</td>\n",
       "      <td>{u'normalize': False, u'positive': False, u'cr...</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.468627</td>\n",
       "      <td>0.758251</td>\n",
       "      <td>0.660417</td>\n",
       "      <td>0.982653</td>\n",
       "      <td>0.722702</td>\n",
       "      <td>0.478973</td>\n",
       "      <td>0.703630</td>\n",
       "      <td>0.489093</td>\n",
       "      <td>0.905926</td>\n",
       "      <td>0.732640</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.4, u'n_alp...</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.531155</td>\n",
       "      <td>0.737551</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.892246</td>\n",
       "      <td>0.748214</td>\n",
       "      <td>0.475383</td>\n",
       "      <td>0.707195</td>\n",
       "      <td>0.543245</td>\n",
       "      <td>0.908914</td>\n",
       "      <td>0.731758</td>\n",
       "      <td>[7, 0, 4]</td>\n",
       "      <td>{u'normalize': True, u'fit_intercept': True}</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.677915</td>\n",
       "      <td>0.677873</td>\n",
       "      <td>0.572106</td>\n",
       "      <td>0.774922</td>\n",
       "      <td>0.781322</td>\n",
       "      <td>0.623996</td>\n",
       "      <td>0.720576</td>\n",
       "      <td>0.548083</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>0.726411</td>\n",
       "      <td>[6, 1, 9]</td>\n",
       "      <td>{u'normalize': True, u'positive': False, u'cri...</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.373999</td>\n",
       "      <td>0.779665</td>\n",
       "      <td>0.602904</td>\n",
       "      <td>1.054485</td>\n",
       "      <td>0.702431</td>\n",
       "      <td>0.407944</td>\n",
       "      <td>0.720262</td>\n",
       "      <td>0.516974</td>\n",
       "      <td>0.936280</td>\n",
       "      <td>0.723682</td>\n",
       "      <td>[7, 1, 11]</td>\n",
       "      <td>{u'normalize': True, u'l1_ratio': 0.6, u'n_alp...</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.552625</td>\n",
       "      <td>0.755980</td>\n",
       "      <td>0.606267</td>\n",
       "      <td>0.962445</td>\n",
       "      <td>0.728404</td>\n",
       "      <td>0.493804</td>\n",
       "      <td>0.761326</td>\n",
       "      <td>0.664197</td>\n",
       "      <td>1.000121</td>\n",
       "      <td>0.704841</td>\n",
       "      <td>[8, 2, 10]</td>\n",
       "      <td>{u'normalize': True, u'alpha': 10, u'l1_ratio'...</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dev_evs   dev_mae  dev_median_ae   dev_mse  dev_set_score  eval_evs  \\\n",
       "0    0.739102  0.623838       0.501034  0.639926       0.819417  0.694692   \n",
       "1    0.744669  0.617112       0.483138  0.626944       0.823081  0.699515   \n",
       "2    0.733978  0.636807       0.511334  0.669066       0.811194  0.710737   \n",
       "3    0.745762  0.616421       0.480062  0.625489       0.823491  0.700215   \n",
       "4    0.733186  0.637282       0.511426  0.669930       0.810950  0.710194   \n",
       "5    0.748417  0.614667       0.475049  0.621882       0.824509  0.701933   \n",
       "6    0.732047  0.637962       0.511549  0.671181       0.810597  0.709410   \n",
       "7    0.694322  0.657794       0.518539  0.714293       0.798431  0.684489   \n",
       "8    0.752357  0.632101       0.494662  0.648956       0.816869  0.711772   \n",
       "9    0.732740  0.638443       0.486457  0.673809       0.809855  0.706970   \n",
       "10   0.712333  0.643176       0.498564  0.684339       0.806884  0.673801   \n",
       "11   0.760021  0.618858       0.497022  0.623746       0.823983  0.708178   \n",
       "12   0.715423  0.645702       0.518645  0.667201       0.811720  0.668582   \n",
       "13   0.761455  0.617882       0.493286  0.622160       0.824431  0.709157   \n",
       "14   0.715333  0.645763       0.518736  0.667319       0.811687  0.668516   \n",
       "15   0.761590  0.617789       0.492898  0.622011       0.824473  0.709249   \n",
       "16   0.753805  0.612044       0.464145  0.613458       0.826886  0.688871   \n",
       "17   0.727966  0.635300       0.502775  0.655139       0.815124  0.670222   \n",
       "18   0.753141  0.612524       0.463647  0.614427       0.826613  0.688424   \n",
       "19   0.727571  0.635538       0.502972  0.655631       0.814985  0.669901   \n",
       "20   0.748437  0.615994       0.466295  0.621203       0.824700  0.685186   \n",
       "21   0.709217  0.648809       0.456101  0.717894       0.797415  0.706953   \n",
       "22   0.749687  0.621763       0.505157  0.636988       0.820246  0.685149   \n",
       "23   0.714051  0.645477       0.454366  0.711626       0.799184  0.707913   \n",
       "24   0.707464  0.653229       0.528076  0.680128       0.808072  0.663530   \n",
       "25   0.707902  0.649701       0.456368  0.719311       0.797015  0.706007   \n",
       "26   0.707269  0.653343       0.528153  0.680364       0.808006  0.663397   \n",
       "27   0.707560  0.649931       0.456474  0.719676       0.796912  0.705765   \n",
       "28   0.706498  0.653795       0.528483  0.681305       0.807740  0.662874   \n",
       "29   0.707318  0.650090       0.456602  0.719927       0.796841  0.705601   \n",
       "..        ...       ...            ...       ...            ...       ...   \n",
       "135  0.550590  0.712954       0.562368  0.912872       0.742393  0.580783   \n",
       "136  0.603733  0.720595       0.565269  0.848397       0.760588  0.622767   \n",
       "137  0.806119  0.583333       0.492557  0.564538       0.840691  0.726606   \n",
       "138  0.770162  0.626217       0.535170  0.645725       0.817781  0.716766   \n",
       "139  0.533353  0.731591       0.612059  0.921334       0.740005  0.546803   \n",
       "140  0.504935  0.731396       0.610117  0.967370       0.727014  0.594080   \n",
       "141  0.530106  0.732692       0.611526  0.924453       0.739125  0.544051   \n",
       "142  0.497141  0.734525       0.602428  0.975076       0.724840  0.588283   \n",
       "143  0.569702  0.722565       0.631466  0.841763       0.762460  0.540087   \n",
       "144  0.451734  0.749088       0.608370  0.987187       0.721422  0.501453   \n",
       "145  0.776571  0.619039       0.513424  0.630290       0.822136  0.707807   \n",
       "146  0.447570  0.750434       0.611115  0.990570       0.720467  0.497802   \n",
       "147  0.534083  0.734079       0.596311  0.925011       0.738968  0.532096   \n",
       "148  0.729616  0.665953       0.524935  0.709743       0.799715  0.668276   \n",
       "149  0.541978  0.737704       0.652997  0.874577       0.753200  0.520419   \n",
       "150  0.422882  0.757920       0.615778  1.009943       0.715001  0.477688   \n",
       "151  0.735618  0.665637       0.522273  0.701986       0.801904  0.674743   \n",
       "152  0.421128  0.758463       0.614793  1.011325       0.714611  0.476152   \n",
       "153  0.725819  0.671606       0.535260  0.709767       0.799708  0.667863   \n",
       "154  0.506101  0.746404       0.645662  0.950568       0.731756  0.511875   \n",
       "155  0.525478  0.743138       0.650591  0.896135       0.747117  0.504712   \n",
       "156  0.445400  0.759827       0.592218  0.998727       0.718166  0.470838   \n",
       "157  0.675823  0.663516       0.552594  0.787680       0.777722  0.653145   \n",
       "158  0.435712  0.762590       0.598814  1.006578       0.715950  0.462404   \n",
       "159  0.736167  0.637874       0.495982  0.724036       0.795682  0.695098   \n",
       "160  0.468627  0.758251       0.660417  0.982653       0.722702  0.478973   \n",
       "161  0.531155  0.737551       0.583274  0.892246       0.748214  0.475383   \n",
       "162  0.677915  0.677873       0.572106  0.774922       0.781322  0.623996   \n",
       "163  0.373999  0.779665       0.602904  1.054485       0.702431  0.407944   \n",
       "164  0.552625  0.755980       0.606267  0.962445       0.728404  0.493804   \n",
       "\n",
       "     eval_mae  eval_median_ae  eval_mse  eval_set_score  method_ids  \\\n",
       "0    0.616625        0.441952  0.720915        0.787241  [7, 0, 11]   \n",
       "1    0.612576        0.466975  0.721839        0.786968  [8, 0, 11]   \n",
       "2    0.615025        0.434259  0.721846        0.786966  [5, 2, 11]   \n",
       "3    0.612471        0.465889  0.721890        0.786953  [8, 0, 11]   \n",
       "4    0.615207        0.435337  0.721912        0.786947  [5, 2, 11]   \n",
       "5    0.612238        0.459401  0.721978        0.786927  [8, 0, 11]   \n",
       "6    0.615465        0.436924  0.722020        0.786915  [5, 2, 11]   \n",
       "7    0.627613        0.460684  0.736574        0.782620  [3, 2, 11]   \n",
       "8    0.618592        0.453992  0.738298        0.782111  [5, 2, 11]   \n",
       "9    0.624094        0.438201  0.738805        0.781961  [4, 2, 11]   \n",
       "10   0.628156        0.458054  0.741350        0.781210  [6, 0, 11]   \n",
       "11   0.621593        0.435541  0.746938        0.779561  [6, 2, 11]   \n",
       "12   0.625715        0.495734  0.747283        0.779459  [8, 2, 11]   \n",
       "13   0.621469        0.437635  0.747285        0.779459  [6, 2, 11]   \n",
       "14   0.625743        0.495719  0.747314        0.779450  [8, 2, 11]   \n",
       "15   0.621457        0.437951  0.747320        0.779448  [6, 2, 11]   \n",
       "16   0.623567        0.465434  0.748569        0.779080  [8, 0, 11]   \n",
       "17   0.630329        0.460665  0.748615        0.779066  [7, 0, 11]   \n",
       "18   0.623663        0.467851  0.748642        0.779058  [8, 0, 11]   \n",
       "19   0.630404        0.461174  0.748741        0.779029  [7, 0, 11]   \n",
       "20   0.624397        0.481655  0.749268        0.778873  [8, 0, 11]   \n",
       "21   0.641784        0.466925  0.760394        0.775590  [3, 2, 11]   \n",
       "22   0.634194        0.470239  0.760898        0.775441  [6, 0, 11]   \n",
       "23   0.640131        0.460990  0.760953        0.775425   [4, 2, 9]   \n",
       "24   0.635869        0.458075  0.761125        0.775374  [7, 2, 11]   \n",
       "25   0.642356        0.468621  0.761136        0.775371   [3, 2, 7]   \n",
       "26   0.635913        0.457949  0.761176        0.775359  [7, 2, 11]   \n",
       "27   0.642497        0.468358  0.761319        0.775317   [3, 2, 7]   \n",
       "28   0.636078        0.458020  0.761378        0.775300  [7, 2, 11]   \n",
       "29   0.642587        0.468873  0.761425        0.775285  [3, 2, 11]   \n",
       "..        ...             ...       ...             ...         ...   \n",
       "135  0.675239        0.505763  0.837002        0.752981  [5, 1, 11]   \n",
       "136  0.708632        0.570301  0.841065        0.751782   [2, 2, 9]   \n",
       "137  0.658499        0.512103  0.843655        0.751017   [6, 2, 9]   \n",
       "138  0.666046        0.481189  0.845424        0.750495  [3, 2, 10]   \n",
       "139  0.675835        0.501723  0.849062        0.749422  [8, 1, 11]   \n",
       "140  0.700044        0.541446  0.850542        0.748985   [8, 1, 7]   \n",
       "141  0.676669        0.500438  0.850770        0.748918  [8, 1, 11]   \n",
       "142  0.702526        0.544397  0.855595        0.747494   [8, 1, 7]   \n",
       "143  0.682678        0.485593  0.859571        0.746320   [8, 2, 4]   \n",
       "144  0.684844        0.497613  0.863123        0.745272  [8, 1, 11]   \n",
       "145  0.669490        0.470071  0.864982        0.744724  [4, 2, 10]   \n",
       "146  0.686017        0.499541  0.865731        0.744503  [8, 1, 11]   \n",
       "147  0.685192        0.506913  0.868250        0.743759  [6, 1, 11]   \n",
       "148  0.697973        0.546733  0.878031        0.740872  [2, 2, 10]   \n",
       "149  0.692700        0.512165  0.879264        0.740509   [7, 2, 4]   \n",
       "150  0.693306        0.509715  0.879486        0.740443  [7, 1, 11]   \n",
       "151  0.698704        0.555863  0.880352        0.740187   [2, 2, 3]   \n",
       "152  0.693819        0.510340  0.880572        0.740123  [7, 1, 11]   \n",
       "153  0.697610        0.525565  0.884596        0.738935   [2, 2, 4]   \n",
       "154  0.692870        0.473515  0.885048        0.738801  [7, 1, 11]   \n",
       "155  0.700073        0.535032  0.895630        0.735679   [6, 2, 4]   \n",
       "156  0.700127        0.486327  0.896699        0.735363  [7, 1, 11]   \n",
       "157  0.682258        0.539400  0.901762        0.733869   [5, 1, 9]   \n",
       "158  0.702793        0.488876  0.902035        0.733788  [7, 1, 11]   \n",
       "159  0.671311        0.486387  0.903170        0.733453   [5, 1, 9]   \n",
       "160  0.703630        0.489093  0.905926        0.732640  [7, 1, 11]   \n",
       "161  0.707195        0.543245  0.908914        0.731758   [7, 0, 4]   \n",
       "162  0.720576        0.548083  0.927032        0.726411   [6, 1, 9]   \n",
       "163  0.720262        0.516974  0.936280        0.723682  [7, 1, 11]   \n",
       "164  0.761326        0.664197  1.000121        0.704841  [8, 2, 10]   \n",
       "\n",
       "                                            parameters  rank_test_score  \n",
       "0    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...                0  \n",
       "1    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               13  \n",
       "2    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               14  \n",
       "3    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               18  \n",
       "4    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               19  \n",
       "5    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               21  \n",
       "6    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               29  \n",
       "7    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               76  \n",
       "8    {u'normalize': True, u'l1_ratio': 0.4, u'n_alp...               77  \n",
       "9    {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...               96  \n",
       "10   {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...              136  \n",
       "11   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              155  \n",
       "12   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              158  \n",
       "13   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              159  \n",
       "14   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              161  \n",
       "15   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              162  \n",
       "16   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              172  \n",
       "17   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              177  \n",
       "18   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              179  \n",
       "19   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              184  \n",
       "20   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              187  \n",
       "21   {u'normalize': True, u'l1_ratio': 1.0, u'n_alp...              392  \n",
       "22   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              396  \n",
       "23   {u'normalize': True, u'positive': False, u'cri...              406  \n",
       "24   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              409  \n",
       "25   {u'normalize': True, u'n_alphas': 260, u'fit_i...              410  \n",
       "26   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              412  \n",
       "27   {u'normalize': True, u'n_alphas': 310, u'fit_i...              414  \n",
       "28   {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...              416  \n",
       "29   {u'normalize': True, u'l1_ratio': 1.0, u'n_alp...              417  \n",
       "..                                                 ...              ...  \n",
       "135  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1247  \n",
       "136  {u'normalize': True, u'positive': False, u'cri...             1262  \n",
       "137  {u'normalize': False, u'positive': False, u'cr...             1263  \n",
       "138  {u'normalize': True, u'alpha': 0, u'l1_ratio':...             1264  \n",
       "139  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1276  \n",
       "140  {u'normalize': True, u'n_alphas': 160, u'fit_i...             1280  \n",
       "141  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1284  \n",
       "142  {u'normalize': True, u'n_alphas': 210, u'fit_i...             1306  \n",
       "143       {u'normalize': True, u'fit_intercept': True}             1339  \n",
       "144  {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...             1340  \n",
       "145  {u'normalize': False, u'alpha': 0, u'l1_ratio'...             1345  \n",
       "146  {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...             1357  \n",
       "147  {u'normalize': True, u'l1_ratio': 0.4, u'n_alp...             1362  \n",
       "148  {u'normalize': False, u'alpha': 0, u'l1_ratio'...             1372  \n",
       "149       {u'normalize': True, u'fit_intercept': True}             1384  \n",
       "150  {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...             1385  \n",
       "151      {u'normalize': True, u'fit_intercept': False}             1387  \n",
       "152  {u'normalize': True, u'l1_ratio': 0.8, u'n_alp...             1389  \n",
       "153     {u'normalize': False, u'fit_intercept': False}             1395  \n",
       "154  {u'normalize': True, u'l1_ratio': 0.4, u'n_alp...             1397  \n",
       "155       {u'normalize': True, u'fit_intercept': True}             1410  \n",
       "156  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1411  \n",
       "157  {u'normalize': True, u'positive': False, u'cri...             1419  \n",
       "158  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1421  \n",
       "159  {u'normalize': False, u'positive': False, u'cr...             1422  \n",
       "160  {u'normalize': True, u'l1_ratio': 0.4, u'n_alp...             1423  \n",
       "161       {u'normalize': True, u'fit_intercept': True}             1424  \n",
       "162  {u'normalize': True, u'positive': False, u'cri...             1426  \n",
       "163  {u'normalize': True, u'l1_ratio': 0.6, u'n_alp...             1428  \n",
       "164  {u'normalize': True, u'alpha': 10, u'l1_ratio'...             1430  \n",
       "\n",
       "[165 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff.analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f8fe4d4d2f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024165         0.004869         0.467161          0.912829   \n",
      "1       0.063295         0.013676         0.492309          0.917294   \n",
      "2       0.097568         0.019110         0.504288          0.923250   \n",
      "3       0.135895         0.025160         0.491100          0.926304   \n",
      "4       0.052104         0.005416         0.472136          0.909373   \n",
      "5       0.133042         0.012063         0.488230          0.924929   \n",
      "6       0.213503         0.019280         0.497451          0.927842   \n",
      "7       0.297030         0.025394         0.501210          0.928453   \n",
      "0       0.022245         0.004949         0.412033          0.999914   \n",
      "1       0.055298         0.013039         0.421612          0.999914   \n",
      "2       0.090962         0.019441         0.427383          0.999914   \n",
      "3       0.123162         0.027238         0.448949          0.999914   \n",
      "4       0.053856         0.006441         0.424675          0.999914   \n",
      "5       0.131324         0.011887         0.467587          0.999914   \n",
      "6       0.210276         0.018615         0.451756          0.999914   \n",
      "7       0.283644         0.029835         0.436431          0.999914   \n",
      "\n",
      "  method_ids param_criterion param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 10   \n",
      "1  [0, 0, 1]             mse                 25   \n",
      "2  [0, 0, 1]             mse                 40   \n",
      "3  [0, 0, 1]             mse                 55   \n",
      "4  [0, 0, 1]             mae                 10   \n",
      "5  [0, 0, 1]             mae                 25   \n",
      "6  [0, 0, 1]             mae                 40   \n",
      "7  [0, 0, 1]             mae                 55   \n",
      "0  [0, 0, 2]             mse                 10   \n",
      "1  [0, 0, 2]             mse                 25   \n",
      "2  [0, 0, 2]             mse                 40   \n",
      "3  [0, 0, 2]             mse                 55   \n",
      "4  [0, 0, 2]             mae                 10   \n",
      "5  [0, 0, 2]             mae                 25   \n",
      "6  [0, 0, 2]             mae                 40   \n",
      "7  [0, 0, 2]             mae                 55   \n",
      "\n",
      "                                        params  rank_test_score  \\\n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                4   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                1   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                5   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                7   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                6   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                3   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                2   \n",
      "0  {u'n_estimators': 10, u'criterion': u'mse'}                8   \n",
      "1  {u'n_estimators': 25, u'criterion': u'mse'}                7   \n",
      "2  {u'n_estimators': 40, u'criterion': u'mse'}                5   \n",
      "3  {u'n_estimators': 55, u'criterion': u'mse'}                3   \n",
      "4  {u'n_estimators': 10, u'criterion': u'mae'}                6   \n",
      "5  {u'n_estimators': 25, u'criterion': u'mae'}                1   \n",
      "6  {u'n_estimators': 40, u'criterion': u'mae'}                2   \n",
      "7  {u'n_estimators': 55, u'criterion': u'mae'}                4   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.405206            0.900516           0.437757   \n",
      "1           0.494276            0.923324           0.405933   \n",
      "2           0.467146            0.929035           0.452861   \n",
      "3           0.442689            0.925783           0.432066   \n",
      "4           0.439291            0.894901           0.397995   \n",
      "5           0.443830            0.926134           0.450539   \n",
      "6           0.483314            0.930272           0.425251   \n",
      "7           0.496800            0.929725           0.390728   \n",
      "0           0.283188            0.999999           0.395423   \n",
      "1           0.320898            0.999999           0.389036   \n",
      "2           0.309973            0.999999           0.397995   \n",
      "3           0.320773            0.999999           0.421707   \n",
      "4           0.334430            0.999999           0.358684   \n",
      "5           0.328006            0.999999           0.448363   \n",
      "6           0.319685            0.999999           0.411515   \n",
      "7           0.311854            0.999999           0.392534   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.918215           0.559186            0.919755      0.000108   \n",
      "1            0.925264           0.576696            0.903296      0.003123   \n",
      "2            0.933157           0.593257            0.907558      0.003846   \n",
      "3            0.937133           0.599064            0.915997      0.000581   \n",
      "4            0.919887           0.579476            0.913330      0.001840   \n",
      "5            0.935124           0.570797            0.913528      0.001926   \n",
      "6            0.936006           0.583939            0.917249      0.000627   \n",
      "7            0.934790           0.616149            0.920845      0.008962   \n",
      "0            0.999976           0.558876            0.999767      0.000651   \n",
      "1            0.999976           0.555985            0.999767      0.001725   \n",
      "2            0.999976           0.575444            0.999767      0.003258   \n",
      "3            0.999976           0.605744            0.999767      0.007825   \n",
      "4            0.999976           0.581883            0.999767      0.002929   \n",
      "5            0.999976           0.627892            0.999767      0.002699   \n",
      "6            0.999976           0.625488            0.999767      0.008569   \n",
      "7            0.999976           0.606243            0.999767      0.002479   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        0.000159        0.066246         0.008729  \n",
      "1        0.001511        0.069603         0.009930  \n",
      "2        0.000704        0.063013         0.011223  \n",
      "3        0.000289        0.076261         0.008637  \n",
      "4        0.000344        0.077556         0.010577  \n",
      "5        0.000172        0.058292         0.008858  \n",
      "6        0.000468        0.065445         0.007848  \n",
      "7        0.000330        0.091917         0.005764  \n",
      "0        0.000149        0.113255         0.000104  \n",
      "1        0.001890        0.098767         0.000104  \n",
      "2        0.000755        0.110435         0.000104  \n",
      "3        0.002300        0.118016         0.000104  \n",
      "4        0.000910        0.111307         0.000104  \n",
      "5        0.000089        0.123283         0.000104  \n",
      "6        0.000113        0.128135         0.000104  \n",
      "7        0.001238        0.124209         0.000104  \n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024165         0.004869         0.467161          0.912829   \n",
      "1       0.063295         0.013676         0.492309          0.917294   \n",
      "2       0.097568         0.019110         0.504288          0.923250   \n",
      "3       0.135895         0.025160         0.491100          0.926304   \n",
      "4       0.052104         0.005416         0.472136          0.909373   \n",
      "5       0.133042         0.012063         0.488230          0.924929   \n",
      "6       0.213503         0.019280         0.497451          0.927842   \n",
      "7       0.297030         0.025394         0.501210          0.928453   \n",
      "0       0.022245         0.004949         0.412033          0.999914   \n",
      "1       0.055298         0.013039         0.421612          0.999914   \n",
      "2       0.090962         0.019441         0.427383          0.999914   \n",
      "3       0.123162         0.027238         0.448949          0.999914   \n",
      "4       0.053856         0.006441         0.424675          0.999914   \n",
      "5       0.131324         0.011887         0.467587          0.999914   \n",
      "6       0.210276         0.018615         0.451756          0.999914   \n",
      "7       0.283644         0.029835         0.436431          0.999914   \n",
      "0       0.001577         0.000264         0.432161          0.494461   \n",
      "1       0.000414         0.000186         0.432161          0.494461   \n",
      "2       0.000347         0.000180         0.435058          0.489384   \n",
      "3       0.000346         0.000180         0.435058          0.489384   \n",
      "\n",
      "  method_ids param_criterion param_fit_intercept param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 NaN                 10   \n",
      "1  [0, 0, 1]             mse                 NaN                 25   \n",
      "2  [0, 0, 1]             mse                 NaN                 40   \n",
      "3  [0, 0, 1]             mse                 NaN                 55   \n",
      "4  [0, 0, 1]             mae                 NaN                 10   \n",
      "5  [0, 0, 1]             mae                 NaN                 25   \n",
      "6  [0, 0, 1]             mae                 NaN                 40   \n",
      "7  [0, 0, 1]             mae                 NaN                 55   \n",
      "0  [0, 0, 2]             mse                 NaN                 10   \n",
      "1  [0, 0, 2]             mse                 NaN                 25   \n",
      "2  [0, 0, 2]             mse                 NaN                 40   \n",
      "3  [0, 0, 2]             mse                 NaN                 55   \n",
      "4  [0, 0, 2]             mae                 NaN                 10   \n",
      "5  [0, 0, 2]             mae                 NaN                 25   \n",
      "6  [0, 0, 2]             mae                 NaN                 40   \n",
      "7  [0, 0, 2]             mae                 NaN                 55   \n",
      "0  [0, 0, 3]             NaN                True                NaN   \n",
      "1  [0, 0, 3]             NaN                True                NaN   \n",
      "2  [0, 0, 3]             NaN               False                NaN   \n",
      "3  [0, 0, 3]             NaN               False                NaN   \n",
      "\n",
      "  param_normalize                                          params  \\\n",
      "0             NaN     {u'n_estimators': 10, u'criterion': u'mse'}   \n",
      "1             NaN     {u'n_estimators': 25, u'criterion': u'mse'}   \n",
      "2             NaN     {u'n_estimators': 40, u'criterion': u'mse'}   \n",
      "3             NaN     {u'n_estimators': 55, u'criterion': u'mse'}   \n",
      "4             NaN     {u'n_estimators': 10, u'criterion': u'mae'}   \n",
      "5             NaN     {u'n_estimators': 25, u'criterion': u'mae'}   \n",
      "6             NaN     {u'n_estimators': 40, u'criterion': u'mae'}   \n",
      "7             NaN     {u'n_estimators': 55, u'criterion': u'mae'}   \n",
      "0             NaN     {u'n_estimators': 10, u'criterion': u'mse'}   \n",
      "1             NaN     {u'n_estimators': 25, u'criterion': u'mse'}   \n",
      "2             NaN     {u'n_estimators': 40, u'criterion': u'mse'}   \n",
      "3             NaN     {u'n_estimators': 55, u'criterion': u'mse'}   \n",
      "4             NaN     {u'n_estimators': 10, u'criterion': u'mae'}   \n",
      "5             NaN     {u'n_estimators': 25, u'criterion': u'mae'}   \n",
      "6             NaN     {u'n_estimators': 40, u'criterion': u'mae'}   \n",
      "7             NaN     {u'n_estimators': 55, u'criterion': u'mae'}   \n",
      "0            True    {u'normalize': True, u'fit_intercept': True}   \n",
      "1           False   {u'normalize': False, u'fit_intercept': True}   \n",
      "2            True   {u'normalize': True, u'fit_intercept': False}   \n",
      "3           False  {u'normalize': False, u'fit_intercept': False}   \n",
      "\n",
      "        ...         split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0       ...                  0.405206            0.900516           0.437757   \n",
      "1       ...                  0.494276            0.923324           0.405933   \n",
      "2       ...                  0.467146            0.929035           0.452861   \n",
      "3       ...                  0.442689            0.925783           0.432066   \n",
      "4       ...                  0.439291            0.894901           0.397995   \n",
      "5       ...                  0.443830            0.926134           0.450539   \n",
      "6       ...                  0.483314            0.930272           0.425251   \n",
      "7       ...                  0.496800            0.929725           0.390728   \n",
      "0       ...                  0.283188            0.999999           0.395423   \n",
      "1       ...                  0.320898            0.999999           0.389036   \n",
      "2       ...                  0.309973            0.999999           0.397995   \n",
      "3       ...                  0.320773            0.999999           0.421707   \n",
      "4       ...                  0.334430            0.999999           0.358684   \n",
      "5       ...                  0.328006            0.999999           0.448363   \n",
      "6       ...                  0.319685            0.999999           0.411515   \n",
      "7       ...                  0.311854            0.999999           0.392534   \n",
      "0       ...                  0.477591            0.463774           0.293992   \n",
      "1       ...                  0.477591            0.463774           0.293992   \n",
      "2       ...                  0.474478            0.463719           0.307132   \n",
      "3       ...                  0.474478            0.463719           0.307132   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.918215           0.559186            0.919755      0.000108   \n",
      "1            0.925264           0.576696            0.903296      0.003123   \n",
      "2            0.933157           0.593257            0.907558      0.003846   \n",
      "3            0.937133           0.599064            0.915997      0.000581   \n",
      "4            0.919887           0.579476            0.913330      0.001840   \n",
      "5            0.935124           0.570797            0.913528      0.001926   \n",
      "6            0.936006           0.583939            0.917249      0.000627   \n",
      "7            0.934790           0.616149            0.920845      0.008962   \n",
      "0            0.999976           0.558876            0.999767      0.000651   \n",
      "1            0.999976           0.555985            0.999767      0.001725   \n",
      "2            0.999976           0.575444            0.999767      0.003258   \n",
      "3            0.999976           0.605744            0.999767      0.007825   \n",
      "4            0.999976           0.581883            0.999767      0.002929   \n",
      "5            0.999976           0.627892            0.999767      0.002699   \n",
      "6            0.999976           0.625488            0.999767      0.008569   \n",
      "7            0.999976           0.606243            0.999767      0.002479   \n",
      "0            0.567092           0.524410            0.452518      0.001449   \n",
      "1            0.567092           0.524410            0.452518      0.000012   \n",
      "2            0.556049           0.523140            0.448383      0.000006   \n",
      "3            0.556049           0.523140            0.448383      0.000009   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0    1.590706e-04        0.066246         0.008729  \n",
      "1    1.511005e-03        0.069603         0.009930  \n",
      "2    7.039070e-04        0.063013         0.011223  \n",
      "3    2.888158e-04        0.076261         0.008637  \n",
      "4    3.442162e-04        0.077556         0.010577  \n",
      "5    1.717574e-04        0.058292         0.008858  \n",
      "6    4.681239e-04        0.065445         0.007848  \n",
      "7    3.297199e-04        0.091917         0.005764  \n",
      "0    1.492319e-04        0.113255         0.000104  \n",
      "1    1.890016e-03        0.098767         0.000104  \n",
      "2    7.548232e-04        0.110435         0.000104  \n",
      "3    2.299589e-03        0.118016         0.000104  \n",
      "4    9.104882e-04        0.111307         0.000104  \n",
      "5    8.946138e-05        0.123283         0.000104  \n",
      "6    1.126224e-04        0.128135         0.000104  \n",
      "7    1.238335e-03        0.124209         0.000104  \n",
      "0    7.148106e-05        0.099298         0.051562  \n",
      "1    5.857317e-06        0.099298         0.051562  \n",
      "2    0.000000e+00        0.092380         0.047554  \n",
      "3    9.602742e-07        0.092380         0.047554  \n",
      "\n",
      "[20 rows x 21 columns]\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0       0.024165         0.004869         0.467161          0.912829   \n",
      "1       0.063295         0.013676         0.492309          0.917294   \n",
      "2       0.097568         0.019110         0.504288          0.923250   \n",
      "3       0.135895         0.025160         0.491100          0.926304   \n",
      "4       0.052104         0.005416         0.472136          0.909373   \n",
      "5       0.133042         0.012063         0.488230          0.924929   \n",
      "6       0.213503         0.019280         0.497451          0.927842   \n",
      "7       0.297030         0.025394         0.501210          0.928453   \n",
      "0       0.022245         0.004949         0.412033          0.999914   \n",
      "1       0.055298         0.013039         0.421612          0.999914   \n",
      "2       0.090962         0.019441         0.427383          0.999914   \n",
      "3       0.123162         0.027238         0.448949          0.999914   \n",
      "4       0.053856         0.006441         0.424675          0.999914   \n",
      "5       0.131324         0.011887         0.467587          0.999914   \n",
      "6       0.210276         0.018615         0.451756          0.999914   \n",
      "7       0.283644         0.029835         0.436431          0.999914   \n",
      "0       0.001577         0.000264         0.432161          0.494461   \n",
      "1       0.000414         0.000186         0.432161          0.494461   \n",
      "2       0.000347         0.000180         0.435058          0.489384   \n",
      "3       0.000346         0.000180         0.435058          0.489384   \n",
      "0       0.000483         0.000179         0.358135          0.409781   \n",
      "1       0.000383         0.000173         0.434713          0.494294   \n",
      "2       0.000341         0.000176         0.438055          0.489173   \n",
      "3       0.000336         0.000175         0.438055          0.489173   \n",
      "\n",
      "  method_ids param_criterion param_fit_intercept param_n_estimators  \\\n",
      "0  [0, 0, 1]             mse                 NaN                 10   \n",
      "1  [0, 0, 1]             mse                 NaN                 25   \n",
      "2  [0, 0, 1]             mse                 NaN                 40   \n",
      "3  [0, 0, 1]             mse                 NaN                 55   \n",
      "4  [0, 0, 1]             mae                 NaN                 10   \n",
      "5  [0, 0, 1]             mae                 NaN                 25   \n",
      "6  [0, 0, 1]             mae                 NaN                 40   \n",
      "7  [0, 0, 1]             mae                 NaN                 55   \n",
      "0  [0, 0, 2]             mse                 NaN                 10   \n",
      "1  [0, 0, 2]             mse                 NaN                 25   \n",
      "2  [0, 0, 2]             mse                 NaN                 40   \n",
      "3  [0, 0, 2]             mse                 NaN                 55   \n",
      "4  [0, 0, 2]             mae                 NaN                 10   \n",
      "5  [0, 0, 2]             mae                 NaN                 25   \n",
      "6  [0, 0, 2]             mae                 NaN                 40   \n",
      "7  [0, 0, 2]             mae                 NaN                 55   \n",
      "0  [0, 0, 3]             NaN                True                NaN   \n",
      "1  [0, 0, 3]             NaN                True                NaN   \n",
      "2  [0, 0, 3]             NaN               False                NaN   \n",
      "3  [0, 0, 3]             NaN               False                NaN   \n",
      "0  [0, 0, 4]             NaN                True                NaN   \n",
      "1  [0, 0, 4]             NaN                True                NaN   \n",
      "2  [0, 0, 4]             NaN               False                NaN   \n",
      "3  [0, 0, 4]             NaN               False                NaN   \n",
      "\n",
      "  param_normalize                                          params  \\\n",
      "0             NaN     {u'n_estimators': 10, u'criterion': u'mse'}   \n",
      "1             NaN     {u'n_estimators': 25, u'criterion': u'mse'}   \n",
      "2             NaN     {u'n_estimators': 40, u'criterion': u'mse'}   \n",
      "3             NaN     {u'n_estimators': 55, u'criterion': u'mse'}   \n",
      "4             NaN     {u'n_estimators': 10, u'criterion': u'mae'}   \n",
      "5             NaN     {u'n_estimators': 25, u'criterion': u'mae'}   \n",
      "6             NaN     {u'n_estimators': 40, u'criterion': u'mae'}   \n",
      "7             NaN     {u'n_estimators': 55, u'criterion': u'mae'}   \n",
      "0             NaN     {u'n_estimators': 10, u'criterion': u'mse'}   \n",
      "1             NaN     {u'n_estimators': 25, u'criterion': u'mse'}   \n",
      "2             NaN     {u'n_estimators': 40, u'criterion': u'mse'}   \n",
      "3             NaN     {u'n_estimators': 55, u'criterion': u'mse'}   \n",
      "4             NaN     {u'n_estimators': 10, u'criterion': u'mae'}   \n",
      "5             NaN     {u'n_estimators': 25, u'criterion': u'mae'}   \n",
      "6             NaN     {u'n_estimators': 40, u'criterion': u'mae'}   \n",
      "7             NaN     {u'n_estimators': 55, u'criterion': u'mae'}   \n",
      "0            True    {u'normalize': True, u'fit_intercept': True}   \n",
      "1           False   {u'normalize': False, u'fit_intercept': True}   \n",
      "2            True   {u'normalize': True, u'fit_intercept': False}   \n",
      "3           False  {u'normalize': False, u'fit_intercept': False}   \n",
      "0            True    {u'normalize': True, u'fit_intercept': True}   \n",
      "1           False   {u'normalize': False, u'fit_intercept': True}   \n",
      "2            True   {u'normalize': True, u'fit_intercept': False}   \n",
      "3           False  {u'normalize': False, u'fit_intercept': False}   \n",
      "\n",
      "        ...         split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0       ...                  0.405206            0.900516           0.437757   \n",
      "1       ...                  0.494276            0.923324           0.405933   \n",
      "2       ...                  0.467146            0.929035           0.452861   \n",
      "3       ...                  0.442689            0.925783           0.432066   \n",
      "4       ...                  0.439291            0.894901           0.397995   \n",
      "5       ...                  0.443830            0.926134           0.450539   \n",
      "6       ...                  0.483314            0.930272           0.425251   \n",
      "7       ...                  0.496800            0.929725           0.390728   \n",
      "0       ...                  0.283188            0.999999           0.395423   \n",
      "1       ...                  0.320898            0.999999           0.389036   \n",
      "2       ...                  0.309973            0.999999           0.397995   \n",
      "3       ...                  0.320773            0.999999           0.421707   \n",
      "4       ...                  0.334430            0.999999           0.358684   \n",
      "5       ...                  0.328006            0.999999           0.448363   \n",
      "6       ...                  0.319685            0.999999           0.411515   \n",
      "7       ...                  0.311854            0.999999           0.392534   \n",
      "0       ...                  0.477591            0.463774           0.293992   \n",
      "1       ...                  0.477591            0.463774           0.293992   \n",
      "2       ...                  0.474478            0.463719           0.307132   \n",
      "3       ...                  0.474478            0.463719           0.307132   \n",
      "0       ...                  0.346831            0.382781           0.316059   \n",
      "1       ...                  0.481139            0.463593           0.294810   \n",
      "2       ...                  0.478163            0.463543           0.308503   \n",
      "3       ...                  0.478163            0.463543           0.308503   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.918215           0.559186            0.919755      0.000108   \n",
      "1            0.925264           0.576696            0.903296      0.003123   \n",
      "2            0.933157           0.593257            0.907558      0.003846   \n",
      "3            0.937133           0.599064            0.915997      0.000581   \n",
      "4            0.919887           0.579476            0.913330      0.001840   \n",
      "5            0.935124           0.570797            0.913528      0.001926   \n",
      "6            0.936006           0.583939            0.917249      0.000627   \n",
      "7            0.934790           0.616149            0.920845      0.008962   \n",
      "0            0.999976           0.558876            0.999767      0.000651   \n",
      "1            0.999976           0.555985            0.999767      0.001725   \n",
      "2            0.999976           0.575444            0.999767      0.003258   \n",
      "3            0.999976           0.605744            0.999767      0.007825   \n",
      "4            0.999976           0.581883            0.999767      0.002929   \n",
      "5            0.999976           0.627892            0.999767      0.002699   \n",
      "6            0.999976           0.625488            0.999767      0.008569   \n",
      "7            0.999976           0.606243            0.999767      0.002479   \n",
      "0            0.567092           0.524410            0.452518      0.001449   \n",
      "1            0.567092           0.524410            0.452518      0.000012   \n",
      "2            0.556049           0.523140            0.448383      0.000006   \n",
      "3            0.556049           0.523140            0.448383      0.000009   \n",
      "0            0.465484           0.411636            0.381077      0.000077   \n",
      "1            0.567061           0.527691            0.452229      0.000004   \n",
      "2            0.555975           0.527068            0.448002      0.000002   \n",
      "3            0.555975           0.527068            0.448002      0.000006   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0    1.590706e-04        0.066246         0.008729  \n",
      "1    1.511005e-03        0.069603         0.009930  \n",
      "2    7.039070e-04        0.063013         0.011223  \n",
      "3    2.888158e-04        0.076261         0.008637  \n",
      "4    3.442162e-04        0.077556         0.010577  \n",
      "5    1.717574e-04        0.058292         0.008858  \n",
      "6    4.681239e-04        0.065445         0.007848  \n",
      "7    3.297199e-04        0.091917         0.005764  \n",
      "0    1.492319e-04        0.113255         0.000104  \n",
      "1    1.890016e-03        0.098767         0.000104  \n",
      "2    7.548232e-04        0.110435         0.000104  \n",
      "3    2.299589e-03        0.118016         0.000104  \n",
      "4    9.104882e-04        0.111307         0.000104  \n",
      "5    8.946138e-05        0.123283         0.000104  \n",
      "6    1.126224e-04        0.128135         0.000104  \n",
      "7    1.238335e-03        0.124209         0.000104  \n",
      "0    7.148106e-05        0.099298         0.051562  \n",
      "1    5.857317e-06        0.099298         0.051562  \n",
      "2    0.000000e+00        0.092380         0.047554  \n",
      "3    9.602742e-07        0.092380         0.047554  \n",
      "0    7.636841e-06        0.039770         0.039394  \n",
      "1    5.150430e-07        0.100478         0.051662  \n",
      "2    7.786718e-07        0.093522         0.047660  \n",
      "3    7.786718e-07        0.093522         0.047660  \n",
      "\n",
      "[24 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "10       0.000354         0.000190         0.241141          0.267017   \n",
      "11       0.000320         0.000187         0.241141          0.267017   \n",
      "12       0.000400         0.000191        -0.066149          0.000000   \n",
      "13       0.000378         0.000186        -0.065550          0.000715   \n",
      "14       0.000313         0.000186         0.218408          0.244866   \n",
      "15       0.000341         0.000194         0.218408          0.244866   \n",
      "16       0.000802         0.000230        -0.066149          0.000000   \n",
      "17       0.000419         0.000205        -0.066149          0.000000   \n",
      "18       0.000330         0.000186         0.189198          0.216385   \n",
      "19       0.000307         0.000186         0.189198          0.216385   \n",
      "20       0.000391         0.000185        -0.066149          0.000000   \n",
      "21       0.000366         0.000184        -0.066149          0.000000   \n",
      "22       0.000321         0.000187         0.153510          0.181576   \n",
      "23       0.000348         0.000194         0.153510          0.181576   \n",
      "24       0.000539         0.000231        -0.066149          0.000000   \n",
      "25       0.000441         0.000218        -0.066149          0.000000   \n",
      "26       0.000344         0.000190         0.111344          0.140438   \n",
      "27       0.000325         0.000189         0.111344          0.140438   \n",
      "28       0.000421         0.000186        -0.066149          0.000000   \n",
      "29       0.000487         0.000237        -0.066149          0.000000   \n",
      "30       0.000350         0.000194         0.062701          0.092970   \n",
      "31       0.000305         0.000186         0.062701          0.092970   \n",
      "32       0.000417         0.000197        -0.066149          0.000000   \n",
      "33       0.000338         0.000182        -0.066149          0.000000   \n",
      "34       0.000356         0.000203         0.007580          0.039174   \n",
      "35       0.000299         0.000185         0.007580          0.039174   \n",
      "36       0.000385         0.000183        -0.066149          0.000000   \n",
      "37       0.000332         0.000184        -0.066149          0.000000   \n",
      "38       0.000293         0.000183        -0.054019         -0.020951   \n",
      "39       0.000294         0.000184        -0.054019         -0.020951   \n",
      "\n",
      "   method_ids param_alpha param_criterion param_fit_intercept  \\\n",
      "0   [0, 0, 1]         NaN             mse                 NaN   \n",
      "1   [0, 0, 1]         NaN             mse                 NaN   \n",
      "2   [0, 0, 1]         NaN             mse                 NaN   \n",
      "3   [0, 0, 1]         NaN             mse                 NaN   \n",
      "4   [0, 0, 1]         NaN             mae                 NaN   \n",
      "5   [0, 0, 1]         NaN             mae                 NaN   \n",
      "6   [0, 0, 1]         NaN             mae                 NaN   \n",
      "7   [0, 0, 1]         NaN             mae                 NaN   \n",
      "0   [0, 0, 2]         NaN             mse                 NaN   \n",
      "1   [0, 0, 2]         NaN             mse                 NaN   \n",
      "2   [0, 0, 2]         NaN             mse                 NaN   \n",
      "3   [0, 0, 2]         NaN             mse                 NaN   \n",
      "4   [0, 0, 2]         NaN             mae                 NaN   \n",
      "5   [0, 0, 2]         NaN             mae                 NaN   \n",
      "6   [0, 0, 2]         NaN             mae                 NaN   \n",
      "7   [0, 0, 2]         NaN             mae                 NaN   \n",
      "0   [0, 0, 3]         NaN             NaN                True   \n",
      "1   [0, 0, 3]         NaN             NaN                True   \n",
      "2   [0, 0, 3]         NaN             NaN               False   \n",
      "3   [0, 0, 3]         NaN             NaN               False   \n",
      "0   [0, 0, 4]         NaN             NaN                True   \n",
      "1   [0, 0, 4]         NaN             NaN                True   \n",
      "2   [0, 0, 4]         NaN             NaN               False   \n",
      "3   [0, 0, 4]         NaN             NaN               False   \n",
      "0   [0, 0, 6]           1             NaN                True   \n",
      "1   [0, 0, 6]           1             NaN                True   \n",
      "2   [0, 0, 6]           1             NaN               False   \n",
      "3   [0, 0, 6]           1             NaN               False   \n",
      "4   [0, 0, 6]           2             NaN                True   \n",
      "5   [0, 0, 6]           2             NaN                True   \n",
      "..        ...         ...             ...                 ...   \n",
      "10  [0, 0, 6]           3             NaN               False   \n",
      "11  [0, 0, 6]           3             NaN               False   \n",
      "12  [0, 0, 6]           4             NaN                True   \n",
      "13  [0, 0, 6]           4             NaN                True   \n",
      "14  [0, 0, 6]           4             NaN               False   \n",
      "15  [0, 0, 6]           4             NaN               False   \n",
      "16  [0, 0, 6]           5             NaN                True   \n",
      "17  [0, 0, 6]           5             NaN                True   \n",
      "18  [0, 0, 6]           5             NaN               False   \n",
      "19  [0, 0, 6]           5             NaN               False   \n",
      "20  [0, 0, 6]           6             NaN                True   \n",
      "21  [0, 0, 6]           6             NaN                True   \n",
      "22  [0, 0, 6]           6             NaN               False   \n",
      "23  [0, 0, 6]           6             NaN               False   \n",
      "24  [0, 0, 6]           7             NaN                True   \n",
      "25  [0, 0, 6]           7             NaN                True   \n",
      "26  [0, 0, 6]           7             NaN               False   \n",
      "27  [0, 0, 6]           7             NaN               False   \n",
      "28  [0, 0, 6]           8             NaN                True   \n",
      "29  [0, 0, 6]           8             NaN                True   \n",
      "30  [0, 0, 6]           8             NaN               False   \n",
      "31  [0, 0, 6]           8             NaN               False   \n",
      "32  [0, 0, 6]           9             NaN                True   \n",
      "33  [0, 0, 6]           9             NaN                True   \n",
      "34  [0, 0, 6]           9             NaN               False   \n",
      "35  [0, 0, 6]           9             NaN               False   \n",
      "36  [0, 0, 6]          10             NaN                True   \n",
      "37  [0, 0, 6]          10             NaN                True   \n",
      "38  [0, 0, 6]          10             NaN               False   \n",
      "39  [0, 0, 6]          10             NaN               False   \n",
      "\n",
      "   param_n_estimators param_normalize       ...        split0_test_score  \\\n",
      "0                  10             NaN       ...                 0.405206   \n",
      "1                  25             NaN       ...                 0.494276   \n",
      "2                  40             NaN       ...                 0.467146   \n",
      "3                  55             NaN       ...                 0.442689   \n",
      "4                  10             NaN       ...                 0.439291   \n",
      "5                  25             NaN       ...                 0.443830   \n",
      "6                  40             NaN       ...                 0.483314   \n",
      "7                  55             NaN       ...                 0.496800   \n",
      "0                  10             NaN       ...                 0.283188   \n",
      "1                  25             NaN       ...                 0.320898   \n",
      "2                  40             NaN       ...                 0.309973   \n",
      "3                  55             NaN       ...                 0.320773   \n",
      "4                  10             NaN       ...                 0.334430   \n",
      "5                  25             NaN       ...                 0.328006   \n",
      "6                  40             NaN       ...                 0.319685   \n",
      "7                  55             NaN       ...                 0.311854   \n",
      "0                 NaN            True       ...                 0.477591   \n",
      "1                 NaN           False       ...                 0.477591   \n",
      "2                 NaN            True       ...                 0.474478   \n",
      "3                 NaN           False       ...                 0.474478   \n",
      "0                 NaN            True       ...                 0.346831   \n",
      "1                 NaN           False       ...                 0.481139   \n",
      "2                 NaN            True       ...                 0.478163   \n",
      "3                 NaN           False       ...                 0.478163   \n",
      "0                 NaN            True       ...                -0.139219   \n",
      "1                 NaN           False       ...                 0.256755   \n",
      "2                 NaN            True       ...                 0.327736   \n",
      "3                 NaN           False       ...                 0.327736   \n",
      "4                 NaN            True       ...                -0.139219   \n",
      "5                 NaN           False       ...                 0.126198   \n",
      "..                ...             ...       ...                      ...   \n",
      "10                NaN            True       ...                 0.310454   \n",
      "11                NaN           False       ...                 0.310454   \n",
      "12                NaN            True       ...                -0.139219   \n",
      "13                NaN           False       ...                -0.139219   \n",
      "14                NaN            True       ...                 0.311027   \n",
      "15                NaN           False       ...                 0.311027   \n",
      "16                NaN            True       ...                -0.139219   \n",
      "17                NaN           False       ...                -0.139219   \n",
      "18                NaN            True       ...                 0.306155   \n",
      "19                NaN           False       ...                 0.306155   \n",
      "20                NaN            True       ...                -0.139219   \n",
      "21                NaN           False       ...                -0.139219   \n",
      "22                NaN            True       ...                 0.295838   \n",
      "23                NaN           False       ...                 0.295838   \n",
      "24                NaN            True       ...                -0.139219   \n",
      "25                NaN           False       ...                -0.139219   \n",
      "26                NaN            True       ...                 0.280076   \n",
      "27                NaN           False       ...                 0.280076   \n",
      "28                NaN            True       ...                -0.139219   \n",
      "29                NaN           False       ...                -0.139219   \n",
      "30                NaN            True       ...                 0.258870   \n",
      "31                NaN           False       ...                 0.258870   \n",
      "32                NaN            True       ...                -0.139219   \n",
      "33                NaN           False       ...                -0.139219   \n",
      "34                NaN            True       ...                 0.232219   \n",
      "35                NaN           False       ...                 0.232219   \n",
      "36                NaN            True       ...                -0.139219   \n",
      "37                NaN           False       ...                -0.139219   \n",
      "38                NaN            True       ...                 0.200123   \n",
      "39                NaN           False       ...                 0.200123   \n",
      "\n",
      "    split0_train_score  split1_test_score  split1_train_score  \\\n",
      "0             0.900516           0.437757            0.918215   \n",
      "1             0.923324           0.405933            0.925264   \n",
      "2             0.929035           0.452861            0.933157   \n",
      "3             0.925783           0.432066            0.937133   \n",
      "4             0.894901           0.397995            0.919887   \n",
      "5             0.926134           0.450539            0.935124   \n",
      "6             0.930272           0.425251            0.936006   \n",
      "7             0.929725           0.390728            0.934790   \n",
      "0             0.999999           0.395423            0.999976   \n",
      "1             0.999999           0.389036            0.999976   \n",
      "2             0.999999           0.397995            0.999976   \n",
      "3             0.999999           0.421707            0.999976   \n",
      "4             0.999999           0.358684            0.999976   \n",
      "5             0.999999           0.448363            0.999976   \n",
      "6             0.999999           0.411515            0.999976   \n",
      "7             0.999999           0.392534            0.999976   \n",
      "0             0.463774           0.293992            0.567092   \n",
      "1             0.463774           0.293992            0.567092   \n",
      "2             0.463719           0.307132            0.556049   \n",
      "3             0.463719           0.307132            0.556049   \n",
      "0             0.382781           0.316059            0.465484   \n",
      "1             0.463593           0.294810            0.567061   \n",
      "2             0.463543           0.308503            0.555975   \n",
      "3             0.463543           0.308503            0.555975   \n",
      "0             0.000000          -0.032746            0.000000   \n",
      "1             0.292910           0.234169            0.345164   \n",
      "2             0.304397           0.233787            0.346858   \n",
      "3             0.304397           0.233787            0.346858   \n",
      "4             0.000000          -0.032746            0.000000   \n",
      "5             0.204443           0.178140            0.257960   \n",
      "..                 ...                ...                 ...   \n",
      "10            0.250649           0.173097            0.298234   \n",
      "11            0.250649           0.173097            0.298234   \n",
      "12            0.000000          -0.032746            0.000000   \n",
      "13            0.000000          -0.030943            0.002145   \n",
      "14            0.230892           0.141567            0.274770   \n",
      "15            0.230892           0.141567            0.274770   \n",
      "16            0.000000          -0.032746            0.000000   \n",
      "17            0.000000          -0.032746            0.000000   \n",
      "18            0.205491           0.102760            0.244603   \n",
      "19            0.205491           0.102760            0.244603   \n",
      "20            0.000000          -0.032746            0.000000   \n",
      "21            0.000000          -0.032746            0.000000   \n",
      "22            0.174445           0.056675            0.207731   \n",
      "23            0.174445           0.056675            0.207731   \n",
      "24            0.000000          -0.032746            0.000000   \n",
      "25            0.000000          -0.032746            0.000000   \n",
      "26            0.137754           0.003313            0.164156   \n",
      "27            0.137754           0.003313            0.164156   \n",
      "28            0.000000          -0.032746            0.000000   \n",
      "29            0.000000          -0.032746            0.000000   \n",
      "30            0.095418          -0.057326            0.113877   \n",
      "31            0.095418          -0.057326            0.113877   \n",
      "32            0.000000          -0.032746            0.000000   \n",
      "33            0.000000          -0.032746            0.000000   \n",
      "34            0.047437          -0.125243            0.056894   \n",
      "35            0.047437          -0.125243            0.056894   \n",
      "36            0.000000          -0.032746            0.000000   \n",
      "37            0.000000          -0.032746            0.000000   \n",
      "38           -0.006188          -0.200438           -0.006793   \n",
      "39           -0.006188          -0.200438           -0.006793   \n",
      "\n",
      "    split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
      "0            0.559186            0.919755  1.081822e-04    1.590706e-04   \n",
      "1            0.576696            0.903296  3.123020e-03    1.511005e-03   \n",
      "2            0.593257            0.907558  3.845929e-03    7.039070e-04   \n",
      "3            0.599064            0.915997  5.805324e-04    2.888158e-04   \n",
      "4            0.579476            0.913330  1.840379e-03    3.442162e-04   \n",
      "5            0.570797            0.913528  1.925634e-03    1.717574e-04   \n",
      "6            0.583939            0.917249  6.273349e-04    4.681239e-04   \n",
      "7            0.616149            0.920845  8.961997e-03    3.297199e-04   \n",
      "0            0.558876            0.999767  6.513482e-04    1.492319e-04   \n",
      "1            0.555985            0.999767  1.725471e-03    1.890016e-03   \n",
      "2            0.575444            0.999767  3.258254e-03    7.548232e-04   \n",
      "3            0.605744            0.999767  7.824721e-03    2.299589e-03   \n",
      "4            0.581883            0.999767  2.928583e-03    9.104882e-04   \n",
      "5            0.627892            0.999767  2.698708e-03    8.946138e-05   \n",
      "6            0.625488            0.999767  8.568505e-03    1.126224e-04   \n",
      "7            0.606243            0.999767  2.479396e-03    1.238335e-03   \n",
      "0            0.524410            0.452518  1.449068e-03    7.148106e-05   \n",
      "1            0.524410            0.452518  1.248154e-05    5.857317e-06   \n",
      "2            0.523140            0.448383  6.344901e-06    0.000000e+00   \n",
      "3            0.523140            0.448383  8.878936e-06    9.602742e-07   \n",
      "0            0.411636            0.381077  7.674025e-05    7.636841e-06   \n",
      "1            0.527691            0.452229  4.103448e-06    5.150430e-07   \n",
      "2            0.527068            0.448002  2.063235e-06    7.786718e-07   \n",
      "3            0.527068            0.448002  5.629686e-06    7.786718e-07   \n",
      "0           -0.025697            0.000000  5.744530e-05    6.575629e-06   \n",
      "1            0.293167            0.282289  2.153113e-06    1.189441e-06   \n",
      "2            0.298829            0.290443  5.081294e-06    4.052337e-07   \n",
      "3            0.298829            0.290443  3.552357e-06    4.052337e-07   \n",
      "4           -0.025697            0.000000  6.122516e-05    2.051724e-05   \n",
      "5            0.203580            0.199404  1.353791e-04    1.253002e-05   \n",
      "..                ...                 ...           ...             ...   \n",
      "10           0.239127            0.252168  3.957573e-05    3.899843e-06   \n",
      "11           0.239127            0.252168  5.299118e-06    1.730247e-06   \n",
      "12          -0.025697            0.000000  3.059447e-06    7.216731e-06   \n",
      "13          -0.025697            0.000000  3.001108e-05    2.836636e-06   \n",
      "14           0.201635            0.228934  2.915690e-06    4.495664e-07   \n",
      "15           0.201635            0.228934  4.001898e-05    8.729724e-06   \n",
      "16          -0.025697            0.000000  4.285570e-04    4.465158e-05   \n",
      "17          -0.025697            0.000000  5.250492e-05    2.681971e-05   \n",
      "18           0.157421            0.199062  2.336340e-05    1.276523e-06   \n",
      "19           0.157421            0.199062  4.960533e-06    1.189441e-06   \n",
      "20          -0.025697            0.000000  2.928658e-06    4.495664e-07   \n",
      "21          -0.025697            0.000000  3.564728e-05    1.123916e-07   \n",
      "22           0.106485            0.162552  2.558409e-05    1.215701e-06   \n",
      "23           0.106485            0.162552  3.217758e-05    9.156294e-06   \n",
      "24          -0.025697            0.000000  1.323917e-04    3.366368e-05   \n",
      "25          -0.025697            0.000000  3.922901e-05    1.801389e-05   \n",
      "26           0.048828            0.119403  3.875425e-05    3.789781e-06   \n",
      "27           0.048828            0.119403  7.469603e-06    1.276523e-06   \n",
      "28          -0.025697            0.000000  1.342550e-05    4.495664e-07   \n",
      "29          -0.025697            0.000000  3.513150e-05    1.414126e-05   \n",
      "30          -0.015551            0.069616  4.574338e-05    1.314982e-05   \n",
      "31          -0.015551            0.069616  4.512491e-06    5.150430e-07   \n",
      "32          -0.025697            0.000000  3.869782e-05    2.051724e-05   \n",
      "33          -0.025697            0.000000  7.867412e-07    8.778064e-07   \n",
      "34          -0.086652            0.013191  5.453309e-05    2.810060e-05   \n",
      "35          -0.086652            0.013191  4.495664e-07    1.296163e-06   \n",
      "36          -0.025697            0.000000  1.910657e-06    8.991328e-07   \n",
      "37          -0.025697            0.000000  1.123916e-07    2.464940e-06   \n",
      "38          -0.164475           -0.049873  1.461091e-06    3.371748e-07   \n",
      "39          -0.164475           -0.049873  1.189441e-06    4.495664e-07   \n",
      "\n",
      "    std_test_score  std_train_score  \n",
      "0         0.066246         0.008729  \n",
      "1         0.069603         0.009930  \n",
      "2         0.063013         0.011223  \n",
      "3         0.076261         0.008637  \n",
      "4         0.077556         0.010577  \n",
      "5         0.058292         0.008858  \n",
      "6         0.065445         0.007848  \n",
      "7         0.091917         0.005764  \n",
      "0         0.113255         0.000104  \n",
      "1         0.098767         0.000104  \n",
      "2         0.110435         0.000104  \n",
      "3         0.118016         0.000104  \n",
      "4         0.111307         0.000104  \n",
      "5         0.123283         0.000104  \n",
      "6         0.128135         0.000104  \n",
      "7         0.124209         0.000104  \n",
      "0         0.099298         0.051562  \n",
      "1         0.099298         0.051562  \n",
      "2         0.092380         0.047554  \n",
      "3         0.092380         0.047554  \n",
      "0         0.039770         0.039394  \n",
      "1         0.100478         0.051662  \n",
      "2         0.093522         0.047660  \n",
      "3         0.093522         0.047660  \n",
      "0         0.052024         0.000000  \n",
      "1         0.024263         0.027481  \n",
      "2         0.039295         0.023991  \n",
      "3         0.039295         0.023991  \n",
      "4         0.052024         0.000000  \n",
      "5         0.032248         0.026496  \n",
      "..             ...              ...  \n",
      "10        0.056143         0.022082  \n",
      "11        0.056143         0.022082  \n",
      "12        0.052024         0.000000  \n",
      "13        0.052414         0.001011  \n",
      "14        0.070245         0.021161  \n",
      "15        0.070245         0.021161  \n",
      "16        0.052024         0.000000  \n",
      "17        0.052024         0.000000  \n",
      "18        0.086077         0.020125  \n",
      "19        0.086077         0.020125  \n",
      "20        0.052024         0.000000  \n",
      "21        0.052024         0.000000  \n",
      "22        0.103197         0.019121  \n",
      "23        0.103197         0.019121  \n",
      "24        0.052024         0.000000  \n",
      "25        0.052024         0.000000  \n",
      "26        0.121377         0.018369  \n",
      "27        0.121377         0.018369  \n",
      "28        0.052024         0.000000  \n",
      "29        0.052024         0.000000  \n",
      "30        0.140492         0.018152  \n",
      "31        0.140492         0.018152  \n",
      "32        0.052024         0.000000  \n",
      "33        0.052024         0.000000  \n",
      "34        0.160468         0.018774  \n",
      "35        0.160468         0.018774  \n",
      "36        0.052024         0.000000  \n",
      "37        0.052024         0.000000  \n",
      "38        0.181263         0.020452  \n",
      "39        0.181263         0.020452  \n",
      "\n",
      "[64 rows x 22 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "14       0.067413         0.000199         0.404032          0.479045   \n",
      "15       0.065136         0.000200         0.417573          0.485216   \n",
      "16       0.079357         0.000196         0.404097          0.479091   \n",
      "17       0.074196         0.000198         0.417553          0.485195   \n",
      "18       0.085602         0.000208         0.404102          0.479098   \n",
      "19       0.080829         0.000201         0.417567          0.485207   \n",
      "20       0.090575         0.000200         0.404025          0.479041   \n",
      "21       0.088209         0.000198         0.417646          0.485230   \n",
      "22       0.011202         0.000200         0.437682          0.486718   \n",
      "23       0.010782         0.000193         0.437682          0.486718   \n",
      "24       0.020818         0.000190         0.437084          0.486610   \n",
      "25       0.019781         0.000220         0.437084          0.486610   \n",
      "26       0.029745         0.000212         0.436829          0.486466   \n",
      "27       0.027880         0.000198         0.436829          0.486466   \n",
      "28       0.037435         0.000196         0.436990          0.486543   \n",
      "29       0.038597         0.000200         0.436990          0.486543   \n",
      "30       0.047992         0.000224         0.436878          0.486483   \n",
      "31       0.048102         0.000225         0.436878          0.486483   \n",
      "32       0.057080         0.000217         0.436884          0.486471   \n",
      "33       0.055011         0.000229         0.436884          0.486471   \n",
      "34       0.065086         0.000249         0.436958          0.486510   \n",
      "35       0.065585         0.000212         0.436958          0.486510   \n",
      "36       0.073776         0.000218         0.436897          0.486479   \n",
      "37       0.072618         0.000221         0.436897          0.486479   \n",
      "38       0.081407         0.000215         0.436956          0.486527   \n",
      "39       0.086412         0.000204         0.436956          0.486527   \n",
      "40       0.092016         0.000286         0.436951          0.486516   \n",
      "41       0.088068         0.000211         0.436951          0.486516   \n",
      "42       0.099695         0.000237         0.436909          0.486494   \n",
      "43       0.096826         0.000208         0.436909          0.486494   \n",
      "\n",
      "   method_ids param_alpha param_criterion param_fit_intercept param_n_alphas  \\\n",
      "0   [0, 0, 1]         NaN             mse                 NaN            NaN   \n",
      "1   [0, 0, 1]         NaN             mse                 NaN            NaN   \n",
      "2   [0, 0, 1]         NaN             mse                 NaN            NaN   \n",
      "3   [0, 0, 1]         NaN             mse                 NaN            NaN   \n",
      "4   [0, 0, 1]         NaN             mae                 NaN            NaN   \n",
      "5   [0, 0, 1]         NaN             mae                 NaN            NaN   \n",
      "6   [0, 0, 1]         NaN             mae                 NaN            NaN   \n",
      "7   [0, 0, 1]         NaN             mae                 NaN            NaN   \n",
      "0   [0, 0, 2]         NaN             mse                 NaN            NaN   \n",
      "1   [0, 0, 2]         NaN             mse                 NaN            NaN   \n",
      "2   [0, 0, 2]         NaN             mse                 NaN            NaN   \n",
      "3   [0, 0, 2]         NaN             mse                 NaN            NaN   \n",
      "4   [0, 0, 2]         NaN             mae                 NaN            NaN   \n",
      "5   [0, 0, 2]         NaN             mae                 NaN            NaN   \n",
      "6   [0, 0, 2]         NaN             mae                 NaN            NaN   \n",
      "7   [0, 0, 2]         NaN             mae                 NaN            NaN   \n",
      "0   [0, 0, 3]         NaN             NaN                True            NaN   \n",
      "1   [0, 0, 3]         NaN             NaN                True            NaN   \n",
      "2   [0, 0, 3]         NaN             NaN               False            NaN   \n",
      "3   [0, 0, 3]         NaN             NaN               False            NaN   \n",
      "0   [0, 0, 4]         NaN             NaN                True            NaN   \n",
      "1   [0, 0, 4]         NaN             NaN                True            NaN   \n",
      "2   [0, 0, 4]         NaN             NaN               False            NaN   \n",
      "3   [0, 0, 4]         NaN             NaN               False            NaN   \n",
      "0   [0, 0, 6]           1             NaN                True            NaN   \n",
      "1   [0, 0, 6]           1             NaN                True            NaN   \n",
      "2   [0, 0, 6]           1             NaN               False            NaN   \n",
      "3   [0, 0, 6]           1             NaN               False            NaN   \n",
      "4   [0, 0, 6]           2             NaN                True            NaN   \n",
      "5   [0, 0, 6]           2             NaN                True            NaN   \n",
      "..        ...         ...             ...                 ...            ...   \n",
      "14  [0, 0, 7]         NaN             NaN                True            360   \n",
      "15  [0, 0, 7]         NaN             NaN                True            360   \n",
      "16  [0, 0, 7]         NaN             NaN                True            410   \n",
      "17  [0, 0, 7]         NaN             NaN                True            410   \n",
      "18  [0, 0, 7]         NaN             NaN                True            460   \n",
      "19  [0, 0, 7]         NaN             NaN                True            460   \n",
      "20  [0, 0, 7]         NaN             NaN                True            510   \n",
      "21  [0, 0, 7]         NaN             NaN                True            510   \n",
      "22  [0, 0, 7]         NaN             NaN               False             10   \n",
      "23  [0, 0, 7]         NaN             NaN               False             10   \n",
      "24  [0, 0, 7]         NaN             NaN               False             60   \n",
      "25  [0, 0, 7]         NaN             NaN               False             60   \n",
      "26  [0, 0, 7]         NaN             NaN               False            110   \n",
      "27  [0, 0, 7]         NaN             NaN               False            110   \n",
      "28  [0, 0, 7]         NaN             NaN               False            160   \n",
      "29  [0, 0, 7]         NaN             NaN               False            160   \n",
      "30  [0, 0, 7]         NaN             NaN               False            210   \n",
      "31  [0, 0, 7]         NaN             NaN               False            210   \n",
      "32  [0, 0, 7]         NaN             NaN               False            260   \n",
      "33  [0, 0, 7]         NaN             NaN               False            260   \n",
      "34  [0, 0, 7]         NaN             NaN               False            310   \n",
      "35  [0, 0, 7]         NaN             NaN               False            310   \n",
      "36  [0, 0, 7]         NaN             NaN               False            360   \n",
      "37  [0, 0, 7]         NaN             NaN               False            360   \n",
      "38  [0, 0, 7]         NaN             NaN               False            410   \n",
      "39  [0, 0, 7]         NaN             NaN               False            410   \n",
      "40  [0, 0, 7]         NaN             NaN               False            460   \n",
      "41  [0, 0, 7]         NaN             NaN               False            460   \n",
      "42  [0, 0, 7]         NaN             NaN               False            510   \n",
      "43  [0, 0, 7]         NaN             NaN               False            510   \n",
      "\n",
      "   param_n_estimators       ...        split0_test_score split0_train_score  \\\n",
      "0                  10       ...                 0.405206           0.900516   \n",
      "1                  25       ...                 0.494276           0.923324   \n",
      "2                  40       ...                 0.467146           0.929035   \n",
      "3                  55       ...                 0.442689           0.925783   \n",
      "4                  10       ...                 0.439291           0.894901   \n",
      "5                  25       ...                 0.443830           0.926134   \n",
      "6                  40       ...                 0.483314           0.930272   \n",
      "7                  55       ...                 0.496800           0.929725   \n",
      "0                  10       ...                 0.283188           0.999999   \n",
      "1                  25       ...                 0.320898           0.999999   \n",
      "2                  40       ...                 0.309973           0.999999   \n",
      "3                  55       ...                 0.320773           0.999999   \n",
      "4                  10       ...                 0.334430           0.999999   \n",
      "5                  25       ...                 0.328006           0.999999   \n",
      "6                  40       ...                 0.319685           0.999999   \n",
      "7                  55       ...                 0.311854           0.999999   \n",
      "0                 NaN       ...                 0.477591           0.463774   \n",
      "1                 NaN       ...                 0.477591           0.463774   \n",
      "2                 NaN       ...                 0.474478           0.463719   \n",
      "3                 NaN       ...                 0.474478           0.463719   \n",
      "0                 NaN       ...                 0.346831           0.382781   \n",
      "1                 NaN       ...                 0.481139           0.463593   \n",
      "2                 NaN       ...                 0.478163           0.463543   \n",
      "3                 NaN       ...                 0.478163           0.463543   \n",
      "0                 NaN       ...                -0.139219           0.000000   \n",
      "1                 NaN       ...                 0.256755           0.292910   \n",
      "2                 NaN       ...                 0.327736           0.304397   \n",
      "3                 NaN       ...                 0.327736           0.304397   \n",
      "4                 NaN       ...                -0.139219           0.000000   \n",
      "5                 NaN       ...                 0.126198           0.204443   \n",
      "..                ...       ...                      ...                ...   \n",
      "14                NaN       ...                 0.472158           0.460908   \n",
      "15                NaN       ...                 0.479746           0.460341   \n",
      "16                NaN       ...                 0.472188           0.460929   \n",
      "17                NaN       ...                 0.479657           0.460273   \n",
      "18                NaN       ...                 0.472089           0.460860   \n",
      "19                NaN       ...                 0.479696           0.460303   \n",
      "20                NaN       ...                 0.472121           0.460882   \n",
      "21                NaN       ...                 0.479629           0.460251   \n",
      "22                NaN       ...                 0.472070           0.459147   \n",
      "23                NaN       ...                 0.472070           0.459147   \n",
      "24                NaN       ...                 0.472649           0.459541   \n",
      "25                NaN       ...                 0.472649           0.459541   \n",
      "26                NaN       ...                 0.472136           0.459192   \n",
      "27                NaN       ...                 0.472136           0.459192   \n",
      "28                NaN       ...                 0.472336           0.459329   \n",
      "29                NaN       ...                 0.472336           0.459329   \n",
      "30                NaN       ...                 0.472139           0.459194   \n",
      "31                NaN       ...                 0.472139           0.459194   \n",
      "32                NaN       ...                 0.472014           0.459109   \n",
      "33                NaN       ...                 0.472014           0.459109   \n",
      "34                NaN       ...                 0.472140           0.459195   \n",
      "35                NaN       ...                 0.472140           0.459195   \n",
      "36                NaN       ...                 0.472050           0.459133   \n",
      "37                NaN       ...                 0.472050           0.459133   \n",
      "38                NaN       ...                 0.472295           0.459301   \n",
      "39                NaN       ...                 0.472295           0.459301   \n",
      "40                NaN       ...                 0.472210           0.459243   \n",
      "41                NaN       ...                 0.472210           0.459243   \n",
      "42                NaN       ...                 0.472141           0.459196   \n",
      "43                NaN       ...                 0.472141           0.459196   \n",
      "\n",
      "    split1_test_score  split1_train_score  split2_test_score  \\\n",
      "0            0.437757            0.918215           0.559186   \n",
      "1            0.405933            0.925264           0.576696   \n",
      "2            0.452861            0.933157           0.593257   \n",
      "3            0.432066            0.937133           0.599064   \n",
      "4            0.397995            0.919887           0.579476   \n",
      "5            0.450539            0.935124           0.570797   \n",
      "6            0.425251            0.936006           0.583939   \n",
      "7            0.390728            0.934790           0.616149   \n",
      "0            0.395423            0.999976           0.558876   \n",
      "1            0.389036            0.999976           0.555985   \n",
      "2            0.397995            0.999976           0.575444   \n",
      "3            0.421707            0.999976           0.605744   \n",
      "4            0.358684            0.999976           0.581883   \n",
      "5            0.448363            0.999976           0.627892   \n",
      "6            0.411515            0.999976           0.625488   \n",
      "7            0.392534            0.999976           0.606243   \n",
      "0            0.293992            0.567092           0.524410   \n",
      "1            0.293992            0.567092           0.524410   \n",
      "2            0.307132            0.556049           0.523140   \n",
      "3            0.307132            0.556049           0.523140   \n",
      "0            0.316059            0.465484           0.411636   \n",
      "1            0.294810            0.567061           0.527691   \n",
      "2            0.308503            0.555975           0.527068   \n",
      "3            0.308503            0.555975           0.527068   \n",
      "0           -0.032746            0.000000          -0.025697   \n",
      "1            0.234169            0.345164           0.293167   \n",
      "2            0.233787            0.346858           0.298829   \n",
      "3            0.233787            0.346858           0.298829   \n",
      "4           -0.032746            0.000000          -0.025697   \n",
      "5            0.178140            0.257960           0.203580   \n",
      "..                ...                 ...                ...   \n",
      "14           0.295072            0.567073           0.444132   \n",
      "15           0.295947            0.566893           0.476357   \n",
      "16           0.295090            0.567072           0.444281   \n",
      "17           0.295961            0.566892           0.476373   \n",
      "18           0.295088            0.567072           0.444397   \n",
      "19           0.295951            0.566893           0.476386   \n",
      "20           0.295072            0.567073           0.444150   \n",
      "21           0.295944            0.566893           0.476700   \n",
      "22           0.315525            0.554793           0.525081   \n",
      "23           0.315525            0.554793           0.525081   \n",
      "24           0.315525            0.554793           0.522696   \n",
      "25           0.315525            0.554793           0.522696   \n",
      "26           0.315525            0.554793           0.522446   \n",
      "27           0.315525            0.554793           0.522446   \n",
      "28           0.315525            0.554793           0.522728   \n",
      "29           0.315525            0.554793           0.522728   \n",
      "30           0.315525            0.554793           0.522591   \n",
      "31           0.315525            0.554793           0.522591   \n",
      "32           0.315525            0.554793           0.522735   \n",
      "33           0.315525            0.554793           0.522735   \n",
      "34           0.315525            0.554793           0.522830   \n",
      "35           0.315525            0.554793           0.522830   \n",
      "36           0.315525            0.554793           0.522738   \n",
      "37           0.315525            0.554793           0.522738   \n",
      "38           0.315525            0.554793           0.522667   \n",
      "39           0.315525            0.554793           0.522667   \n",
      "40           0.315525            0.554793           0.522740   \n",
      "41           0.315525            0.554793           0.522740   \n",
      "42           0.315525            0.554793           0.522683   \n",
      "43           0.315525            0.554793           0.522683   \n",
      "\n",
      "    split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
      "0             0.919755      0.000108    1.590706e-04        0.066246   \n",
      "1             0.903296      0.003123    1.511005e-03        0.069603   \n",
      "2             0.907558      0.003846    7.039070e-04        0.063013   \n",
      "3             0.915997      0.000581    2.888158e-04        0.076261   \n",
      "4             0.913330      0.001840    3.442162e-04        0.077556   \n",
      "5             0.913528      0.001926    1.717574e-04        0.058292   \n",
      "6             0.917249      0.000627    4.681239e-04        0.065445   \n",
      "7             0.920845      0.008962    3.297199e-04        0.091917   \n",
      "0             0.999767      0.000651    1.492319e-04        0.113255   \n",
      "1             0.999767      0.001725    1.890016e-03        0.098767   \n",
      "2             0.999767      0.003258    7.548232e-04        0.110435   \n",
      "3             0.999767      0.007825    2.299589e-03        0.118016   \n",
      "4             0.999767      0.002929    9.104882e-04        0.111307   \n",
      "5             0.999767      0.002699    8.946138e-05        0.123283   \n",
      "6             0.999767      0.008569    1.126224e-04        0.128135   \n",
      "7             0.999767      0.002479    1.238335e-03        0.124209   \n",
      "0             0.452518      0.001449    7.148106e-05        0.099298   \n",
      "1             0.452518      0.000012    5.857317e-06        0.099298   \n",
      "2             0.448383      0.000006    0.000000e+00        0.092380   \n",
      "3             0.448383      0.000009    9.602742e-07        0.092380   \n",
      "0             0.381077      0.000077    7.636841e-06        0.039770   \n",
      "1             0.452229      0.000004    5.150430e-07        0.100478   \n",
      "2             0.448002      0.000002    7.786718e-07        0.093522   \n",
      "3             0.448002      0.000006    7.786718e-07        0.093522   \n",
      "0             0.000000      0.000057    6.575629e-06        0.052024   \n",
      "1             0.282289      0.000002    1.189441e-06        0.024263   \n",
      "2             0.290443      0.000005    4.052337e-07        0.039295   \n",
      "3             0.290443      0.000004    4.052337e-07        0.039295   \n",
      "4             0.000000      0.000061    2.051724e-05        0.052024   \n",
      "5             0.199404      0.000135    1.253002e-05        0.032248   \n",
      "..                 ...           ...             ...             ...   \n",
      "14            0.409155      0.001438    2.922181e-06        0.077688   \n",
      "15            0.428413      0.001593    9.692431e-06        0.085783   \n",
      "16            0.409272      0.006253    2.725351e-06        0.077714   \n",
      "17            0.428420      0.001572    1.408263e-06        0.085759   \n",
      "18            0.409363      0.003500    4.979595e-06        0.077706   \n",
      "19            0.428425      0.001445    4.970708e-06        0.085776   \n",
      "20            0.409169      0.001288    3.339752e-06        0.077680   \n",
      "21            0.428546      0.001531    1.561394e-06        0.085835   \n",
      "22            0.446212      0.000352    1.120708e-05        0.088828   \n",
      "23            0.446212      0.000214    5.676610e-06        0.088828   \n",
      "24            0.445497      0.001883    2.614162e-06        0.088131   \n",
      "25            0.445497      0.000410    4.171364e-05        0.088131   \n",
      "26            0.445412      0.001040    3.018986e-05        0.087981   \n",
      "27            0.445412      0.000469    7.979803e-06        0.087981   \n",
      "28            0.445508      0.001665    1.655632e-06        0.088099   \n",
      "29            0.445508      0.001122    7.786718e-07        0.088099   \n",
      "30            0.445462      0.002582    2.633587e-05        0.088028   \n",
      "31            0.445462      0.002714    1.364112e-05        0.088028   \n",
      "32            0.445510      0.001988    3.899843e-06        0.088058   \n",
      "33            0.445510      0.001012    3.203160e-05        0.088058   \n",
      "34            0.445541      0.002501    2.884854e-05        0.088105   \n",
      "35            0.445541      0.001061    6.511927e-06        0.088105   \n",
      "36            0.445511      0.000978    3.617543e-06        0.088063   \n",
      "37            0.445511      0.000673    9.055722e-06        0.088063   \n",
      "38            0.445487      0.000073    2.570305e-06        0.088074   \n",
      "39            0.445487      0.015617    5.821624e-06        0.088074   \n",
      "40            0.445512      0.002962    5.397465e-05        0.088085   \n",
      "41            0.445512      0.002336    2.836636e-06        0.088085   \n",
      "42            0.445493      0.003431    1.449111e-05        0.088058   \n",
      "43            0.445493      0.001979    2.170642e-06        0.088058   \n",
      "\n",
      "    std_train_score  \n",
      "0          0.008729  \n",
      "1          0.009930  \n",
      "2          0.011223  \n",
      "3          0.008637  \n",
      "4          0.010577  \n",
      "5          0.008858  \n",
      "6          0.007848  \n",
      "7          0.005764  \n",
      "0          0.000104  \n",
      "1          0.000104  \n",
      "2          0.000104  \n",
      "3          0.000104  \n",
      "4          0.000104  \n",
      "5          0.000104  \n",
      "6          0.000104  \n",
      "7          0.000104  \n",
      "0          0.051562  \n",
      "1          0.051562  \n",
      "2          0.047554  \n",
      "3          0.047554  \n",
      "0          0.039394  \n",
      "1          0.051662  \n",
      "2          0.047660  \n",
      "3          0.047660  \n",
      "0          0.000000  \n",
      "1          0.027481  \n",
      "2          0.023991  \n",
      "3          0.023991  \n",
      "4          0.000000  \n",
      "5          0.026496  \n",
      "..              ...  \n",
      "14         0.065733  \n",
      "15         0.059207  \n",
      "16         0.065689  \n",
      "17         0.059214  \n",
      "18         0.065663  \n",
      "19         0.059209  \n",
      "20         0.065730  \n",
      "21         0.059177  \n",
      "22         0.048426  \n",
      "23         0.048426  \n",
      "24         0.048552  \n",
      "25         0.048552  \n",
      "26         0.048641  \n",
      "27         0.048641  \n",
      "28         0.048589  \n",
      "29         0.048589  \n",
      "30         0.048627  \n",
      "31         0.048627  \n",
      "32         0.048629  \n",
      "33         0.048629  \n",
      "34         0.048604  \n",
      "35         0.048604  \n",
      "36         0.048624  \n",
      "37         0.048624  \n",
      "38         0.048600  \n",
      "39         0.048600  \n",
      "40         0.048604  \n",
      "41         0.048604  \n",
      "42         0.048618  \n",
      "43         0.048618  \n",
      "\n",
      "[108 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=9.523e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.761e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.849e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.042e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.400e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.127e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.634e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.029e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.556e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.278e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.248e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.646e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.323e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.169e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.192e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.096e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.920e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.602e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.792e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.845e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.762e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.601e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.523e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.035e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.868e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.342e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.412e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.003e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.227e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.209e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.757e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.887e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.943e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.173e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.586e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.127e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.833e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.359e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.794e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.416e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.265e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.652e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.282e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.068e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.034e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.243e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.863e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.607e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.178e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.830e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.216e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.608e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.420e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.042e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.210e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.193e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.092e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.577e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.789e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.349e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.860e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.930e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.749e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.460e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.230e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "34       0.006922         0.000195         0.436831          0.488855   \n",
      "35       0.007793         0.000249         0.411408          0.473630   \n",
      "36       0.007818         0.000216         0.436831          0.488855   \n",
      "37       0.007703         0.000206         0.411408          0.473630   \n",
      "38       0.006909         0.000202         0.436831          0.488855   \n",
      "39       0.007762         0.000197         0.411408          0.473630   \n",
      "40       0.007418         0.000234         0.436831          0.488855   \n",
      "41       0.007724         0.000201         0.411408          0.473630   \n",
      "42       0.008572         0.000218         0.436831          0.488855   \n",
      "43       0.007317         0.000191         0.411408          0.473630   \n",
      "44       0.007050         0.000194         0.436831          0.488855   \n",
      "45       0.007969         0.000225         0.411408          0.473630   \n",
      "46       0.007193         0.000201         0.436831          0.488855   \n",
      "47       0.007724         0.000197         0.411408          0.473630   \n",
      "48       0.006766         0.000193         0.436831          0.488855   \n",
      "49       0.007778         0.000259         0.411408          0.473630   \n",
      "50       0.007306         0.000202         0.436831          0.488855   \n",
      "51       0.007887         0.000205         0.411408          0.473630   \n",
      "52       0.006738         0.000192         0.436831          0.488855   \n",
      "53       0.007542         0.000195         0.411408          0.473630   \n",
      "54       0.006878         0.000212         0.436831          0.488855   \n",
      "55       0.008369         0.000200         0.411408          0.473630   \n",
      "56       0.007277         0.000200         0.436831          0.488855   \n",
      "57       0.007449         0.000197         0.411408          0.473630   \n",
      "58       0.007457         0.000261         0.436831          0.488855   \n",
      "59       0.009349         0.000217         0.411408          0.473630   \n",
      "60       0.007183         0.000222         0.436831          0.488855   \n",
      "61       0.008412         0.000202         0.411408          0.473630   \n",
      "62       0.006883         0.000217         0.436831          0.488855   \n",
      "63       0.008046         0.000206         0.411408          0.473630   \n",
      "\n",
      "   method_ids param_alpha param_criterion param_fit_intercept  \\\n",
      "0   [0, 0, 1]         NaN             mse                 NaN   \n",
      "1   [0, 0, 1]         NaN             mse                 NaN   \n",
      "2   [0, 0, 1]         NaN             mse                 NaN   \n",
      "3   [0, 0, 1]         NaN             mse                 NaN   \n",
      "4   [0, 0, 1]         NaN             mae                 NaN   \n",
      "5   [0, 0, 1]         NaN             mae                 NaN   \n",
      "6   [0, 0, 1]         NaN             mae                 NaN   \n",
      "7   [0, 0, 1]         NaN             mae                 NaN   \n",
      "0   [0, 0, 2]         NaN             mse                 NaN   \n",
      "1   [0, 0, 2]         NaN             mse                 NaN   \n",
      "2   [0, 0, 2]         NaN             mse                 NaN   \n",
      "3   [0, 0, 2]         NaN             mse                 NaN   \n",
      "4   [0, 0, 2]         NaN             mae                 NaN   \n",
      "5   [0, 0, 2]         NaN             mae                 NaN   \n",
      "6   [0, 0, 2]         NaN             mae                 NaN   \n",
      "7   [0, 0, 2]         NaN             mae                 NaN   \n",
      "0   [0, 0, 3]         NaN             NaN                True   \n",
      "1   [0, 0, 3]         NaN             NaN                True   \n",
      "2   [0, 0, 3]         NaN             NaN               False   \n",
      "3   [0, 0, 3]         NaN             NaN               False   \n",
      "0   [0, 0, 4]         NaN             NaN                True   \n",
      "1   [0, 0, 4]         NaN             NaN                True   \n",
      "2   [0, 0, 4]         NaN             NaN               False   \n",
      "3   [0, 0, 4]         NaN             NaN               False   \n",
      "0   [0, 0, 6]           1             NaN                True   \n",
      "1   [0, 0, 6]           1             NaN                True   \n",
      "2   [0, 0, 6]           1             NaN               False   \n",
      "3   [0, 0, 6]           1             NaN               False   \n",
      "4   [0, 0, 6]           2             NaN                True   \n",
      "5   [0, 0, 6]           2             NaN                True   \n",
      "..        ...         ...             ...                 ...   \n",
      "34  [0, 0, 8]         NaN             NaN               False   \n",
      "35  [0, 0, 8]         NaN             NaN               False   \n",
      "36  [0, 0, 8]         NaN             NaN               False   \n",
      "37  [0, 0, 8]         NaN             NaN               False   \n",
      "38  [0, 0, 8]         NaN             NaN               False   \n",
      "39  [0, 0, 8]         NaN             NaN               False   \n",
      "40  [0, 0, 8]         NaN             NaN               False   \n",
      "41  [0, 0, 8]         NaN             NaN               False   \n",
      "42  [0, 0, 8]         NaN             NaN               False   \n",
      "43  [0, 0, 8]         NaN             NaN               False   \n",
      "44  [0, 0, 8]         NaN             NaN               False   \n",
      "45  [0, 0, 8]         NaN             NaN               False   \n",
      "46  [0, 0, 8]         NaN             NaN               False   \n",
      "47  [0, 0, 8]         NaN             NaN               False   \n",
      "48  [0, 0, 8]         NaN             NaN               False   \n",
      "49  [0, 0, 8]         NaN             NaN               False   \n",
      "50  [0, 0, 8]         NaN             NaN               False   \n",
      "51  [0, 0, 8]         NaN             NaN               False   \n",
      "52  [0, 0, 8]         NaN             NaN               False   \n",
      "53  [0, 0, 8]         NaN             NaN               False   \n",
      "54  [0, 0, 8]         NaN             NaN               False   \n",
      "55  [0, 0, 8]         NaN             NaN               False   \n",
      "56  [0, 0, 8]         NaN             NaN               False   \n",
      "57  [0, 0, 8]         NaN             NaN               False   \n",
      "58  [0, 0, 8]         NaN             NaN               False   \n",
      "59  [0, 0, 8]         NaN             NaN               False   \n",
      "60  [0, 0, 8]         NaN             NaN               False   \n",
      "61  [0, 0, 8]         NaN             NaN               False   \n",
      "62  [0, 0, 8]         NaN             NaN               False   \n",
      "63  [0, 0, 8]         NaN             NaN               False   \n",
      "\n",
      "   param_max_n_alphas param_n_alphas       ...        split0_test_score  \\\n",
      "0                 NaN            NaN       ...                 0.405206   \n",
      "1                 NaN            NaN       ...                 0.494276   \n",
      "2                 NaN            NaN       ...                 0.467146   \n",
      "3                 NaN            NaN       ...                 0.442689   \n",
      "4                 NaN            NaN       ...                 0.439291   \n",
      "5                 NaN            NaN       ...                 0.443830   \n",
      "6                 NaN            NaN       ...                 0.483314   \n",
      "7                 NaN            NaN       ...                 0.496800   \n",
      "0                 NaN            NaN       ...                 0.283188   \n",
      "1                 NaN            NaN       ...                 0.320898   \n",
      "2                 NaN            NaN       ...                 0.309973   \n",
      "3                 NaN            NaN       ...                 0.320773   \n",
      "4                 NaN            NaN       ...                 0.334430   \n",
      "5                 NaN            NaN       ...                 0.328006   \n",
      "6                 NaN            NaN       ...                 0.319685   \n",
      "7                 NaN            NaN       ...                 0.311854   \n",
      "0                 NaN            NaN       ...                 0.477591   \n",
      "1                 NaN            NaN       ...                 0.477591   \n",
      "2                 NaN            NaN       ...                 0.474478   \n",
      "3                 NaN            NaN       ...                 0.474478   \n",
      "0                 NaN            NaN       ...                 0.346831   \n",
      "1                 NaN            NaN       ...                 0.481139   \n",
      "2                 NaN            NaN       ...                 0.478163   \n",
      "3                 NaN            NaN       ...                 0.478163   \n",
      "0                 NaN            NaN       ...                -0.139219   \n",
      "1                 NaN            NaN       ...                 0.256755   \n",
      "2                 NaN            NaN       ...                 0.327736   \n",
      "3                 NaN            NaN       ...                 0.327736   \n",
      "4                 NaN            NaN       ...                -0.139219   \n",
      "5                 NaN            NaN       ...                 0.126198   \n",
      "..                ...            ...       ...                      ...   \n",
      "34                600            NaN       ...                 0.474868   \n",
      "35                600            NaN       ...                 0.480679   \n",
      "36                700            NaN       ...                 0.474868   \n",
      "37                700            NaN       ...                 0.480679   \n",
      "38                800            NaN       ...                 0.474868   \n",
      "39                800            NaN       ...                 0.480679   \n",
      "40                900            NaN       ...                 0.474868   \n",
      "41                900            NaN       ...                 0.480679   \n",
      "42               1000            NaN       ...                 0.474868   \n",
      "43               1000            NaN       ...                 0.480679   \n",
      "44               1100            NaN       ...                 0.474868   \n",
      "45               1100            NaN       ...                 0.480679   \n",
      "46               1200            NaN       ...                 0.474868   \n",
      "47               1200            NaN       ...                 0.480679   \n",
      "48               1300            NaN       ...                 0.474868   \n",
      "49               1300            NaN       ...                 0.480679   \n",
      "50               1400            NaN       ...                 0.474868   \n",
      "51               1400            NaN       ...                 0.480679   \n",
      "52               1500            NaN       ...                 0.474868   \n",
      "53               1500            NaN       ...                 0.480679   \n",
      "54               1600            NaN       ...                 0.474868   \n",
      "55               1600            NaN       ...                 0.480679   \n",
      "56               1700            NaN       ...                 0.474868   \n",
      "57               1700            NaN       ...                 0.480679   \n",
      "58               1800            NaN       ...                 0.474868   \n",
      "59               1800            NaN       ...                 0.480679   \n",
      "60               1900            NaN       ...                 0.474868   \n",
      "61               1900            NaN       ...                 0.480679   \n",
      "62               2000            NaN       ...                 0.474868   \n",
      "63               2000            NaN       ...                 0.480679   \n",
      "\n",
      "   split0_train_score split1_test_score  split1_train_score  \\\n",
      "0            0.900516          0.437757            0.918215   \n",
      "1            0.923324          0.405933            0.925264   \n",
      "2            0.929035          0.452861            0.933157   \n",
      "3            0.925783          0.432066            0.937133   \n",
      "4            0.894901          0.397995            0.919887   \n",
      "5            0.926134          0.450539            0.935124   \n",
      "6            0.930272          0.425251            0.936006   \n",
      "7            0.929725          0.390728            0.934790   \n",
      "0            0.999999          0.395423            0.999976   \n",
      "1            0.999999          0.389036            0.999976   \n",
      "2            0.999999          0.397995            0.999976   \n",
      "3            0.999999          0.421707            0.999976   \n",
      "4            0.999999          0.358684            0.999976   \n",
      "5            0.999999          0.448363            0.999976   \n",
      "6            0.999999          0.411515            0.999976   \n",
      "7            0.999999          0.392534            0.999976   \n",
      "0            0.463774          0.293992            0.567092   \n",
      "1            0.463774          0.293992            0.567092   \n",
      "2            0.463719          0.307132            0.556049   \n",
      "3            0.463719          0.307132            0.556049   \n",
      "0            0.382781          0.316059            0.465484   \n",
      "1            0.463593          0.294810            0.567061   \n",
      "2            0.463543          0.308503            0.555975   \n",
      "3            0.463543          0.308503            0.555975   \n",
      "0            0.000000         -0.032746            0.000000   \n",
      "1            0.292910          0.234169            0.345164   \n",
      "2            0.304397          0.233787            0.346858   \n",
      "3            0.304397          0.233787            0.346858   \n",
      "4            0.000000         -0.032746            0.000000   \n",
      "5            0.204443          0.178140            0.257960   \n",
      "..                ...               ...                 ...   \n",
      "34           0.463718          0.307132            0.556049   \n",
      "35           0.462979          0.311370            0.555448   \n",
      "36           0.463718          0.307132            0.556049   \n",
      "37           0.462979          0.311370            0.555448   \n",
      "38           0.463718          0.307132            0.556049   \n",
      "39           0.462979          0.311370            0.555448   \n",
      "40           0.463718          0.307132            0.556049   \n",
      "41           0.462979          0.311370            0.555448   \n",
      "42           0.463718          0.307132            0.556049   \n",
      "43           0.462979          0.311370            0.555448   \n",
      "44           0.463718          0.307132            0.556049   \n",
      "45           0.462979          0.311370            0.555448   \n",
      "46           0.463718          0.307132            0.556049   \n",
      "47           0.462979          0.311370            0.555448   \n",
      "48           0.463718          0.307132            0.556049   \n",
      "49           0.462979          0.311370            0.555448   \n",
      "50           0.463718          0.307132            0.556049   \n",
      "51           0.462979          0.311370            0.555448   \n",
      "52           0.463718          0.307132            0.556049   \n",
      "53           0.462979          0.311370            0.555448   \n",
      "54           0.463718          0.307132            0.556049   \n",
      "55           0.462979          0.311370            0.555448   \n",
      "56           0.463718          0.307132            0.556049   \n",
      "57           0.462979          0.311370            0.555448   \n",
      "58           0.463718          0.307132            0.556049   \n",
      "59           0.462979          0.311370            0.555448   \n",
      "60           0.463718          0.307132            0.556049   \n",
      "61           0.462979          0.311370            0.555448   \n",
      "62           0.463718          0.307132            0.556049   \n",
      "63           0.462979          0.311370            0.555448   \n",
      "\n",
      "    split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
      "0            0.559186            0.919755      0.000108    1.590706e-04   \n",
      "1            0.576696            0.903296      0.003123    1.511005e-03   \n",
      "2            0.593257            0.907558      0.003846    7.039070e-04   \n",
      "3            0.599064            0.915997      0.000581    2.888158e-04   \n",
      "4            0.579476            0.913330      0.001840    3.442162e-04   \n",
      "5            0.570797            0.913528      0.001926    1.717574e-04   \n",
      "6            0.583939            0.917249      0.000627    4.681239e-04   \n",
      "7            0.616149            0.920845      0.008962    3.297199e-04   \n",
      "0            0.558876            0.999767      0.000651    1.492319e-04   \n",
      "1            0.555985            0.999767      0.001725    1.890016e-03   \n",
      "2            0.575444            0.999767      0.003258    7.548232e-04   \n",
      "3            0.605744            0.999767      0.007825    2.299589e-03   \n",
      "4            0.581883            0.999767      0.002929    9.104882e-04   \n",
      "5            0.627892            0.999767      0.002699    8.946138e-05   \n",
      "6            0.625488            0.999767      0.008569    1.126224e-04   \n",
      "7            0.606243            0.999767      0.002479    1.238335e-03   \n",
      "0            0.524410            0.452518      0.001449    7.148106e-05   \n",
      "1            0.524410            0.452518      0.000012    5.857317e-06   \n",
      "2            0.523140            0.448383      0.000006    0.000000e+00   \n",
      "3            0.523140            0.448383      0.000009    9.602742e-07   \n",
      "0            0.411636            0.381077      0.000077    7.636841e-06   \n",
      "1            0.527691            0.452229      0.000004    5.150430e-07   \n",
      "2            0.527068            0.448002      0.000002    7.786718e-07   \n",
      "3            0.527068            0.448002      0.000006    7.786718e-07   \n",
      "0           -0.025697            0.000000      0.000057    6.575629e-06   \n",
      "1            0.293167            0.282289      0.000002    1.189441e-06   \n",
      "2            0.298829            0.290443      0.000005    4.052337e-07   \n",
      "3            0.298829            0.290443      0.000004    4.052337e-07   \n",
      "4           -0.025697            0.000000      0.000061    2.051724e-05   \n",
      "5            0.203580            0.199404      0.000135    1.253002e-05   \n",
      "..                ...                 ...           ...             ...   \n",
      "34           0.528083            0.446799      0.000642    6.633009e-06   \n",
      "35           0.441429            0.402462      0.000570    5.567855e-05   \n",
      "36           0.528083            0.446799      0.001241    2.741364e-05   \n",
      "37           0.441429            0.402462      0.000488    8.654153e-06   \n",
      "38           0.528083            0.446799      0.000530    1.473959e-05   \n",
      "39           0.441429            0.402462      0.000477    4.466064e-06   \n",
      "40           0.528083            0.446799      0.000708    3.609310e-05   \n",
      "41           0.441429            0.402462      0.000187    7.655015e-06   \n",
      "42           0.528083            0.446799      0.000586    9.368364e-06   \n",
      "43           0.441429            0.402462      0.000347    8.991328e-07   \n",
      "44           0.528083            0.446799      0.000555    2.485354e-06   \n",
      "45           0.441429            0.402462      0.000273    2.701072e-05   \n",
      "46           0.528083            0.446799      0.000497    2.245020e-06   \n",
      "47           0.441429            0.402462      0.000288    6.949219e-06   \n",
      "48           0.528083            0.446799      0.000341    8.485379e-07   \n",
      "49           0.441429            0.402462      0.000796    7.980729e-05   \n",
      "50           0.528083            0.446799      0.000705    1.910657e-06   \n",
      "51           0.441429            0.402462      0.000590    1.743809e-05   \n",
      "52           0.528083            0.446799      0.000482    1.959614e-06   \n",
      "53           0.441429            0.402462      0.000690    4.230764e-06   \n",
      "54           0.528083            0.446799      0.000321    2.061306e-05   \n",
      "55           0.441429            0.402462      0.000959    6.187665e-06   \n",
      "56           0.528083            0.446799      0.000137    2.081521e-06   \n",
      "57           0.441429            0.402462      0.000549    9.340682e-06   \n",
      "58           0.528083            0.446799      0.000296    8.638901e-05   \n",
      "59           0.441429            0.402462      0.001272    2.200265e-05   \n",
      "60           0.528083            0.446799      0.000680    3.690075e-05   \n",
      "61           0.441429            0.402462      0.001129    9.934425e-06   \n",
      "62           0.528083            0.446799      0.000360    3.173167e-05   \n",
      "63           0.441429            0.402462      0.000697    6.138475e-06   \n",
      "\n",
      "    std_test_score  std_train_score  \n",
      "0         0.066246         0.008729  \n",
      "1         0.069603         0.009930  \n",
      "2         0.063013         0.011223  \n",
      "3         0.076261         0.008637  \n",
      "4         0.077556         0.010577  \n",
      "5         0.058292         0.008858  \n",
      "6         0.065445         0.007848  \n",
      "7         0.091917         0.005764  \n",
      "0         0.113255         0.000104  \n",
      "1         0.098767         0.000104  \n",
      "2         0.110435         0.000104  \n",
      "3         0.118016         0.000104  \n",
      "4         0.111307         0.000104  \n",
      "5         0.123283         0.000104  \n",
      "6         0.128135         0.000104  \n",
      "7         0.124209         0.000104  \n",
      "0         0.099298         0.051562  \n",
      "1         0.099298         0.051562  \n",
      "2         0.092380         0.047554  \n",
      "3         0.092380         0.047554  \n",
      "0         0.039770         0.039394  \n",
      "1         0.100478         0.051662  \n",
      "2         0.093522         0.047660  \n",
      "3         0.093522         0.047660  \n",
      "0         0.052024         0.000000  \n",
      "1         0.024263         0.027481  \n",
      "2         0.039295         0.023991  \n",
      "3         0.039295         0.023991  \n",
      "4         0.052024         0.000000  \n",
      "5         0.032248         0.026496  \n",
      "..             ...              ...  \n",
      "34        0.094014         0.048013  \n",
      "35        0.072348         0.062909  \n",
      "36        0.094014         0.048013  \n",
      "37        0.072348         0.062909  \n",
      "38        0.094014         0.048013  \n",
      "39        0.072348         0.062909  \n",
      "40        0.094014         0.048013  \n",
      "41        0.072348         0.062909  \n",
      "42        0.094014         0.048013  \n",
      "43        0.072348         0.062909  \n",
      "44        0.094014         0.048013  \n",
      "45        0.072348         0.062909  \n",
      "46        0.094014         0.048013  \n",
      "47        0.072348         0.062909  \n",
      "48        0.094014         0.048013  \n",
      "49        0.072348         0.062909  \n",
      "50        0.094014         0.048013  \n",
      "51        0.072348         0.062909  \n",
      "52        0.094014         0.048013  \n",
      "53        0.072348         0.062909  \n",
      "54        0.094014         0.048013  \n",
      "55        0.072348         0.062909  \n",
      "56        0.094014         0.048013  \n",
      "57        0.072348         0.062909  \n",
      "58        0.094014         0.048013  \n",
      "59        0.072348         0.062909  \n",
      "60        0.094014         0.048013  \n",
      "61        0.072348         0.062909  \n",
      "62        0.094014         0.048013  \n",
      "63        0.072348         0.062909  \n",
      "\n",
      "[172 rows x 24 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "50       0.007306         0.000202         0.436831          0.488855   \n",
      "51       0.007887         0.000205         0.411408          0.473630   \n",
      "52       0.006738         0.000192         0.436831          0.488855   \n",
      "53       0.007542         0.000195         0.411408          0.473630   \n",
      "54       0.006878         0.000212         0.436831          0.488855   \n",
      "55       0.008369         0.000200         0.411408          0.473630   \n",
      "56       0.007277         0.000200         0.436831          0.488855   \n",
      "57       0.007449         0.000197         0.411408          0.473630   \n",
      "58       0.007457         0.000261         0.436831          0.488855   \n",
      "59       0.009349         0.000217         0.411408          0.473630   \n",
      "60       0.007183         0.000222         0.436831          0.488855   \n",
      "61       0.008412         0.000202         0.411408          0.473630   \n",
      "62       0.006883         0.000217         0.436831          0.488855   \n",
      "63       0.008046         0.000206         0.411408          0.473630   \n",
      "0        0.000581         0.000185        -0.066149          0.000000   \n",
      "1        0.001340         0.000187         0.411447          0.482077   \n",
      "2        0.000407         0.000180        -0.066149          0.000000   \n",
      "3        0.001953         0.000189         0.439529          0.490608   \n",
      "4        0.000360         0.000181        -1.250235         -1.223002   \n",
      "5        0.001698         0.000184         0.437440          0.487391   \n",
      "6        0.000338         0.000180        -1.250235         -1.223002   \n",
      "7        0.001723         0.000184         0.437440          0.487391   \n",
      "8        0.000473         0.000180        -0.066149          0.000000   \n",
      "9        0.001282         0.000183         0.388607          0.449845   \n",
      "10       0.000492         0.000185        -0.066149          0.000000   \n",
      "11       0.002076         0.000217         0.411437          0.476997   \n",
      "12       0.000359         0.000203        -1.250235         -1.223002   \n",
      "13       0.001751         0.000188         0.407478          0.472108   \n",
      "14       0.000368         0.000186        -1.250235         -1.223002   \n",
      "15       0.001868         0.000233         0.407478          0.472108   \n",
      "\n",
      "   method_ids param_alpha param_criterion param_fit_intercept  \\\n",
      "0   [0, 0, 1]         NaN             mse                 NaN   \n",
      "1   [0, 0, 1]         NaN             mse                 NaN   \n",
      "2   [0, 0, 1]         NaN             mse                 NaN   \n",
      "3   [0, 0, 1]         NaN             mse                 NaN   \n",
      "4   [0, 0, 1]         NaN             mae                 NaN   \n",
      "5   [0, 0, 1]         NaN             mae                 NaN   \n",
      "6   [0, 0, 1]         NaN             mae                 NaN   \n",
      "7   [0, 0, 1]         NaN             mae                 NaN   \n",
      "0   [0, 0, 2]         NaN             mse                 NaN   \n",
      "1   [0, 0, 2]         NaN             mse                 NaN   \n",
      "2   [0, 0, 2]         NaN             mse                 NaN   \n",
      "3   [0, 0, 2]         NaN             mse                 NaN   \n",
      "4   [0, 0, 2]         NaN             mae                 NaN   \n",
      "5   [0, 0, 2]         NaN             mae                 NaN   \n",
      "6   [0, 0, 2]         NaN             mae                 NaN   \n",
      "7   [0, 0, 2]         NaN             mae                 NaN   \n",
      "0   [0, 0, 3]         NaN             NaN                True   \n",
      "1   [0, 0, 3]         NaN             NaN                True   \n",
      "2   [0, 0, 3]         NaN             NaN               False   \n",
      "3   [0, 0, 3]         NaN             NaN               False   \n",
      "0   [0, 0, 4]         NaN             NaN                True   \n",
      "1   [0, 0, 4]         NaN             NaN                True   \n",
      "2   [0, 0, 4]         NaN             NaN               False   \n",
      "3   [0, 0, 4]         NaN             NaN               False   \n",
      "0   [0, 0, 6]           1             NaN                True   \n",
      "1   [0, 0, 6]           1             NaN                True   \n",
      "2   [0, 0, 6]           1             NaN               False   \n",
      "3   [0, 0, 6]           1             NaN               False   \n",
      "4   [0, 0, 6]           2             NaN                True   \n",
      "5   [0, 0, 6]           2             NaN                True   \n",
      "..        ...         ...             ...                 ...   \n",
      "50  [0, 0, 8]         NaN             NaN               False   \n",
      "51  [0, 0, 8]         NaN             NaN               False   \n",
      "52  [0, 0, 8]         NaN             NaN               False   \n",
      "53  [0, 0, 8]         NaN             NaN               False   \n",
      "54  [0, 0, 8]         NaN             NaN               False   \n",
      "55  [0, 0, 8]         NaN             NaN               False   \n",
      "56  [0, 0, 8]         NaN             NaN               False   \n",
      "57  [0, 0, 8]         NaN             NaN               False   \n",
      "58  [0, 0, 8]         NaN             NaN               False   \n",
      "59  [0, 0, 8]         NaN             NaN               False   \n",
      "60  [0, 0, 8]         NaN             NaN               False   \n",
      "61  [0, 0, 8]         NaN             NaN               False   \n",
      "62  [0, 0, 8]         NaN             NaN               False   \n",
      "63  [0, 0, 8]         NaN             NaN               False   \n",
      "0   [0, 0, 9]         NaN             aic                True   \n",
      "1   [0, 0, 9]         NaN             aic                True   \n",
      "2   [0, 0, 9]         NaN             aic                True   \n",
      "3   [0, 0, 9]         NaN             aic                True   \n",
      "4   [0, 0, 9]         NaN             aic               False   \n",
      "5   [0, 0, 9]         NaN             aic               False   \n",
      "6   [0, 0, 9]         NaN             aic               False   \n",
      "7   [0, 0, 9]         NaN             aic               False   \n",
      "8   [0, 0, 9]         NaN             bic                True   \n",
      "9   [0, 0, 9]         NaN             bic                True   \n",
      "10  [0, 0, 9]         NaN             bic                True   \n",
      "11  [0, 0, 9]         NaN             bic                True   \n",
      "12  [0, 0, 9]         NaN             bic               False   \n",
      "13  [0, 0, 9]         NaN             bic               False   \n",
      "14  [0, 0, 9]         NaN             bic               False   \n",
      "15  [0, 0, 9]         NaN             bic               False   \n",
      "\n",
      "   param_max_n_alphas param_n_alphas       ...        split0_test_score  \\\n",
      "0                 NaN            NaN       ...                 0.405206   \n",
      "1                 NaN            NaN       ...                 0.494276   \n",
      "2                 NaN            NaN       ...                 0.467146   \n",
      "3                 NaN            NaN       ...                 0.442689   \n",
      "4                 NaN            NaN       ...                 0.439291   \n",
      "5                 NaN            NaN       ...                 0.443830   \n",
      "6                 NaN            NaN       ...                 0.483314   \n",
      "7                 NaN            NaN       ...                 0.496800   \n",
      "0                 NaN            NaN       ...                 0.283188   \n",
      "1                 NaN            NaN       ...                 0.320898   \n",
      "2                 NaN            NaN       ...                 0.309973   \n",
      "3                 NaN            NaN       ...                 0.320773   \n",
      "4                 NaN            NaN       ...                 0.334430   \n",
      "5                 NaN            NaN       ...                 0.328006   \n",
      "6                 NaN            NaN       ...                 0.319685   \n",
      "7                 NaN            NaN       ...                 0.311854   \n",
      "0                 NaN            NaN       ...                 0.477591   \n",
      "1                 NaN            NaN       ...                 0.477591   \n",
      "2                 NaN            NaN       ...                 0.474478   \n",
      "3                 NaN            NaN       ...                 0.474478   \n",
      "0                 NaN            NaN       ...                 0.346831   \n",
      "1                 NaN            NaN       ...                 0.481139   \n",
      "2                 NaN            NaN       ...                 0.478163   \n",
      "3                 NaN            NaN       ...                 0.478163   \n",
      "0                 NaN            NaN       ...                -0.139219   \n",
      "1                 NaN            NaN       ...                 0.256755   \n",
      "2                 NaN            NaN       ...                 0.327736   \n",
      "3                 NaN            NaN       ...                 0.327736   \n",
      "4                 NaN            NaN       ...                -0.139219   \n",
      "5                 NaN            NaN       ...                 0.126198   \n",
      "..                ...            ...       ...                      ...   \n",
      "50               1400            NaN       ...                 0.474868   \n",
      "51               1400            NaN       ...                 0.480679   \n",
      "52               1500            NaN       ...                 0.474868   \n",
      "53               1500            NaN       ...                 0.480679   \n",
      "54               1600            NaN       ...                 0.474868   \n",
      "55               1600            NaN       ...                 0.480679   \n",
      "56               1700            NaN       ...                 0.474868   \n",
      "57               1700            NaN       ...                 0.480679   \n",
      "58               1800            NaN       ...                 0.474868   \n",
      "59               1800            NaN       ...                 0.480679   \n",
      "60               1900            NaN       ...                 0.474868   \n",
      "61               1900            NaN       ...                 0.480679   \n",
      "62               2000            NaN       ...                 0.474868   \n",
      "63               2000            NaN       ...                 0.480679   \n",
      "0                 NaN            NaN       ...                -0.139219   \n",
      "1                 NaN            NaN       ...                 0.477591   \n",
      "2                 NaN            NaN       ...                -0.139219   \n",
      "3                 NaN            NaN       ...                 0.479262   \n",
      "4                 NaN            NaN       ...                -0.888964   \n",
      "5                 NaN            NaN       ...                 0.472968   \n",
      "6                 NaN            NaN       ...                -0.888964   \n",
      "7                 NaN            NaN       ...                 0.472968   \n",
      "8                 NaN            NaN       ...                -0.139219   \n",
      "9                 NaN            NaN       ...                 0.403252   \n",
      "10                NaN            NaN       ...                -0.139219   \n",
      "11                NaN            NaN       ...                 0.479262   \n",
      "12                NaN            NaN       ...                -0.888964   \n",
      "13                NaN            NaN       ...                 0.472968   \n",
      "14                NaN            NaN       ...                -0.888964   \n",
      "15                NaN            NaN       ...                 0.472968   \n",
      "\n",
      "   split0_train_score split1_test_score split1_train_score  split2_test_score  \\\n",
      "0            0.900516          0.437757           0.918215           0.559186   \n",
      "1            0.923324          0.405933           0.925264           0.576696   \n",
      "2            0.929035          0.452861           0.933157           0.593257   \n",
      "3            0.925783          0.432066           0.937133           0.599064   \n",
      "4            0.894901          0.397995           0.919887           0.579476   \n",
      "5            0.926134          0.450539           0.935124           0.570797   \n",
      "6            0.930272          0.425251           0.936006           0.583939   \n",
      "7            0.929725          0.390728           0.934790           0.616149   \n",
      "0            0.999999          0.395423           0.999976           0.558876   \n",
      "1            0.999999          0.389036           0.999976           0.555985   \n",
      "2            0.999999          0.397995           0.999976           0.575444   \n",
      "3            0.999999          0.421707           0.999976           0.605744   \n",
      "4            0.999999          0.358684           0.999976           0.581883   \n",
      "5            0.999999          0.448363           0.999976           0.627892   \n",
      "6            0.999999          0.411515           0.999976           0.625488   \n",
      "7            0.999999          0.392534           0.999976           0.606243   \n",
      "0            0.463774          0.293992           0.567092           0.524410   \n",
      "1            0.463774          0.293992           0.567092           0.524410   \n",
      "2            0.463719          0.307132           0.556049           0.523140   \n",
      "3            0.463719          0.307132           0.556049           0.523140   \n",
      "0            0.382781          0.316059           0.465484           0.411636   \n",
      "1            0.463593          0.294810           0.567061           0.527691   \n",
      "2            0.463543          0.308503           0.555975           0.527068   \n",
      "3            0.463543          0.308503           0.555975           0.527068   \n",
      "0            0.000000         -0.032746           0.000000          -0.025697   \n",
      "1            0.292910          0.234169           0.345164           0.293167   \n",
      "2            0.304397          0.233787           0.346858           0.298829   \n",
      "3            0.304397          0.233787           0.346858           0.298829   \n",
      "4            0.000000         -0.032746           0.000000          -0.025697   \n",
      "5            0.204443          0.178140           0.257960           0.203580   \n",
      "..                ...               ...                ...                ...   \n",
      "50           0.463718          0.307132           0.556049           0.528083   \n",
      "51           0.462979          0.311370           0.555448           0.441429   \n",
      "52           0.463718          0.307132           0.556049           0.528083   \n",
      "53           0.462979          0.311370           0.555448           0.441429   \n",
      "54           0.463718          0.307132           0.556049           0.528083   \n",
      "55           0.462979          0.311370           0.555448           0.441429   \n",
      "56           0.463718          0.307132           0.556049           0.528083   \n",
      "57           0.462979          0.311370           0.555448           0.441429   \n",
      "58           0.463718          0.307132           0.556049           0.528083   \n",
      "59           0.462979          0.311370           0.555448           0.441429   \n",
      "60           0.463718          0.307132           0.556049           0.528083   \n",
      "61           0.462979          0.311370           0.555448           0.441429   \n",
      "62           0.463718          0.307132           0.556049           0.528083   \n",
      "63           0.462979          0.311370           0.555448           0.441429   \n",
      "0            0.000000         -0.032746           0.000000          -0.025697   \n",
      "1            0.463774          0.301816           0.565696           0.454223   \n",
      "2            0.000000         -0.032746           0.000000          -0.025697   \n",
      "3            0.459975          0.309778           0.560463           0.529119   \n",
      "4           -1.432482         -1.460262          -1.107619          -1.405365   \n",
      "5            0.459784          0.310189           0.555517           0.528781   \n",
      "6           -1.432482         -1.460262          -1.107619          -1.405365   \n",
      "7            0.459784          0.310189           0.555517           0.528781   \n",
      "8            0.000000         -0.032746           0.000000          -0.025697   \n",
      "9            0.402812          0.308190           0.529963           0.454223   \n",
      "10           0.000000         -0.032746           0.000000          -0.025697   \n",
      "11           0.459975          0.309778           0.560463           0.444542   \n",
      "12          -1.432482         -1.460262          -1.107619          -1.405365   \n",
      "13           0.459784          0.310189           0.555517           0.438571   \n",
      "14          -1.432482         -1.460262          -1.107619          -1.405365   \n",
      "15           0.459784          0.310189           0.555517           0.438571   \n",
      "\n",
      "    split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
      "0             0.919755      0.000108    1.590706e-04        0.066246   \n",
      "1             0.903296      0.003123    1.511005e-03        0.069603   \n",
      "2             0.907558      0.003846    7.039070e-04        0.063013   \n",
      "3             0.915997      0.000581    2.888158e-04        0.076261   \n",
      "4             0.913330      0.001840    3.442162e-04        0.077556   \n",
      "5             0.913528      0.001926    1.717574e-04        0.058292   \n",
      "6             0.917249      0.000627    4.681239e-04        0.065445   \n",
      "7             0.920845      0.008962    3.297199e-04        0.091917   \n",
      "0             0.999767      0.000651    1.492319e-04        0.113255   \n",
      "1             0.999767      0.001725    1.890016e-03        0.098767   \n",
      "2             0.999767      0.003258    7.548232e-04        0.110435   \n",
      "3             0.999767      0.007825    2.299589e-03        0.118016   \n",
      "4             0.999767      0.002929    9.104882e-04        0.111307   \n",
      "5             0.999767      0.002699    8.946138e-05        0.123283   \n",
      "6             0.999767      0.008569    1.126224e-04        0.128135   \n",
      "7             0.999767      0.002479    1.238335e-03        0.124209   \n",
      "0             0.452518      0.001449    7.148106e-05        0.099298   \n",
      "1             0.452518      0.000012    5.857317e-06        0.099298   \n",
      "2             0.448383      0.000006    0.000000e+00        0.092380   \n",
      "3             0.448383      0.000009    9.602742e-07        0.092380   \n",
      "0             0.381077      0.000077    7.636841e-06        0.039770   \n",
      "1             0.452229      0.000004    5.150430e-07        0.100478   \n",
      "2             0.448002      0.000002    7.786718e-07        0.093522   \n",
      "3             0.448002      0.000006    7.786718e-07        0.093522   \n",
      "0             0.000000      0.000057    6.575629e-06        0.052024   \n",
      "1             0.282289      0.000002    1.189441e-06        0.024263   \n",
      "2             0.290443      0.000005    4.052337e-07        0.039295   \n",
      "3             0.290443      0.000004    4.052337e-07        0.039295   \n",
      "4             0.000000      0.000061    2.051724e-05        0.052024   \n",
      "5             0.199404      0.000135    1.253002e-05        0.032248   \n",
      "..                 ...           ...             ...             ...   \n",
      "50            0.446799      0.000705    1.910657e-06        0.094014   \n",
      "51            0.402462      0.000590    1.743809e-05        0.072348   \n",
      "52            0.446799      0.000482    1.959614e-06        0.094014   \n",
      "53            0.402462      0.000690    4.230764e-06        0.072348   \n",
      "54            0.446799      0.000321    2.061306e-05        0.094014   \n",
      "55            0.402462      0.000959    6.187665e-06        0.072348   \n",
      "56            0.446799      0.000137    2.081521e-06        0.094014   \n",
      "57            0.402462      0.000549    9.340682e-06        0.072348   \n",
      "58            0.446799      0.000296    8.638901e-05        0.094014   \n",
      "59            0.402462      0.001272    2.200265e-05        0.072348   \n",
      "60            0.446799      0.000680    3.690075e-05        0.094014   \n",
      "61            0.402462      0.001129    9.934425e-06        0.072348   \n",
      "62            0.446799      0.000360    3.173167e-05        0.094014   \n",
      "63            0.402462      0.000697    6.138475e-06        0.072348   \n",
      "0             0.000000      0.000123    7.570390e-06        0.052024   \n",
      "1             0.416760      0.000098    3.017877e-06        0.077901   \n",
      "2             0.000000      0.000047    1.072147e-06        0.052024   \n",
      "3             0.451387      0.000154    7.080671e-06        0.093742   \n",
      "4            -1.128905      0.000036    7.786718e-07        0.257800   \n",
      "5             0.446872      0.000013    4.495664e-07        0.092592   \n",
      "6            -1.128905      0.000003    1.296163e-06        0.257800   \n",
      "7             0.446872      0.000034    5.619580e-07        0.092592   \n",
      "8             0.000000      0.000064    4.495664e-07        0.052024   \n",
      "9             0.416760      0.000057    1.730247e-06        0.060415   \n",
      "10            0.000000      0.000070    7.123358e-06        0.052024   \n",
      "11            0.410552      0.000050    5.018521e-05        0.073082   \n",
      "12           -1.128905      0.000009    2.974048e-05        0.257800   \n",
      "13            0.401022      0.000039    2.170642e-06        0.070034   \n",
      "14           -1.128905      0.000014    4.449061e-06        0.257800   \n",
      "15            0.401022      0.000150    6.751143e-05        0.070034   \n",
      "\n",
      "    std_train_score  \n",
      "0          0.008729  \n",
      "1          0.009930  \n",
      "2          0.011223  \n",
      "3          0.008637  \n",
      "4          0.010577  \n",
      "5          0.008858  \n",
      "6          0.007848  \n",
      "7          0.005764  \n",
      "0          0.000104  \n",
      "1          0.000104  \n",
      "2          0.000104  \n",
      "3          0.000104  \n",
      "4          0.000104  \n",
      "5          0.000104  \n",
      "6          0.000104  \n",
      "7          0.000104  \n",
      "0          0.051562  \n",
      "1          0.051562  \n",
      "2          0.047554  \n",
      "3          0.047554  \n",
      "0          0.039394  \n",
      "1          0.051662  \n",
      "2          0.047660  \n",
      "3          0.047660  \n",
      "0          0.000000  \n",
      "1          0.027481  \n",
      "2          0.023991  \n",
      "3          0.023991  \n",
      "4          0.000000  \n",
      "5          0.026496  \n",
      "..              ...  \n",
      "50         0.048013  \n",
      "51         0.062909  \n",
      "52         0.048013  \n",
      "53         0.062909  \n",
      "54         0.048013  \n",
      "55         0.062909  \n",
      "56         0.048013  \n",
      "57         0.062909  \n",
      "58         0.048013  \n",
      "59         0.062909  \n",
      "60         0.048013  \n",
      "61         0.062909  \n",
      "62         0.048013  \n",
      "63         0.062909  \n",
      "0          0.000000  \n",
      "1          0.062165  \n",
      "2          0.000000  \n",
      "3          0.049519  \n",
      "4          0.148379  \n",
      "5          0.048460  \n",
      "6          0.148379  \n",
      "7          0.048460  \n",
      "8          0.000000  \n",
      "9          0.056937  \n",
      "10         0.000000  \n",
      "11         0.062373  \n",
      "12         0.148379  \n",
      "13         0.063671  \n",
      "14         0.148379  \n",
      "15         0.063671  \n",
      "\n",
      "[188 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/coordinate_descent.py:470: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/model_selection/_validation.py:238: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/model_selection/_search.py:645: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  best_estimator.fit(X, y, **self.fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0         0.024165         0.004869         0.467161          0.912829   \n",
      "1         0.063295         0.013676         0.492309          0.917294   \n",
      "2         0.097568         0.019110         0.504288          0.923250   \n",
      "3         0.135895         0.025160         0.491100          0.926304   \n",
      "4         0.052104         0.005416         0.472136          0.909373   \n",
      "5         0.133042         0.012063         0.488230          0.924929   \n",
      "6         0.213503         0.019280         0.497451          0.927842   \n",
      "7         0.297030         0.025394         0.501210          0.928453   \n",
      "0         0.022245         0.004949         0.412033          0.999914   \n",
      "1         0.055298         0.013039         0.421612          0.999914   \n",
      "2         0.090962         0.019441         0.427383          0.999914   \n",
      "3         0.123162         0.027238         0.448949          0.999914   \n",
      "4         0.053856         0.006441         0.424675          0.999914   \n",
      "5         0.131324         0.011887         0.467587          0.999914   \n",
      "6         0.210276         0.018615         0.451756          0.999914   \n",
      "7         0.283644         0.029835         0.436431          0.999914   \n",
      "0         0.001577         0.000264         0.432161          0.494461   \n",
      "1         0.000414         0.000186         0.432161          0.494461   \n",
      "2         0.000347         0.000180         0.435058          0.489384   \n",
      "3         0.000346         0.000180         0.435058          0.489384   \n",
      "0         0.000483         0.000179         0.358135          0.409781   \n",
      "1         0.000383         0.000173         0.434713          0.494294   \n",
      "2         0.000341         0.000176         0.438055          0.489173   \n",
      "3         0.000336         0.000175         0.438055          0.489173   \n",
      "0         0.000454         0.000188        -0.066149          0.000000   \n",
      "1         0.000361         0.000185         0.261347          0.306788   \n",
      "2         0.000422         0.000187         0.286931          0.313899   \n",
      "3         0.000423         0.000188         0.286931          0.313899   \n",
      "4         0.000453         0.000199        -0.066149          0.000000   \n",
      "5         0.000574         0.000238         0.169152          0.220602   \n",
      "..             ...              ...              ...               ...   \n",
      "450       0.000320         0.000184        -1.250235         -1.223002   \n",
      "451       0.000303         0.000182        -1.250235         -1.223002   \n",
      "452       0.000362         0.000195        -1.250235         -1.223002   \n",
      "453       0.000356         0.000192        -1.250235         -1.223002   \n",
      "454       0.000383         0.000252        -1.250235         -1.223002   \n",
      "455       0.000333         0.000188        -1.250235         -1.223002   \n",
      "456       0.001924         0.000224        -0.065938          0.000199   \n",
      "457       0.002262         0.000220         0.104722          0.164244   \n",
      "458       0.000526         0.000229        -0.066149          0.000000   \n",
      "459       0.000375         0.000189        -0.066149          0.000000   \n",
      "460       0.000381         0.000180        -0.066149          0.000000   \n",
      "461       0.000337         0.000183        -0.066149          0.000000   \n",
      "462       0.000390         0.000181        -0.066149          0.000000   \n",
      "463       0.000327         0.000181        -0.066149          0.000000   \n",
      "464       0.000370         0.000181        -0.066149          0.000000   \n",
      "465       0.000364         0.000180        -0.066149          0.000000   \n",
      "466       0.000383         0.000180        -0.066149          0.000000   \n",
      "467       0.000542         0.000244        -0.066149          0.000000   \n",
      "468       0.001953         0.000211         0.121977          0.152365   \n",
      "469       0.001854         0.000245         0.121977          0.152365   \n",
      "470       0.000376         0.000203        -0.974362         -0.933145   \n",
      "471       0.000315         0.000187        -0.974362         -0.933145   \n",
      "472       0.000315         0.000188        -1.250235         -1.223002   \n",
      "473       0.000327         0.000194        -1.250235         -1.223002   \n",
      "474       0.000337         0.000184        -1.250235         -1.223002   \n",
      "475       0.000367         0.000193        -1.250235         -1.223002   \n",
      "476       0.000418         0.000244        -1.250235         -1.223002   \n",
      "477       0.000415         0.000237        -1.250235         -1.223002   \n",
      "478       0.000332         0.000192        -1.250235         -1.223002   \n",
      "479       0.000314         0.000186        -1.250235         -1.223002   \n",
      "\n",
      "     method_ids param_alpha param_criterion param_fit_intercept  \\\n",
      "0     [0, 0, 1]         NaN             mse                 NaN   \n",
      "1     [0, 0, 1]         NaN             mse                 NaN   \n",
      "2     [0, 0, 1]         NaN             mse                 NaN   \n",
      "3     [0, 0, 1]         NaN             mse                 NaN   \n",
      "4     [0, 0, 1]         NaN             mae                 NaN   \n",
      "5     [0, 0, 1]         NaN             mae                 NaN   \n",
      "6     [0, 0, 1]         NaN             mae                 NaN   \n",
      "7     [0, 0, 1]         NaN             mae                 NaN   \n",
      "0     [0, 0, 2]         NaN             mse                 NaN   \n",
      "1     [0, 0, 2]         NaN             mse                 NaN   \n",
      "2     [0, 0, 2]         NaN             mse                 NaN   \n",
      "3     [0, 0, 2]         NaN             mse                 NaN   \n",
      "4     [0, 0, 2]         NaN             mae                 NaN   \n",
      "5     [0, 0, 2]         NaN             mae                 NaN   \n",
      "6     [0, 0, 2]         NaN             mae                 NaN   \n",
      "7     [0, 0, 2]         NaN             mae                 NaN   \n",
      "0     [0, 0, 3]         NaN             NaN                True   \n",
      "1     [0, 0, 3]         NaN             NaN                True   \n",
      "2     [0, 0, 3]         NaN             NaN               False   \n",
      "3     [0, 0, 3]         NaN             NaN               False   \n",
      "0     [0, 0, 4]         NaN             NaN                True   \n",
      "1     [0, 0, 4]         NaN             NaN                True   \n",
      "2     [0, 0, 4]         NaN             NaN               False   \n",
      "3     [0, 0, 4]         NaN             NaN               False   \n",
      "0     [0, 0, 6]           1             NaN                True   \n",
      "1     [0, 0, 6]           1             NaN                True   \n",
      "2     [0, 0, 6]           1             NaN               False   \n",
      "3     [0, 0, 6]           1             NaN               False   \n",
      "4     [0, 0, 6]           2             NaN                True   \n",
      "5     [0, 0, 6]           2             NaN                True   \n",
      "..          ...         ...             ...                 ...   \n",
      "450  [0, 0, 10]          80             NaN               False   \n",
      "451  [0, 0, 10]          80             NaN               False   \n",
      "452  [0, 0, 10]          80             NaN               False   \n",
      "453  [0, 0, 10]          80             NaN               False   \n",
      "454  [0, 0, 10]          80             NaN               False   \n",
      "455  [0, 0, 10]          80             NaN               False   \n",
      "456  [0, 0, 10]          90             NaN                True   \n",
      "457  [0, 0, 10]          90             NaN                True   \n",
      "458  [0, 0, 10]          90             NaN                True   \n",
      "459  [0, 0, 10]          90             NaN                True   \n",
      "460  [0, 0, 10]          90             NaN                True   \n",
      "461  [0, 0, 10]          90             NaN                True   \n",
      "462  [0, 0, 10]          90             NaN                True   \n",
      "463  [0, 0, 10]          90             NaN                True   \n",
      "464  [0, 0, 10]          90             NaN                True   \n",
      "465  [0, 0, 10]          90             NaN                True   \n",
      "466  [0, 0, 10]          90             NaN                True   \n",
      "467  [0, 0, 10]          90             NaN                True   \n",
      "468  [0, 0, 10]          90             NaN               False   \n",
      "469  [0, 0, 10]          90             NaN               False   \n",
      "470  [0, 0, 10]          90             NaN               False   \n",
      "471  [0, 0, 10]          90             NaN               False   \n",
      "472  [0, 0, 10]          90             NaN               False   \n",
      "473  [0, 0, 10]          90             NaN               False   \n",
      "474  [0, 0, 10]          90             NaN               False   \n",
      "475  [0, 0, 10]          90             NaN               False   \n",
      "476  [0, 0, 10]          90             NaN               False   \n",
      "477  [0, 0, 10]          90             NaN               False   \n",
      "478  [0, 0, 10]          90             NaN               False   \n",
      "479  [0, 0, 10]          90             NaN               False   \n",
      "\n",
      "    param_l1_ratio param_max_n_alphas       ...        split0_test_score  \\\n",
      "0              NaN                NaN       ...                 0.405206   \n",
      "1              NaN                NaN       ...                 0.494276   \n",
      "2              NaN                NaN       ...                 0.467146   \n",
      "3              NaN                NaN       ...                 0.442689   \n",
      "4              NaN                NaN       ...                 0.439291   \n",
      "5              NaN                NaN       ...                 0.443830   \n",
      "6              NaN                NaN       ...                 0.483314   \n",
      "7              NaN                NaN       ...                 0.496800   \n",
      "0              NaN                NaN       ...                 0.283188   \n",
      "1              NaN                NaN       ...                 0.320898   \n",
      "2              NaN                NaN       ...                 0.309973   \n",
      "3              NaN                NaN       ...                 0.320773   \n",
      "4              NaN                NaN       ...                 0.334430   \n",
      "5              NaN                NaN       ...                 0.328006   \n",
      "6              NaN                NaN       ...                 0.319685   \n",
      "7              NaN                NaN       ...                 0.311854   \n",
      "0              NaN                NaN       ...                 0.477591   \n",
      "1              NaN                NaN       ...                 0.477591   \n",
      "2              NaN                NaN       ...                 0.474478   \n",
      "3              NaN                NaN       ...                 0.474478   \n",
      "0              NaN                NaN       ...                 0.346831   \n",
      "1              NaN                NaN       ...                 0.481139   \n",
      "2              NaN                NaN       ...                 0.478163   \n",
      "3              NaN                NaN       ...                 0.478163   \n",
      "0              NaN                NaN       ...                -0.139219   \n",
      "1              NaN                NaN       ...                 0.256755   \n",
      "2              NaN                NaN       ...                 0.327736   \n",
      "3              NaN                NaN       ...                 0.327736   \n",
      "4              NaN                NaN       ...                -0.139219   \n",
      "5              NaN                NaN       ...                 0.126198   \n",
      "..             ...                ...       ...                      ...   \n",
      "450            0.6                NaN       ...                -0.888964   \n",
      "451            0.6                NaN       ...                -0.888964   \n",
      "452            0.8                NaN       ...                -0.888964   \n",
      "453            0.8                NaN       ...                -0.888964   \n",
      "454              1                NaN       ...                -0.888964   \n",
      "455              1                NaN       ...                -0.888964   \n",
      "456              0                NaN       ...                -0.139005   \n",
      "457              0                NaN       ...                 0.049452   \n",
      "458            0.2                NaN       ...                -0.139219   \n",
      "459            0.2                NaN       ...                -0.139219   \n",
      "460            0.4                NaN       ...                -0.139219   \n",
      "461            0.4                NaN       ...                -0.139219   \n",
      "462            0.6                NaN       ...                -0.139219   \n",
      "463            0.6                NaN       ...                -0.139219   \n",
      "464            0.8                NaN       ...                -0.139219   \n",
      "465            0.8                NaN       ...                -0.139219   \n",
      "466              1                NaN       ...                -0.139219   \n",
      "467              1                NaN       ...                -0.139219   \n",
      "468              0                NaN       ...                 0.249007   \n",
      "469              0                NaN       ...                 0.249007   \n",
      "470            0.2                NaN       ...                -0.496119   \n",
      "471            0.2                NaN       ...                -0.496119   \n",
      "472            0.4                NaN       ...                -0.888964   \n",
      "473            0.4                NaN       ...                -0.888964   \n",
      "474            0.6                NaN       ...                -0.888964   \n",
      "475            0.6                NaN       ...                -0.888964   \n",
      "476            0.8                NaN       ...                -0.888964   \n",
      "477            0.8                NaN       ...                -0.888964   \n",
      "478              1                NaN       ...                -0.888964   \n",
      "479              1                NaN       ...                -0.888964   \n",
      "\n",
      "    split0_train_score split1_test_score split1_train_score split2_test_score  \\\n",
      "0             0.900516          0.437757           0.918215          0.559186   \n",
      "1             0.923324          0.405933           0.925264          0.576696   \n",
      "2             0.929035          0.452861           0.933157          0.593257   \n",
      "3             0.925783          0.432066           0.937133          0.599064   \n",
      "4             0.894901          0.397995           0.919887          0.579476   \n",
      "5             0.926134          0.450539           0.935124          0.570797   \n",
      "6             0.930272          0.425251           0.936006          0.583939   \n",
      "7             0.929725          0.390728           0.934790          0.616149   \n",
      "0             0.999999          0.395423           0.999976          0.558876   \n",
      "1             0.999999          0.389036           0.999976          0.555985   \n",
      "2             0.999999          0.397995           0.999976          0.575444   \n",
      "3             0.999999          0.421707           0.999976          0.605744   \n",
      "4             0.999999          0.358684           0.999976          0.581883   \n",
      "5             0.999999          0.448363           0.999976          0.627892   \n",
      "6             0.999999          0.411515           0.999976          0.625488   \n",
      "7             0.999999          0.392534           0.999976          0.606243   \n",
      "0             0.463774          0.293992           0.567092          0.524410   \n",
      "1             0.463774          0.293992           0.567092          0.524410   \n",
      "2             0.463719          0.307132           0.556049          0.523140   \n",
      "3             0.463719          0.307132           0.556049          0.523140   \n",
      "0             0.382781          0.316059           0.465484          0.411636   \n",
      "1             0.463593          0.294810           0.567061          0.527691   \n",
      "2             0.463543          0.308503           0.555975          0.527068   \n",
      "3             0.463543          0.308503           0.555975          0.527068   \n",
      "0             0.000000         -0.032746           0.000000         -0.025697   \n",
      "1             0.292910          0.234169           0.345164          0.293167   \n",
      "2             0.304397          0.233787           0.346858          0.298829   \n",
      "3             0.304397          0.233787           0.346858          0.298829   \n",
      "4             0.000000         -0.032746           0.000000         -0.025697   \n",
      "5             0.204443          0.178140           0.257960          0.203580   \n",
      "..                 ...               ...                ...               ...   \n",
      "450          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "451          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "452          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "453          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "454          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "455          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "456           0.000190         -0.032538           0.000217         -0.025486   \n",
      "457           0.164034          0.125602           0.173398          0.139705   \n",
      "458           0.000000         -0.032746           0.000000         -0.025697   \n",
      "459           0.000000         -0.032746           0.000000         -0.025697   \n",
      "460           0.000000         -0.032746           0.000000         -0.025697   \n",
      "461           0.000000         -0.032746           0.000000         -0.025697   \n",
      "462           0.000000         -0.032746           0.000000         -0.025697   \n",
      "463           0.000000         -0.032746           0.000000         -0.025697   \n",
      "464           0.000000         -0.032746           0.000000         -0.025697   \n",
      "465           0.000000         -0.032746           0.000000         -0.025697   \n",
      "466           0.000000         -0.032746           0.000000         -0.025697   \n",
      "467           0.000000         -0.032746           0.000000         -0.025697   \n",
      "468           0.129038          0.047473           0.178626          0.068083   \n",
      "469           0.129038          0.047473           0.178626          0.068083   \n",
      "470          -0.948374         -1.238417          -0.911439         -1.193694   \n",
      "471          -0.948374         -1.238417          -0.911439         -1.193694   \n",
      "472          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "473          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "474          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "475          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "476          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "477          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "478          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "479          -1.432482         -1.460262          -1.107619         -1.405365   \n",
      "\n",
      "     split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
      "0              0.919755      0.000108    1.590706e-04        0.066246   \n",
      "1              0.903296      0.003123    1.511005e-03        0.069603   \n",
      "2              0.907558      0.003846    7.039070e-04        0.063013   \n",
      "3              0.915997      0.000581    2.888158e-04        0.076261   \n",
      "4              0.913330      0.001840    3.442162e-04        0.077556   \n",
      "5              0.913528      0.001926    1.717574e-04        0.058292   \n",
      "6              0.917249      0.000627    4.681239e-04        0.065445   \n",
      "7              0.920845      0.008962    3.297199e-04        0.091917   \n",
      "0              0.999767      0.000651    1.492319e-04        0.113255   \n",
      "1              0.999767      0.001725    1.890016e-03        0.098767   \n",
      "2              0.999767      0.003258    7.548232e-04        0.110435   \n",
      "3              0.999767      0.007825    2.299589e-03        0.118016   \n",
      "4              0.999767      0.002929    9.104882e-04        0.111307   \n",
      "5              0.999767      0.002699    8.946138e-05        0.123283   \n",
      "6              0.999767      0.008569    1.126224e-04        0.128135   \n",
      "7              0.999767      0.002479    1.238335e-03        0.124209   \n",
      "0              0.452518      0.001449    7.148106e-05        0.099298   \n",
      "1              0.452518      0.000012    5.857317e-06        0.099298   \n",
      "2              0.448383      0.000006    0.000000e+00        0.092380   \n",
      "3              0.448383      0.000009    9.602742e-07        0.092380   \n",
      "0              0.381077      0.000077    7.636841e-06        0.039770   \n",
      "1              0.452229      0.000004    5.150430e-07        0.100478   \n",
      "2              0.448002      0.000002    7.786718e-07        0.093522   \n",
      "3              0.448002      0.000006    7.786718e-07        0.093522   \n",
      "0              0.000000      0.000057    6.575629e-06        0.052024   \n",
      "1              0.282289      0.000002    1.189441e-06        0.024263   \n",
      "2              0.290443      0.000005    4.052337e-07        0.039295   \n",
      "3              0.290443      0.000004    4.052337e-07        0.039295   \n",
      "4              0.000000      0.000061    2.051724e-05        0.052024   \n",
      "5              0.199404      0.000135    1.253002e-05        0.032248   \n",
      "..                  ...           ...             ...             ...   \n",
      "450           -1.128905      0.000048    1.959614e-06        0.257800   \n",
      "451           -1.128905      0.000017    8.778064e-07        0.257800   \n",
      "452           -1.128905      0.000012    1.540872e-05        0.257800   \n",
      "453           -1.128905      0.000078    1.043306e-05        0.257800   \n",
      "454           -1.128905      0.000008    6.390695e-05        0.257800   \n",
      "455           -1.128905      0.000036    3.821314e-06        0.257800   \n",
      "456            0.000191      0.000030    2.612035e-05        0.052022   \n",
      "457            0.155301      0.000738    4.525224e-05        0.039709   \n",
      "458            0.000000      0.000026    8.729724e-06        0.052024   \n",
      "459            0.000000      0.000016    6.633009e-06        0.052024   \n",
      "460            0.000000      0.000002    4.495664e-07        0.052024   \n",
      "461            0.000000      0.000002    4.495664e-07        0.052024   \n",
      "462            0.000000      0.000022    8.778064e-07        0.052024   \n",
      "463            0.000000      0.000001    8.778064e-07        0.052024   \n",
      "464            0.000000      0.000002    0.000000e+00        0.052024   \n",
      "465            0.000000      0.000052    4.495664e-07        0.052024   \n",
      "466            0.000000      0.000008    7.786718e-07        0.052024   \n",
      "467            0.000000      0.000148    4.858399e-05        0.052024   \n",
      "468            0.149430      0.000144    2.695032e-05        0.090696   \n",
      "469            0.149430      0.000052    3.933401e-05        0.090696   \n",
      "470           -0.939624      0.000040    2.430649e-05        0.340471   \n",
      "471           -0.939624      0.000007    1.296163e-06        0.340471   \n",
      "472           -1.128905      0.000008    4.126471e-06        0.257800   \n",
      "473           -1.128905      0.000016    7.842486e-06        0.257800   \n",
      "474           -1.128905      0.000035    2.050954e-06        0.257800   \n",
      "475           -1.128905      0.000046    1.439139e-05        0.257800   \n",
      "476           -1.128905      0.000161    8.575832e-05        0.257800   \n",
      "477           -1.128905      0.000005    7.530237e-06        0.257800   \n",
      "478           -1.128905      0.000025    8.645391e-06        0.257800   \n",
      "479           -1.128905      0.000002    2.570305e-06        0.257800   \n",
      "\n",
      "     std_train_score  \n",
      "0           0.008729  \n",
      "1           0.009930  \n",
      "2           0.011223  \n",
      "3           0.008637  \n",
      "4           0.010577  \n",
      "5           0.008858  \n",
      "6           0.007848  \n",
      "7           0.005764  \n",
      "0           0.000104  \n",
      "1           0.000104  \n",
      "2           0.000104  \n",
      "3           0.000104  \n",
      "4           0.000104  \n",
      "5           0.000104  \n",
      "6           0.000104  \n",
      "7           0.000104  \n",
      "0           0.051562  \n",
      "1           0.051562  \n",
      "2           0.047554  \n",
      "3           0.047554  \n",
      "0           0.039394  \n",
      "1           0.051662  \n",
      "2           0.047660  \n",
      "3           0.047660  \n",
      "0           0.000000  \n",
      "1           0.027481  \n",
      "2           0.023991  \n",
      "3           0.023991  \n",
      "4           0.000000  \n",
      "5           0.026496  \n",
      "..               ...  \n",
      "450         0.148379  \n",
      "451         0.148379  \n",
      "452         0.148379  \n",
      "453         0.148379  \n",
      "454         0.148379  \n",
      "455         0.148379  \n",
      "456         0.000013  \n",
      "457         0.007390  \n",
      "458         0.000000  \n",
      "459         0.000000  \n",
      "460         0.000000  \n",
      "461         0.000000  \n",
      "462         0.000000  \n",
      "463         0.000000  \n",
      "464         0.000000  \n",
      "465         0.000000  \n",
      "466         0.000000  \n",
      "467         0.000000  \n",
      "468         0.020350  \n",
      "469         0.020350  \n",
      "470         0.015759  \n",
      "471         0.015759  \n",
      "472         0.148379  \n",
      "473         0.148379  \n",
      "474         0.148379  \n",
      "475         0.148379  \n",
      "476         0.148379  \n",
      "477         0.148379  \n",
      "478         0.148379  \n",
      "479         0.148379  \n",
      "\n",
      "[668 rows x 26 columns]\n",
      "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0         0.024165         0.004869         0.467161          0.912829   \n",
      "1         0.063295         0.013676         0.492309          0.917294   \n",
      "2         0.097568         0.019110         0.504288          0.923250   \n",
      "3         0.135895         0.025160         0.491100          0.926304   \n",
      "4         0.052104         0.005416         0.472136          0.909373   \n",
      "5         0.133042         0.012063         0.488230          0.924929   \n",
      "6         0.213503         0.019280         0.497451          0.927842   \n",
      "7         0.297030         0.025394         0.501210          0.928453   \n",
      "0         0.022245         0.004949         0.412033          0.999914   \n",
      "1         0.055298         0.013039         0.421612          0.999914   \n",
      "2         0.090962         0.019441         0.427383          0.999914   \n",
      "3         0.123162         0.027238         0.448949          0.999914   \n",
      "4         0.053856         0.006441         0.424675          0.999914   \n",
      "5         0.131324         0.011887         0.467587          0.999914   \n",
      "6         0.210276         0.018615         0.451756          0.999914   \n",
      "7         0.283644         0.029835         0.436431          0.999914   \n",
      "0         0.001577         0.000264         0.432161          0.494461   \n",
      "1         0.000414         0.000186         0.432161          0.494461   \n",
      "2         0.000347         0.000180         0.435058          0.489384   \n",
      "3         0.000346         0.000180         0.435058          0.489384   \n",
      "0         0.000483         0.000179         0.358135          0.409781   \n",
      "1         0.000383         0.000173         0.434713          0.494294   \n",
      "2         0.000341         0.000176         0.438055          0.489173   \n",
      "3         0.000336         0.000175         0.438055          0.489173   \n",
      "0         0.000454         0.000188        -0.066149          0.000000   \n",
      "1         0.000361         0.000185         0.261347          0.306788   \n",
      "2         0.000422         0.000187         0.286931          0.313899   \n",
      "3         0.000423         0.000188         0.286931          0.313899   \n",
      "4         0.000453         0.000199        -0.066149          0.000000   \n",
      "5         0.000574         0.000238         0.169152          0.220602   \n",
      "..             ...              ...              ...               ...   \n",
      "190       0.075649         0.000216         0.435922          0.485908   \n",
      "191       0.076295         0.000218         0.435922          0.485908   \n",
      "192       0.078426         0.000210         0.435968          0.485937   \n",
      "193       0.083999         0.000292         0.435968          0.485937   \n",
      "194       0.099561         0.000210         0.435956          0.485910   \n",
      "195       0.095372         0.000237         0.435956          0.485910   \n",
      "196       0.101176         0.000233         0.435990          0.485933   \n",
      "197       0.099992         0.000218         0.435990          0.485933   \n",
      "198       0.011504         0.000196         0.437682          0.486718   \n",
      "199       0.012661         0.000204         0.437682          0.486718   \n",
      "200       0.022740         0.000214         0.437084          0.486610   \n",
      "201       0.023490         0.000256         0.437084          0.486610   \n",
      "202       0.031117         0.000218         0.436829          0.486466   \n",
      "203       0.031029         0.000225         0.436829          0.486466   \n",
      "204       0.039176         0.000206         0.436990          0.486543   \n",
      "205       0.041093         0.000219         0.436990          0.486543   \n",
      "206       0.046504         0.000201         0.436878          0.486483   \n",
      "207       0.046139         0.000199         0.436878          0.486483   \n",
      "208       0.054371         0.000200         0.436884          0.486471   \n",
      "209       0.056604         0.000201         0.436884          0.486471   \n",
      "210       0.062589         0.000203         0.436958          0.486510   \n",
      "211       0.064783         0.000215         0.436958          0.486510   \n",
      "212       0.071664         0.000210         0.436897          0.486479   \n",
      "213       0.074883         0.000226         0.436897          0.486479   \n",
      "214       0.081957         0.000217         0.436956          0.486527   \n",
      "215       0.082827         0.000211         0.436956          0.486527   \n",
      "216       0.091498         0.000214         0.436951          0.486516   \n",
      "217       0.088703         0.000218         0.436951          0.486516   \n",
      "218       0.093279         0.000232         0.436909          0.486494   \n",
      "219       0.102064         0.000210         0.436909          0.486494   \n",
      "\n",
      "     method_ids param_alpha param_criterion param_fit_intercept  \\\n",
      "0     [0, 0, 1]         NaN             mse                 NaN   \n",
      "1     [0, 0, 1]         NaN             mse                 NaN   \n",
      "2     [0, 0, 1]         NaN             mse                 NaN   \n",
      "3     [0, 0, 1]         NaN             mse                 NaN   \n",
      "4     [0, 0, 1]         NaN             mae                 NaN   \n",
      "5     [0, 0, 1]         NaN             mae                 NaN   \n",
      "6     [0, 0, 1]         NaN             mae                 NaN   \n",
      "7     [0, 0, 1]         NaN             mae                 NaN   \n",
      "0     [0, 0, 2]         NaN             mse                 NaN   \n",
      "1     [0, 0, 2]         NaN             mse                 NaN   \n",
      "2     [0, 0, 2]         NaN             mse                 NaN   \n",
      "3     [0, 0, 2]         NaN             mse                 NaN   \n",
      "4     [0, 0, 2]         NaN             mae                 NaN   \n",
      "5     [0, 0, 2]         NaN             mae                 NaN   \n",
      "6     [0, 0, 2]         NaN             mae                 NaN   \n",
      "7     [0, 0, 2]         NaN             mae                 NaN   \n",
      "0     [0, 0, 3]         NaN             NaN                True   \n",
      "1     [0, 0, 3]         NaN             NaN                True   \n",
      "2     [0, 0, 3]         NaN             NaN               False   \n",
      "3     [0, 0, 3]         NaN             NaN               False   \n",
      "0     [0, 0, 4]         NaN             NaN                True   \n",
      "1     [0, 0, 4]         NaN             NaN                True   \n",
      "2     [0, 0, 4]         NaN             NaN               False   \n",
      "3     [0, 0, 4]         NaN             NaN               False   \n",
      "0     [0, 0, 6]           1             NaN                True   \n",
      "1     [0, 0, 6]           1             NaN                True   \n",
      "2     [0, 0, 6]           1             NaN               False   \n",
      "3     [0, 0, 6]           1             NaN               False   \n",
      "4     [0, 0, 6]           2             NaN                True   \n",
      "5     [0, 0, 6]           2             NaN                True   \n",
      "..          ...         ...             ...                 ...   \n",
      "190  [0, 0, 11]         NaN             NaN               False   \n",
      "191  [0, 0, 11]         NaN             NaN               False   \n",
      "192  [0, 0, 11]         NaN             NaN               False   \n",
      "193  [0, 0, 11]         NaN             NaN               False   \n",
      "194  [0, 0, 11]         NaN             NaN               False   \n",
      "195  [0, 0, 11]         NaN             NaN               False   \n",
      "196  [0, 0, 11]         NaN             NaN               False   \n",
      "197  [0, 0, 11]         NaN             NaN               False   \n",
      "198  [0, 0, 11]         NaN             NaN               False   \n",
      "199  [0, 0, 11]         NaN             NaN               False   \n",
      "200  [0, 0, 11]         NaN             NaN               False   \n",
      "201  [0, 0, 11]         NaN             NaN               False   \n",
      "202  [0, 0, 11]         NaN             NaN               False   \n",
      "203  [0, 0, 11]         NaN             NaN               False   \n",
      "204  [0, 0, 11]         NaN             NaN               False   \n",
      "205  [0, 0, 11]         NaN             NaN               False   \n",
      "206  [0, 0, 11]         NaN             NaN               False   \n",
      "207  [0, 0, 11]         NaN             NaN               False   \n",
      "208  [0, 0, 11]         NaN             NaN               False   \n",
      "209  [0, 0, 11]         NaN             NaN               False   \n",
      "210  [0, 0, 11]         NaN             NaN               False   \n",
      "211  [0, 0, 11]         NaN             NaN               False   \n",
      "212  [0, 0, 11]         NaN             NaN               False   \n",
      "213  [0, 0, 11]         NaN             NaN               False   \n",
      "214  [0, 0, 11]         NaN             NaN               False   \n",
      "215  [0, 0, 11]         NaN             NaN               False   \n",
      "216  [0, 0, 11]         NaN             NaN               False   \n",
      "217  [0, 0, 11]         NaN             NaN               False   \n",
      "218  [0, 0, 11]         NaN             NaN               False   \n",
      "219  [0, 0, 11]         NaN             NaN               False   \n",
      "\n",
      "    param_l1_ratio param_max_n_alphas       ...        split0_test_score  \\\n",
      "0              NaN                NaN       ...                 0.405206   \n",
      "1              NaN                NaN       ...                 0.494276   \n",
      "2              NaN                NaN       ...                 0.467146   \n",
      "3              NaN                NaN       ...                 0.442689   \n",
      "4              NaN                NaN       ...                 0.439291   \n",
      "5              NaN                NaN       ...                 0.443830   \n",
      "6              NaN                NaN       ...                 0.483314   \n",
      "7              NaN                NaN       ...                 0.496800   \n",
      "0              NaN                NaN       ...                 0.283188   \n",
      "1              NaN                NaN       ...                 0.320898   \n",
      "2              NaN                NaN       ...                 0.309973   \n",
      "3              NaN                NaN       ...                 0.320773   \n",
      "4              NaN                NaN       ...                 0.334430   \n",
      "5              NaN                NaN       ...                 0.328006   \n",
      "6              NaN                NaN       ...                 0.319685   \n",
      "7              NaN                NaN       ...                 0.311854   \n",
      "0              NaN                NaN       ...                 0.477591   \n",
      "1              NaN                NaN       ...                 0.477591   \n",
      "2              NaN                NaN       ...                 0.474478   \n",
      "3              NaN                NaN       ...                 0.474478   \n",
      "0              NaN                NaN       ...                 0.346831   \n",
      "1              NaN                NaN       ...                 0.481139   \n",
      "2              NaN                NaN       ...                 0.478163   \n",
      "3              NaN                NaN       ...                 0.478163   \n",
      "0              NaN                NaN       ...                -0.139219   \n",
      "1              NaN                NaN       ...                 0.256755   \n",
      "2              NaN                NaN       ...                 0.327736   \n",
      "3              NaN                NaN       ...                 0.327736   \n",
      "4              NaN                NaN       ...                -0.139219   \n",
      "5              NaN                NaN       ...                 0.126198   \n",
      "..             ...                ...       ...                      ...   \n",
      "190            0.8                NaN       ...                 0.471025   \n",
      "191            0.8                NaN       ...                 0.471025   \n",
      "192            0.8                NaN       ...                 0.471127   \n",
      "193            0.8                NaN       ...                 0.471127   \n",
      "194            0.8                NaN       ...                 0.470886   \n",
      "195            0.8                NaN       ...                 0.470886   \n",
      "196            0.8                NaN       ...                 0.470983   \n",
      "197            0.8                NaN       ...                 0.470983   \n",
      "198              1                NaN       ...                 0.472070   \n",
      "199              1                NaN       ...                 0.472070   \n",
      "200              1                NaN       ...                 0.472649   \n",
      "201              1                NaN       ...                 0.472649   \n",
      "202              1                NaN       ...                 0.472136   \n",
      "203              1                NaN       ...                 0.472136   \n",
      "204              1                NaN       ...                 0.472336   \n",
      "205              1                NaN       ...                 0.472336   \n",
      "206              1                NaN       ...                 0.472139   \n",
      "207              1                NaN       ...                 0.472139   \n",
      "208              1                NaN       ...                 0.472014   \n",
      "209              1                NaN       ...                 0.472014   \n",
      "210              1                NaN       ...                 0.472140   \n",
      "211              1                NaN       ...                 0.472140   \n",
      "212              1                NaN       ...                 0.472050   \n",
      "213              1                NaN       ...                 0.472050   \n",
      "214              1                NaN       ...                 0.472295   \n",
      "215              1                NaN       ...                 0.472295   \n",
      "216              1                NaN       ...                 0.472210   \n",
      "217              1                NaN       ...                 0.472210   \n",
      "218              1                NaN       ...                 0.472141   \n",
      "219              1                NaN       ...                 0.472141   \n",
      "\n",
      "    split0_train_score split1_test_score split1_train_score split2_test_score  \\\n",
      "0             0.900516          0.437757           0.918215          0.559186   \n",
      "1             0.923324          0.405933           0.925264          0.576696   \n",
      "2             0.929035          0.452861           0.933157          0.593257   \n",
      "3             0.925783          0.432066           0.937133          0.599064   \n",
      "4             0.894901          0.397995           0.919887          0.579476   \n",
      "5             0.926134          0.450539           0.935124          0.570797   \n",
      "6             0.930272          0.425251           0.936006          0.583939   \n",
      "7             0.929725          0.390728           0.934790          0.616149   \n",
      "0             0.999999          0.395423           0.999976          0.558876   \n",
      "1             0.999999          0.389036           0.999976          0.555985   \n",
      "2             0.999999          0.397995           0.999976          0.575444   \n",
      "3             0.999999          0.421707           0.999976          0.605744   \n",
      "4             0.999999          0.358684           0.999976          0.581883   \n",
      "5             0.999999          0.448363           0.999976          0.627892   \n",
      "6             0.999999          0.411515           0.999976          0.625488   \n",
      "7             0.999999          0.392534           0.999976          0.606243   \n",
      "0             0.463774          0.293992           0.567092          0.524410   \n",
      "1             0.463774          0.293992           0.567092          0.524410   \n",
      "2             0.463719          0.307132           0.556049          0.523140   \n",
      "3             0.463719          0.307132           0.556049          0.523140   \n",
      "0             0.382781          0.316059           0.465484          0.411636   \n",
      "1             0.463593          0.294810           0.567061          0.527691   \n",
      "2             0.463543          0.308503           0.555975          0.527068   \n",
      "3             0.463543          0.308503           0.555975          0.527068   \n",
      "0             0.000000         -0.032746           0.000000         -0.025697   \n",
      "1             0.292910          0.234169           0.345164          0.293167   \n",
      "2             0.304397          0.233787           0.346858          0.298829   \n",
      "3             0.304397          0.233787           0.346858          0.298829   \n",
      "4             0.000000         -0.032746           0.000000         -0.025697   \n",
      "5             0.204443          0.178140           0.257960          0.203580   \n",
      "..                 ...               ...                ...               ...   \n",
      "190           0.458669          0.316106           0.554669          0.520256   \n",
      "191           0.458669          0.316106           0.554669          0.520256   \n",
      "192           0.458740          0.316106           0.554669          0.520293   \n",
      "193           0.458740          0.316106           0.554669          0.520293   \n",
      "194           0.458573          0.316106           0.554669          0.520501   \n",
      "195           0.458573          0.316106           0.554669          0.520501   \n",
      "196           0.458641          0.316106           0.554669          0.520505   \n",
      "197           0.458641          0.316106           0.554669          0.520505   \n",
      "198           0.459147          0.315525           0.554793          0.525081   \n",
      "199           0.459147          0.315525           0.554793          0.525081   \n",
      "200           0.459541          0.315525           0.554793          0.522696   \n",
      "201           0.459541          0.315525           0.554793          0.522696   \n",
      "202           0.459192          0.315525           0.554793          0.522446   \n",
      "203           0.459192          0.315525           0.554793          0.522446   \n",
      "204           0.459329          0.315525           0.554793          0.522728   \n",
      "205           0.459329          0.315525           0.554793          0.522728   \n",
      "206           0.459194          0.315525           0.554793          0.522591   \n",
      "207           0.459194          0.315525           0.554793          0.522591   \n",
      "208           0.459109          0.315525           0.554793          0.522735   \n",
      "209           0.459109          0.315525           0.554793          0.522735   \n",
      "210           0.459195          0.315525           0.554793          0.522830   \n",
      "211           0.459195          0.315525           0.554793          0.522830   \n",
      "212           0.459133          0.315525           0.554793          0.522738   \n",
      "213           0.459133          0.315525           0.554793          0.522738   \n",
      "214           0.459301          0.315525           0.554793          0.522667   \n",
      "215           0.459301          0.315525           0.554793          0.522667   \n",
      "216           0.459243          0.315525           0.554793          0.522740   \n",
      "217           0.459243          0.315525           0.554793          0.522740   \n",
      "218           0.459196          0.315525           0.554793          0.522683   \n",
      "219           0.459196          0.315525           0.554793          0.522683   \n",
      "\n",
      "     split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
      "0              0.919755      0.000108    1.590706e-04        0.066246   \n",
      "1              0.903296      0.003123    1.511005e-03        0.069603   \n",
      "2              0.907558      0.003846    7.039070e-04        0.063013   \n",
      "3              0.915997      0.000581    2.888158e-04        0.076261   \n",
      "4              0.913330      0.001840    3.442162e-04        0.077556   \n",
      "5              0.913528      0.001926    1.717574e-04        0.058292   \n",
      "6              0.917249      0.000627    4.681239e-04        0.065445   \n",
      "7              0.920845      0.008962    3.297199e-04        0.091917   \n",
      "0              0.999767      0.000651    1.492319e-04        0.113255   \n",
      "1              0.999767      0.001725    1.890016e-03        0.098767   \n",
      "2              0.999767      0.003258    7.548232e-04        0.110435   \n",
      "3              0.999767      0.007825    2.299589e-03        0.118016   \n",
      "4              0.999767      0.002929    9.104882e-04        0.111307   \n",
      "5              0.999767      0.002699    8.946138e-05        0.123283   \n",
      "6              0.999767      0.008569    1.126224e-04        0.128135   \n",
      "7              0.999767      0.002479    1.238335e-03        0.124209   \n",
      "0              0.452518      0.001449    7.148106e-05        0.099298   \n",
      "1              0.452518      0.000012    5.857317e-06        0.099298   \n",
      "2              0.448383      0.000006    0.000000e+00        0.092380   \n",
      "3              0.448383      0.000009    9.602742e-07        0.092380   \n",
      "0              0.381077      0.000077    7.636841e-06        0.039770   \n",
      "1              0.452229      0.000004    5.150430e-07        0.100478   \n",
      "2              0.448002      0.000002    7.786718e-07        0.093522   \n",
      "3              0.448002      0.000006    7.786718e-07        0.093522   \n",
      "0              0.000000      0.000057    6.575629e-06        0.052024   \n",
      "1              0.282289      0.000002    1.189441e-06        0.024263   \n",
      "2              0.290443      0.000005    4.052337e-07        0.039295   \n",
      "3              0.290443      0.000004    4.052337e-07        0.039295   \n",
      "4              0.000000      0.000061    2.051724e-05        0.052024   \n",
      "5              0.199404      0.000135    1.253002e-05        0.032248   \n",
      "..                  ...           ...             ...             ...   \n",
      "190            0.444387      0.000802    3.339752e-06        0.086857   \n",
      "191            0.444387      0.000995    5.619580e-07        0.086857   \n",
      "192            0.444402      0.000730    1.557344e-06        0.086883   \n",
      "193            0.444402      0.004277    1.163357e-04        0.086883   \n",
      "194            0.444487      0.013021    3.759663e-06        0.086917   \n",
      "195            0.444487      0.000460    1.067720e-05        0.086917   \n",
      "196            0.444489      0.002988    1.270323e-05        0.086932   \n",
      "197            0.444489      0.002304    8.396337e-06        0.086932   \n",
      "198            0.446212      0.000191    6.954670e-06        0.088828   \n",
      "199            0.446212      0.001342    2.725351e-06        0.088828   \n",
      "200            0.445497      0.001296    1.106187e-05        0.088131   \n",
      "201            0.445497      0.001494    4.744244e-05        0.088131   \n",
      "202            0.445412      0.002460    4.103448e-06        0.087981   \n",
      "203            0.445412      0.000527    1.458625e-05        0.087981   \n",
      "204            0.445508      0.001665    7.815864e-06        0.088099   \n",
      "205            0.445508      0.002481    7.764787e-06        0.088099   \n",
      "206            0.445462      0.001369    4.673383e-06        0.088028   \n",
      "207            0.445462      0.002177    3.991089e-06        0.088028   \n",
      "208            0.445510      0.001470    1.959614e-06        0.088058   \n",
      "209            0.445510      0.002715    2.418379e-06        0.088058   \n",
      "210            0.445541      0.002535    8.535104e-06        0.088105   \n",
      "211            0.445541      0.002894    1.990410e-05        0.088105   \n",
      "212            0.445511      0.002660    3.351079e-06        0.088063   \n",
      "213            0.445511      0.002314    1.644032e-05        0.088063   \n",
      "214            0.445487      0.001030    8.946964e-06        0.088074   \n",
      "215            0.445487      0.002593    5.025048e-06        0.088074   \n",
      "216            0.445512      0.001843    4.545959e-06        0.088085   \n",
      "217            0.445512      0.003405    8.577180e-06        0.088085   \n",
      "218            0.445493      0.002766    4.497181e-05        0.088058   \n",
      "219            0.445493      0.011721    4.131060e-06        0.088058   \n",
      "\n",
      "     std_train_score  \n",
      "0           0.008729  \n",
      "1           0.009930  \n",
      "2           0.011223  \n",
      "3           0.008637  \n",
      "4           0.010577  \n",
      "5           0.008858  \n",
      "6           0.007848  \n",
      "7           0.005764  \n",
      "0           0.000104  \n",
      "1           0.000104  \n",
      "2           0.000104  \n",
      "3           0.000104  \n",
      "4           0.000104  \n",
      "5           0.000104  \n",
      "6           0.000104  \n",
      "7           0.000104  \n",
      "0           0.051562  \n",
      "1           0.051562  \n",
      "2           0.047554  \n",
      "3           0.047554  \n",
      "0           0.039394  \n",
      "1           0.051662  \n",
      "2           0.047660  \n",
      "3           0.047660  \n",
      "0           0.000000  \n",
      "1           0.027481  \n",
      "2           0.023991  \n",
      "3           0.023991  \n",
      "4           0.000000  \n",
      "5           0.026496  \n",
      "..               ...  \n",
      "190         0.048970  \n",
      "191         0.048970  \n",
      "192         0.048952  \n",
      "193         0.048952  \n",
      "194         0.048959  \n",
      "195         0.048959  \n",
      "196         0.048946  \n",
      "197         0.048946  \n",
      "198         0.048426  \n",
      "199         0.048426  \n",
      "200         0.048552  \n",
      "201         0.048552  \n",
      "202         0.048641  \n",
      "203         0.048641  \n",
      "204         0.048589  \n",
      "205         0.048589  \n",
      "206         0.048627  \n",
      "207         0.048627  \n",
      "208         0.048629  \n",
      "209         0.048629  \n",
      "210         0.048604  \n",
      "211         0.048604  \n",
      "212         0.048624  \n",
      "213         0.048624  \n",
      "214         0.048600  \n",
      "215         0.048600  \n",
      "216         0.048604  \n",
      "217         0.048604  \n",
      "218         0.048618  \n",
      "219         0.048618  \n",
      "\n",
      "[888 rows x 26 columns]\n",
      "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0         0.024165         0.004869         0.467161          0.912829   \n",
      "1         0.063295         0.013676         0.492309          0.917294   \n",
      "2         0.097568         0.019110         0.504288          0.923250   \n",
      "3         0.135895         0.025160         0.491100          0.926304   \n",
      "4         0.052104         0.005416         0.472136          0.909373   \n",
      "5         0.133042         0.012063         0.488230          0.924929   \n",
      "6         0.213503         0.019280         0.497451          0.927842   \n",
      "7         0.297030         0.025394         0.501210          0.928453   \n",
      "0         0.022245         0.004949         0.412033          0.999914   \n",
      "1         0.055298         0.013039         0.421612          0.999914   \n",
      "2         0.090962         0.019441         0.427383          0.999914   \n",
      "3         0.123162         0.027238         0.448949          0.999914   \n",
      "4         0.053856         0.006441         0.424675          0.999914   \n",
      "5         0.131324         0.011887         0.467587          0.999914   \n",
      "6         0.210276         0.018615         0.451756          0.999914   \n",
      "7         0.283644         0.029835         0.436431          0.999914   \n",
      "0         0.001577         0.000264         0.432161          0.494461   \n",
      "1         0.000414         0.000186         0.432161          0.494461   \n",
      "2         0.000347         0.000180         0.435058          0.489384   \n",
      "3         0.000346         0.000180         0.435058          0.489384   \n",
      "0         0.000483         0.000179         0.358135          0.409781   \n",
      "1         0.000383         0.000173         0.434713          0.494294   \n",
      "2         0.000341         0.000176         0.438055          0.489173   \n",
      "3         0.000336         0.000175         0.438055          0.489173   \n",
      "0         0.000454         0.000188        -0.066149          0.000000   \n",
      "1         0.000361         0.000185         0.261347          0.306788   \n",
      "2         0.000422         0.000187         0.286931          0.313899   \n",
      "3         0.000423         0.000188         0.286931          0.313899   \n",
      "4         0.000453         0.000199        -0.066149          0.000000   \n",
      "5         0.000574         0.000238         0.169152          0.220602   \n",
      "..             ...              ...              ...               ...   \n",
      "208       0.054371         0.000200         0.436884          0.486471   \n",
      "209       0.056604         0.000201         0.436884          0.486471   \n",
      "210       0.062589         0.000203         0.436958          0.486510   \n",
      "211       0.064783         0.000215         0.436958          0.486510   \n",
      "212       0.071664         0.000210         0.436897          0.486479   \n",
      "213       0.074883         0.000226         0.436897          0.486479   \n",
      "214       0.081957         0.000217         0.436956          0.486527   \n",
      "215       0.082827         0.000211         0.436956          0.486527   \n",
      "216       0.091498         0.000214         0.436951          0.486516   \n",
      "217       0.088703         0.000218         0.436951          0.486516   \n",
      "218       0.093279         0.000232         0.436909          0.486494   \n",
      "219       0.102064         0.000210         0.436909          0.486494   \n",
      "0         0.001383         0.000207         0.421110          0.471141   \n",
      "1         0.001633         0.000196         0.412548          0.470626   \n",
      "2         0.001971         0.000196         0.402922          0.470759   \n",
      "3         0.002222         0.000199         0.399508          0.468018   \n",
      "4         0.002727         0.000199         0.401767          0.469673   \n",
      "5         0.002784         0.000197         0.403669          0.471291   \n",
      "6         0.002952         0.000196         0.402112          0.470723   \n",
      "7         0.003089         0.000197         0.412737          0.473225   \n",
      "8         0.003215         0.000197         0.404897          0.467959   \n",
      "9         0.003415         0.000197         0.395170          0.473250   \n",
      "10        0.003473         0.000213         0.387430          0.466964   \n",
      "11        0.003575         0.000198         0.384586          0.469372   \n",
      "12        0.003769         0.000200         0.403733          0.477933   \n",
      "13        0.003897         0.000195         0.413751          0.475547   \n",
      "14        0.003986         0.000196         0.403597          0.475052   \n",
      "15        0.004694         0.000283         0.411755          0.458943   \n",
      "16        0.005017         0.000263         0.375363          0.462701   \n",
      "17        0.004650         0.000238         0.427560          0.476736   \n",
      "\n",
      "     method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0     [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1     [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2     [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3     [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0     [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1     [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2     [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3     [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0     [0, 0, 6]     NaN           1             NaN                True   \n",
      "1     [0, 0, 6]     NaN           1             NaN                True   \n",
      "2     [0, 0, 6]     NaN           1             NaN               False   \n",
      "3     [0, 0, 6]     NaN           1             NaN               False   \n",
      "4     [0, 0, 6]     NaN           2             NaN                True   \n",
      "5     [0, 0, 6]     NaN           2             NaN                True   \n",
      "..          ...     ...         ...             ...                 ...   \n",
      "208  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "209  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "210  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "211  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "212  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "213  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "214  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "215  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "216  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "217  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "218  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "219  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 12]     0.1         NaN             NaN                 NaN   \n",
      "1    [0, 0, 12]    0.15         NaN             NaN                 NaN   \n",
      "2    [0, 0, 12]     0.2         NaN             NaN                 NaN   \n",
      "3    [0, 0, 12]    0.25         NaN             NaN                 NaN   \n",
      "4    [0, 0, 12]     0.3         NaN             NaN                 NaN   \n",
      "5    [0, 0, 12]    0.35         NaN             NaN                 NaN   \n",
      "6    [0, 0, 12]     0.4         NaN             NaN                 NaN   \n",
      "7    [0, 0, 12]    0.45         NaN             NaN                 NaN   \n",
      "8    [0, 0, 12]     0.5         NaN             NaN                 NaN   \n",
      "9    [0, 0, 12]    0.55         NaN             NaN                 NaN   \n",
      "10   [0, 0, 12]     0.6         NaN             NaN                 NaN   \n",
      "11   [0, 0, 12]    0.65         NaN             NaN                 NaN   \n",
      "12   [0, 0, 12]     0.7         NaN             NaN                 NaN   \n",
      "13   [0, 0, 12]    0.75         NaN             NaN                 NaN   \n",
      "14   [0, 0, 12]     0.8         NaN             NaN                 NaN   \n",
      "15   [0, 0, 12]    0.85         NaN             NaN                 NaN   \n",
      "16   [0, 0, 12]     0.9         NaN             NaN                 NaN   \n",
      "17   [0, 0, 12]    0.95         NaN             NaN                 NaN   \n",
      "\n",
      "    param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0              NaN       ...                 0.405206           0.900516   \n",
      "1              NaN       ...                 0.494276           0.923324   \n",
      "2              NaN       ...                 0.467146           0.929035   \n",
      "3              NaN       ...                 0.442689           0.925783   \n",
      "4              NaN       ...                 0.439291           0.894901   \n",
      "5              NaN       ...                 0.443830           0.926134   \n",
      "6              NaN       ...                 0.483314           0.930272   \n",
      "7              NaN       ...                 0.496800           0.929725   \n",
      "0              NaN       ...                 0.283188           0.999999   \n",
      "1              NaN       ...                 0.320898           0.999999   \n",
      "2              NaN       ...                 0.309973           0.999999   \n",
      "3              NaN       ...                 0.320773           0.999999   \n",
      "4              NaN       ...                 0.334430           0.999999   \n",
      "5              NaN       ...                 0.328006           0.999999   \n",
      "6              NaN       ...                 0.319685           0.999999   \n",
      "7              NaN       ...                 0.311854           0.999999   \n",
      "0              NaN       ...                 0.477591           0.463774   \n",
      "1              NaN       ...                 0.477591           0.463774   \n",
      "2              NaN       ...                 0.474478           0.463719   \n",
      "3              NaN       ...                 0.474478           0.463719   \n",
      "0              NaN       ...                 0.346831           0.382781   \n",
      "1              NaN       ...                 0.481139           0.463593   \n",
      "2              NaN       ...                 0.478163           0.463543   \n",
      "3              NaN       ...                 0.478163           0.463543   \n",
      "0              NaN       ...                -0.139219           0.000000   \n",
      "1              NaN       ...                 0.256755           0.292910   \n",
      "2              NaN       ...                 0.327736           0.304397   \n",
      "3              NaN       ...                 0.327736           0.304397   \n",
      "4              NaN       ...                -0.139219           0.000000   \n",
      "5              NaN       ...                 0.126198           0.204443   \n",
      "..             ...       ...                      ...                ...   \n",
      "208              1       ...                 0.472014           0.459109   \n",
      "209              1       ...                 0.472014           0.459109   \n",
      "210              1       ...                 0.472140           0.459195   \n",
      "211              1       ...                 0.472140           0.459195   \n",
      "212              1       ...                 0.472050           0.459133   \n",
      "213              1       ...                 0.472050           0.459133   \n",
      "214              1       ...                 0.472295           0.459301   \n",
      "215              1       ...                 0.472295           0.459301   \n",
      "216              1       ...                 0.472210           0.459243   \n",
      "217              1       ...                 0.472210           0.459243   \n",
      "218              1       ...                 0.472141           0.459196   \n",
      "219              1       ...                 0.472141           0.459196   \n",
      "0              NaN       ...                 0.430416           0.420739   \n",
      "1              NaN       ...                 0.426334           0.421160   \n",
      "2              NaN       ...                 0.428890           0.421248   \n",
      "3              NaN       ...                 0.424852           0.423594   \n",
      "4              NaN       ...                 0.422066           0.422271   \n",
      "5              NaN       ...                 0.438977           0.425143   \n",
      "6              NaN       ...                 0.447264           0.423504   \n",
      "7              NaN       ...                 0.455550           0.433231   \n",
      "8              NaN       ...                 0.450244           0.429696   \n",
      "9              NaN       ...                 0.416073           0.432025   \n",
      "10             NaN       ...                 0.383885           0.417086   \n",
      "11             NaN       ...                 0.428562           0.439883   \n",
      "12             NaN       ...                 0.474578           0.445554   \n",
      "13             NaN       ...                 0.460773           0.444344   \n",
      "14             NaN       ...                 0.432984           0.434689   \n",
      "15             NaN       ...                 0.436425           0.427840   \n",
      "16             NaN       ...                 0.372872           0.404919   \n",
      "17             NaN       ...                 0.478121           0.442253   \n",
      "\n",
      "    split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0            0.437757           0.918215          0.559186           0.919755   \n",
      "1            0.405933           0.925264          0.576696           0.903296   \n",
      "2            0.452861           0.933157          0.593257           0.907558   \n",
      "3            0.432066           0.937133          0.599064           0.915997   \n",
      "4            0.397995           0.919887          0.579476           0.913330   \n",
      "5            0.450539           0.935124          0.570797           0.913528   \n",
      "6            0.425251           0.936006          0.583939           0.917249   \n",
      "7            0.390728           0.934790          0.616149           0.920845   \n",
      "0            0.395423           0.999976          0.558876           0.999767   \n",
      "1            0.389036           0.999976          0.555985           0.999767   \n",
      "2            0.397995           0.999976          0.575444           0.999767   \n",
      "3            0.421707           0.999976          0.605744           0.999767   \n",
      "4            0.358684           0.999976          0.581883           0.999767   \n",
      "5            0.448363           0.999976          0.627892           0.999767   \n",
      "6            0.411515           0.999976          0.625488           0.999767   \n",
      "7            0.392534           0.999976          0.606243           0.999767   \n",
      "0            0.293992           0.567092          0.524410           0.452518   \n",
      "1            0.293992           0.567092          0.524410           0.452518   \n",
      "2            0.307132           0.556049          0.523140           0.448383   \n",
      "3            0.307132           0.556049          0.523140           0.448383   \n",
      "0            0.316059           0.465484          0.411636           0.381077   \n",
      "1            0.294810           0.567061          0.527691           0.452229   \n",
      "2            0.308503           0.555975          0.527068           0.448002   \n",
      "3            0.308503           0.555975          0.527068           0.448002   \n",
      "0           -0.032746           0.000000         -0.025697           0.000000   \n",
      "1            0.234169           0.345164          0.293167           0.282289   \n",
      "2            0.233787           0.346858          0.298829           0.290443   \n",
      "3            0.233787           0.346858          0.298829           0.290443   \n",
      "4           -0.032746           0.000000         -0.025697           0.000000   \n",
      "5            0.178140           0.257960          0.203580           0.199404   \n",
      "..                ...                ...               ...                ...   \n",
      "208          0.315525           0.554793          0.522735           0.445510   \n",
      "209          0.315525           0.554793          0.522735           0.445510   \n",
      "210          0.315525           0.554793          0.522830           0.445541   \n",
      "211          0.315525           0.554793          0.522830           0.445541   \n",
      "212          0.315525           0.554793          0.522738           0.445511   \n",
      "213          0.315525           0.554793          0.522738           0.445511   \n",
      "214          0.315525           0.554793          0.522667           0.445487   \n",
      "215          0.315525           0.554793          0.522667           0.445487   \n",
      "216          0.315525           0.554793          0.522740           0.445512   \n",
      "217          0.315525           0.554793          0.522740           0.445512   \n",
      "218          0.315525           0.554793          0.522683           0.445493   \n",
      "219          0.315525           0.554793          0.522683           0.445493   \n",
      "0            0.323560           0.556280          0.509254           0.436405   \n",
      "1            0.299646           0.553150          0.511515           0.437567   \n",
      "2            0.277491           0.552655          0.502106           0.438374   \n",
      "3            0.275063           0.549845          0.498338           0.430616   \n",
      "4            0.267493           0.549262          0.515525           0.437488   \n",
      "5            0.259471           0.547958          0.512178           0.440771   \n",
      "6            0.262739           0.548221          0.495848           0.440443   \n",
      "7            0.262935           0.544917          0.519265           0.441527   \n",
      "8            0.268747           0.536105          0.495214           0.438075   \n",
      "9            0.237335           0.548862          0.531876           0.438862   \n",
      "10           0.252377           0.544865          0.526066           0.438941   \n",
      "11           0.222890           0.536913          0.501834           0.431321   \n",
      "12           0.240942           0.546829          0.494918           0.441416   \n",
      "13           0.266815           0.536029          0.513159           0.446267   \n",
      "14           0.262770           0.544459          0.514721           0.446006   \n",
      "15           0.271270           0.501898          0.527307           0.447091   \n",
      "16           0.273006           0.549342          0.480239           0.433841   \n",
      "17           0.280917           0.546973          0.523099           0.440983   \n",
      "\n",
      "     std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0        0.000108    1.590706e-04        0.066246         0.008729  \n",
      "1        0.003123    1.511005e-03        0.069603         0.009930  \n",
      "2        0.003846    7.039070e-04        0.063013         0.011223  \n",
      "3        0.000581    2.888158e-04        0.076261         0.008637  \n",
      "4        0.001840    3.442162e-04        0.077556         0.010577  \n",
      "5        0.001926    1.717574e-04        0.058292         0.008858  \n",
      "6        0.000627    4.681239e-04        0.065445         0.007848  \n",
      "7        0.008962    3.297199e-04        0.091917         0.005764  \n",
      "0        0.000651    1.492319e-04        0.113255         0.000104  \n",
      "1        0.001725    1.890016e-03        0.098767         0.000104  \n",
      "2        0.003258    7.548232e-04        0.110435         0.000104  \n",
      "3        0.007825    2.299589e-03        0.118016         0.000104  \n",
      "4        0.002929    9.104882e-04        0.111307         0.000104  \n",
      "5        0.002699    8.946138e-05        0.123283         0.000104  \n",
      "6        0.008569    1.126224e-04        0.128135         0.000104  \n",
      "7        0.002479    1.238335e-03        0.124209         0.000104  \n",
      "0        0.001449    7.148106e-05        0.099298         0.051562  \n",
      "1        0.000012    5.857317e-06        0.099298         0.051562  \n",
      "2        0.000006    0.000000e+00        0.092380         0.047554  \n",
      "3        0.000009    9.602742e-07        0.092380         0.047554  \n",
      "0        0.000077    7.636841e-06        0.039770         0.039394  \n",
      "1        0.000004    5.150430e-07        0.100478         0.051662  \n",
      "2        0.000002    7.786718e-07        0.093522         0.047660  \n",
      "3        0.000006    7.786718e-07        0.093522         0.047660  \n",
      "0        0.000057    6.575629e-06        0.052024         0.000000  \n",
      "1        0.000002    1.189441e-06        0.024263         0.027481  \n",
      "2        0.000005    4.052337e-07        0.039295         0.023991  \n",
      "3        0.000004    4.052337e-07        0.039295         0.023991  \n",
      "4        0.000061    2.051724e-05        0.052024         0.000000  \n",
      "5        0.000135    1.253002e-05        0.032248         0.026496  \n",
      "..            ...             ...             ...              ...  \n",
      "208      0.001470    1.959614e-06        0.088058         0.048629  \n",
      "209      0.002715    2.418379e-06        0.088058         0.048629  \n",
      "210      0.002535    8.535104e-06        0.088105         0.048604  \n",
      "211      0.002894    1.990410e-05        0.088105         0.048604  \n",
      "212      0.002660    3.351079e-06        0.088063         0.048624  \n",
      "213      0.002314    1.644032e-05        0.088063         0.048624  \n",
      "214      0.001030    8.946964e-06        0.088074         0.048600  \n",
      "215      0.002593    5.025048e-06        0.088074         0.048600  \n",
      "216      0.001843    4.545959e-06        0.088085         0.048604  \n",
      "217      0.003405    8.577180e-06        0.088085         0.048604  \n",
      "218      0.002766    4.497181e-05        0.088058         0.048618  \n",
      "219      0.011721    4.131060e-06        0.088058         0.048618  \n",
      "0        0.000192    1.342550e-05        0.075962         0.060541  \n",
      "1        0.000136    4.495664e-07        0.086895         0.058737  \n",
      "2        0.000059    5.150430e-07        0.093378         0.058330  \n",
      "3        0.000041    5.673271e-06        0.092755         0.057931  \n",
      "4        0.000182    2.170642e-06        0.102103         0.056619  \n",
      "5        0.000076    4.495664e-07        0.105998         0.054586  \n",
      "6        0.000034    7.786718e-07        0.100273         0.055234  \n",
      "7        0.000091    1.700793e-06        0.108803         0.050807  \n",
      "8        0.000117    4.495664e-07        0.097758         0.048308  \n",
      "9        0.000073    2.685665e-06        0.120947         0.053539  \n",
      "10       0.000129    2.281882e-05        0.111562         0.055802  \n",
      "11       0.000090    1.620935e-06        0.117895         0.047886  \n",
      "12       0.000040    5.170013e-06        0.115103         0.048746  \n",
      "13       0.000007    7.786718e-07        0.105809         0.042775  \n",
      "14       0.000065    4.495664e-07        0.104779         0.049296  \n",
      "15       0.000479    9.876792e-05        0.105803         0.031374  \n",
      "16       0.000695    5.045031e-05        0.084470         0.062392  \n",
      "17       0.000260    3.228927e-05        0.105035         0.049667  \n",
      "\n",
      "[906 rows x 27 columns]\n",
      "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0         0.024165         0.004869         0.467161          0.912829   \n",
      "1         0.063295         0.013676         0.492309          0.917294   \n",
      "2         0.097568         0.019110         0.504288          0.923250   \n",
      "3         0.135895         0.025160         0.491100          0.926304   \n",
      "4         0.052104         0.005416         0.472136          0.909373   \n",
      "5         0.133042         0.012063         0.488230          0.924929   \n",
      "6         0.213503         0.019280         0.497451          0.927842   \n",
      "7         0.297030         0.025394         0.501210          0.928453   \n",
      "0         0.022245         0.004949         0.412033          0.999914   \n",
      "1         0.055298         0.013039         0.421612          0.999914   \n",
      "2         0.090962         0.019441         0.427383          0.999914   \n",
      "3         0.123162         0.027238         0.448949          0.999914   \n",
      "4         0.053856         0.006441         0.424675          0.999914   \n",
      "5         0.131324         0.011887         0.467587          0.999914   \n",
      "6         0.210276         0.018615         0.451756          0.999914   \n",
      "7         0.283644         0.029835         0.436431          0.999914   \n",
      "0         0.001577         0.000264         0.432161          0.494461   \n",
      "1         0.000414         0.000186         0.432161          0.494461   \n",
      "2         0.000347         0.000180         0.435058          0.489384   \n",
      "3         0.000346         0.000180         0.435058          0.489384   \n",
      "0         0.000483         0.000179         0.358135          0.409781   \n",
      "1         0.000383         0.000173         0.434713          0.494294   \n",
      "2         0.000341         0.000176         0.438055          0.489173   \n",
      "3         0.000336         0.000175         0.438055          0.489173   \n",
      "0         0.000454         0.000188        -0.066149          0.000000   \n",
      "1         0.000361         0.000185         0.261347          0.306788   \n",
      "2         0.000422         0.000187         0.286931          0.313899   \n",
      "3         0.000423         0.000188         0.286931          0.313899   \n",
      "4         0.000453         0.000199        -0.066149          0.000000   \n",
      "5         0.000574         0.000238         0.169152          0.220602   \n",
      "..             ...              ...              ...               ...   \n",
      "216       0.091498         0.000214         0.436951          0.486516   \n",
      "217       0.088703         0.000218         0.436951          0.486516   \n",
      "218       0.093279         0.000232         0.436909          0.486494   \n",
      "219       0.102064         0.000210         0.436909          0.486494   \n",
      "0         0.001383         0.000207         0.421110          0.471141   \n",
      "1         0.001633         0.000196         0.412548          0.470626   \n",
      "2         0.001971         0.000196         0.402922          0.470759   \n",
      "3         0.002222         0.000199         0.399508          0.468018   \n",
      "4         0.002727         0.000199         0.401767          0.469673   \n",
      "5         0.002784         0.000197         0.403669          0.471291   \n",
      "6         0.002952         0.000196         0.402112          0.470723   \n",
      "7         0.003089         0.000197         0.412737          0.473225   \n",
      "8         0.003215         0.000197         0.404897          0.467959   \n",
      "9         0.003415         0.000197         0.395170          0.473250   \n",
      "10        0.003473         0.000213         0.387430          0.466964   \n",
      "11        0.003575         0.000198         0.384586          0.469372   \n",
      "12        0.003769         0.000200         0.403733          0.477933   \n",
      "13        0.003897         0.000195         0.413751          0.475547   \n",
      "14        0.003986         0.000196         0.403597          0.475052   \n",
      "15        0.004694         0.000283         0.411755          0.458943   \n",
      "16        0.005017         0.000263         0.375363          0.462701   \n",
      "17        0.004650         0.000238         0.427560          0.476736   \n",
      "0         0.024528         0.005535         0.386108          0.891667   \n",
      "1         0.064410         0.011874         0.411488          0.912841   \n",
      "2         0.102493         0.019003         0.419298          0.918743   \n",
      "3         0.133738         0.026777         0.444938          0.915177   \n",
      "4         0.051778         0.004877         0.427890          0.897547   \n",
      "5         0.131375         0.011934         0.459303          0.910718   \n",
      "6         0.206936         0.018571         0.438172          0.920084   \n",
      "7         0.289170         0.025996         0.444466          0.924568   \n",
      "\n",
      "     method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3     [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7     [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3     [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7     [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0     [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1     [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2     [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3     [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0     [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1     [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2     [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3     [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0     [0, 0, 6]     NaN           1             NaN                True   \n",
      "1     [0, 0, 6]     NaN           1             NaN                True   \n",
      "2     [0, 0, 6]     NaN           1             NaN               False   \n",
      "3     [0, 0, 6]     NaN           1             NaN               False   \n",
      "4     [0, 0, 6]     NaN           2             NaN                True   \n",
      "5     [0, 0, 6]     NaN           2             NaN                True   \n",
      "..          ...     ...         ...             ...                 ...   \n",
      "216  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "217  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "218  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "219  [0, 0, 11]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 12]     0.1         NaN             NaN                 NaN   \n",
      "1    [0, 0, 12]    0.15         NaN             NaN                 NaN   \n",
      "2    [0, 0, 12]     0.2         NaN             NaN                 NaN   \n",
      "3    [0, 0, 12]    0.25         NaN             NaN                 NaN   \n",
      "4    [0, 0, 12]     0.3         NaN             NaN                 NaN   \n",
      "5    [0, 0, 12]    0.35         NaN             NaN                 NaN   \n",
      "6    [0, 0, 12]     0.4         NaN             NaN                 NaN   \n",
      "7    [0, 0, 12]    0.45         NaN             NaN                 NaN   \n",
      "8    [0, 0, 12]     0.5         NaN             NaN                 NaN   \n",
      "9    [0, 0, 12]    0.55         NaN             NaN                 NaN   \n",
      "10   [0, 0, 12]     0.6         NaN             NaN                 NaN   \n",
      "11   [0, 0, 12]    0.65         NaN             NaN                 NaN   \n",
      "12   [0, 0, 12]     0.7         NaN             NaN                 NaN   \n",
      "13   [0, 0, 12]    0.75         NaN             NaN                 NaN   \n",
      "14   [0, 0, 12]     0.8         NaN             NaN                 NaN   \n",
      "15   [0, 0, 12]    0.85         NaN             NaN                 NaN   \n",
      "16   [0, 0, 12]     0.9         NaN             NaN                 NaN   \n",
      "17   [0, 0, 12]    0.95         NaN             NaN                 NaN   \n",
      "0     [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "1     [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "2     [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "3     [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "4     [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "5     [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "6     [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "7     [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "\n",
      "    param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0              NaN       ...                 0.405206           0.900516   \n",
      "1              NaN       ...                 0.494276           0.923324   \n",
      "2              NaN       ...                 0.467146           0.929035   \n",
      "3              NaN       ...                 0.442689           0.925783   \n",
      "4              NaN       ...                 0.439291           0.894901   \n",
      "5              NaN       ...                 0.443830           0.926134   \n",
      "6              NaN       ...                 0.483314           0.930272   \n",
      "7              NaN       ...                 0.496800           0.929725   \n",
      "0              NaN       ...                 0.283188           0.999999   \n",
      "1              NaN       ...                 0.320898           0.999999   \n",
      "2              NaN       ...                 0.309973           0.999999   \n",
      "3              NaN       ...                 0.320773           0.999999   \n",
      "4              NaN       ...                 0.334430           0.999999   \n",
      "5              NaN       ...                 0.328006           0.999999   \n",
      "6              NaN       ...                 0.319685           0.999999   \n",
      "7              NaN       ...                 0.311854           0.999999   \n",
      "0              NaN       ...                 0.477591           0.463774   \n",
      "1              NaN       ...                 0.477591           0.463774   \n",
      "2              NaN       ...                 0.474478           0.463719   \n",
      "3              NaN       ...                 0.474478           0.463719   \n",
      "0              NaN       ...                 0.346831           0.382781   \n",
      "1              NaN       ...                 0.481139           0.463593   \n",
      "2              NaN       ...                 0.478163           0.463543   \n",
      "3              NaN       ...                 0.478163           0.463543   \n",
      "0              NaN       ...                -0.139219           0.000000   \n",
      "1              NaN       ...                 0.256755           0.292910   \n",
      "2              NaN       ...                 0.327736           0.304397   \n",
      "3              NaN       ...                 0.327736           0.304397   \n",
      "4              NaN       ...                -0.139219           0.000000   \n",
      "5              NaN       ...                 0.126198           0.204443   \n",
      "..             ...       ...                      ...                ...   \n",
      "216              1       ...                 0.472210           0.459243   \n",
      "217              1       ...                 0.472210           0.459243   \n",
      "218              1       ...                 0.472141           0.459196   \n",
      "219              1       ...                 0.472141           0.459196   \n",
      "0              NaN       ...                 0.430416           0.420739   \n",
      "1              NaN       ...                 0.426334           0.421160   \n",
      "2              NaN       ...                 0.428890           0.421248   \n",
      "3              NaN       ...                 0.424852           0.423594   \n",
      "4              NaN       ...                 0.422066           0.422271   \n",
      "5              NaN       ...                 0.438977           0.425143   \n",
      "6              NaN       ...                 0.447264           0.423504   \n",
      "7              NaN       ...                 0.455550           0.433231   \n",
      "8              NaN       ...                 0.450244           0.429696   \n",
      "9              NaN       ...                 0.416073           0.432025   \n",
      "10             NaN       ...                 0.383885           0.417086   \n",
      "11             NaN       ...                 0.428562           0.439883   \n",
      "12             NaN       ...                 0.474578           0.445554   \n",
      "13             NaN       ...                 0.460773           0.444344   \n",
      "14             NaN       ...                 0.432984           0.434689   \n",
      "15             NaN       ...                 0.436425           0.427840   \n",
      "16             NaN       ...                 0.372872           0.404919   \n",
      "17             NaN       ...                 0.478121           0.442253   \n",
      "0              NaN       ...                 0.249945           0.901259   \n",
      "1              NaN       ...                 0.298759           0.926966   \n",
      "2              NaN       ...                 0.322480           0.929455   \n",
      "3              NaN       ...                 0.345600           0.921293   \n",
      "4              NaN       ...                 0.378931           0.895731   \n",
      "5              NaN       ...                 0.366067           0.913207   \n",
      "6              NaN       ...                 0.358965           0.918982   \n",
      "7              NaN       ...                 0.349797           0.933519   \n",
      "\n",
      "    split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0            0.437757           0.918215          0.559186           0.919755   \n",
      "1            0.405933           0.925264          0.576696           0.903296   \n",
      "2            0.452861           0.933157          0.593257           0.907558   \n",
      "3            0.432066           0.937133          0.599064           0.915997   \n",
      "4            0.397995           0.919887          0.579476           0.913330   \n",
      "5            0.450539           0.935124          0.570797           0.913528   \n",
      "6            0.425251           0.936006          0.583939           0.917249   \n",
      "7            0.390728           0.934790          0.616149           0.920845   \n",
      "0            0.395423           0.999976          0.558876           0.999767   \n",
      "1            0.389036           0.999976          0.555985           0.999767   \n",
      "2            0.397995           0.999976          0.575444           0.999767   \n",
      "3            0.421707           0.999976          0.605744           0.999767   \n",
      "4            0.358684           0.999976          0.581883           0.999767   \n",
      "5            0.448363           0.999976          0.627892           0.999767   \n",
      "6            0.411515           0.999976          0.625488           0.999767   \n",
      "7            0.392534           0.999976          0.606243           0.999767   \n",
      "0            0.293992           0.567092          0.524410           0.452518   \n",
      "1            0.293992           0.567092          0.524410           0.452518   \n",
      "2            0.307132           0.556049          0.523140           0.448383   \n",
      "3            0.307132           0.556049          0.523140           0.448383   \n",
      "0            0.316059           0.465484          0.411636           0.381077   \n",
      "1            0.294810           0.567061          0.527691           0.452229   \n",
      "2            0.308503           0.555975          0.527068           0.448002   \n",
      "3            0.308503           0.555975          0.527068           0.448002   \n",
      "0           -0.032746           0.000000         -0.025697           0.000000   \n",
      "1            0.234169           0.345164          0.293167           0.282289   \n",
      "2            0.233787           0.346858          0.298829           0.290443   \n",
      "3            0.233787           0.346858          0.298829           0.290443   \n",
      "4           -0.032746           0.000000         -0.025697           0.000000   \n",
      "5            0.178140           0.257960          0.203580           0.199404   \n",
      "..                ...                ...               ...                ...   \n",
      "216          0.315525           0.554793          0.522740           0.445512   \n",
      "217          0.315525           0.554793          0.522740           0.445512   \n",
      "218          0.315525           0.554793          0.522683           0.445493   \n",
      "219          0.315525           0.554793          0.522683           0.445493   \n",
      "0            0.323560           0.556280          0.509254           0.436405   \n",
      "1            0.299646           0.553150          0.511515           0.437567   \n",
      "2            0.277491           0.552655          0.502106           0.438374   \n",
      "3            0.275063           0.549845          0.498338           0.430616   \n",
      "4            0.267493           0.549262          0.515525           0.437488   \n",
      "5            0.259471           0.547958          0.512178           0.440771   \n",
      "6            0.262739           0.548221          0.495848           0.440443   \n",
      "7            0.262935           0.544917          0.519265           0.441527   \n",
      "8            0.268747           0.536105          0.495214           0.438075   \n",
      "9            0.237335           0.548862          0.531876           0.438862   \n",
      "10           0.252377           0.544865          0.526066           0.438941   \n",
      "11           0.222890           0.536913          0.501834           0.431321   \n",
      "12           0.240942           0.546829          0.494918           0.441416   \n",
      "13           0.266815           0.536029          0.513159           0.446267   \n",
      "14           0.262770           0.544459          0.514721           0.446006   \n",
      "15           0.271270           0.501898          0.527307           0.447091   \n",
      "16           0.273006           0.549342          0.480239           0.433841   \n",
      "17           0.280917           0.546973          0.523099           0.440983   \n",
      "0            0.363762           0.892187          0.546082           0.881554   \n",
      "1            0.343352           0.912603          0.593565           0.898954   \n",
      "2            0.420893           0.919988          0.515562           0.906785   \n",
      "3            0.414799           0.917573          0.575484           0.906665   \n",
      "4            0.398372           0.900096          0.506894           0.896812   \n",
      "5            0.459015           0.913909          0.553828           0.905039   \n",
      "6            0.438007           0.923851          0.518395           0.917418   \n",
      "7            0.419239           0.922288          0.565382           0.917897   \n",
      "\n",
      "     std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0        0.000108    1.590706e-04        0.066246         0.008729  \n",
      "1        0.003123    1.511005e-03        0.069603         0.009930  \n",
      "2        0.003846    7.039070e-04        0.063013         0.011223  \n",
      "3        0.000581    2.888158e-04        0.076261         0.008637  \n",
      "4        0.001840    3.442162e-04        0.077556         0.010577  \n",
      "5        0.001926    1.717574e-04        0.058292         0.008858  \n",
      "6        0.000627    4.681239e-04        0.065445         0.007848  \n",
      "7        0.008962    3.297199e-04        0.091917         0.005764  \n",
      "0        0.000651    1.492319e-04        0.113255         0.000104  \n",
      "1        0.001725    1.890016e-03        0.098767         0.000104  \n",
      "2        0.003258    7.548232e-04        0.110435         0.000104  \n",
      "3        0.007825    2.299589e-03        0.118016         0.000104  \n",
      "4        0.002929    9.104882e-04        0.111307         0.000104  \n",
      "5        0.002699    8.946138e-05        0.123283         0.000104  \n",
      "6        0.008569    1.126224e-04        0.128135         0.000104  \n",
      "7        0.002479    1.238335e-03        0.124209         0.000104  \n",
      "0        0.001449    7.148106e-05        0.099298         0.051562  \n",
      "1        0.000012    5.857317e-06        0.099298         0.051562  \n",
      "2        0.000006    0.000000e+00        0.092380         0.047554  \n",
      "3        0.000009    9.602742e-07        0.092380         0.047554  \n",
      "0        0.000077    7.636841e-06        0.039770         0.039394  \n",
      "1        0.000004    5.150430e-07        0.100478         0.051662  \n",
      "2        0.000002    7.786718e-07        0.093522         0.047660  \n",
      "3        0.000006    7.786718e-07        0.093522         0.047660  \n",
      "0        0.000057    6.575629e-06        0.052024         0.000000  \n",
      "1        0.000002    1.189441e-06        0.024263         0.027481  \n",
      "2        0.000005    4.052337e-07        0.039295         0.023991  \n",
      "3        0.000004    4.052337e-07        0.039295         0.023991  \n",
      "4        0.000061    2.051724e-05        0.052024         0.000000  \n",
      "5        0.000135    1.253002e-05        0.032248         0.026496  \n",
      "..            ...             ...             ...              ...  \n",
      "216      0.001843    4.545959e-06        0.088085         0.048604  \n",
      "217      0.003405    8.577180e-06        0.088085         0.048604  \n",
      "218      0.002766    4.497181e-05        0.088058         0.048618  \n",
      "219      0.011721    4.131060e-06        0.088058         0.048618  \n",
      "0        0.000192    1.342550e-05        0.075962         0.060541  \n",
      "1        0.000136    4.495664e-07        0.086895         0.058737  \n",
      "2        0.000059    5.150430e-07        0.093378         0.058330  \n",
      "3        0.000041    5.673271e-06        0.092755         0.057931  \n",
      "4        0.000182    2.170642e-06        0.102103         0.056619  \n",
      "5        0.000076    4.495664e-07        0.105998         0.054586  \n",
      "6        0.000034    7.786718e-07        0.100273         0.055234  \n",
      "7        0.000091    1.700793e-06        0.108803         0.050807  \n",
      "8        0.000117    4.495664e-07        0.097758         0.048308  \n",
      "9        0.000073    2.685665e-06        0.120947         0.053539  \n",
      "10       0.000129    2.281882e-05        0.111562         0.055802  \n",
      "11       0.000090    1.620935e-06        0.117895         0.047886  \n",
      "12       0.000040    5.170013e-06        0.115103         0.048746  \n",
      "13       0.000007    7.786718e-07        0.105809         0.042775  \n",
      "14       0.000065    4.495664e-07        0.104779         0.049296  \n",
      "15       0.000479    9.876792e-05        0.105803         0.031374  \n",
      "16       0.000695    5.045031e-05        0.084470         0.062392  \n",
      "17       0.000260    3.228927e-05        0.105035         0.049667  \n",
      "0        0.000329    1.021421e-03        0.122025         0.008053  \n",
      "1        0.005248    6.018053e-05        0.129689         0.011437  \n",
      "2        0.004745    3.540041e-04        0.078903         0.009297  \n",
      "3        0.000828    1.949261e-03        0.096307         0.006207  \n",
      "4        0.001086    8.092499e-05        0.056278         0.001856  \n",
      "5        0.002834    2.007287e-04        0.076721         0.004026  \n",
      "6        0.001478    2.855920e-04        0.065144         0.002739  \n",
      "7        0.006004    7.027910e-04        0.089868         0.006578  \n",
      "\n",
      "[914 rows x 27 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "4        0.002727         0.000199         0.401767          0.469673   \n",
      "5        0.002784         0.000197         0.403669          0.471291   \n",
      "6        0.002952         0.000196         0.402112          0.470723   \n",
      "7        0.003089         0.000197         0.412737          0.473225   \n",
      "8        0.003215         0.000197         0.404897          0.467959   \n",
      "9        0.003415         0.000197         0.395170          0.473250   \n",
      "10       0.003473         0.000213         0.387430          0.466964   \n",
      "11       0.003575         0.000198         0.384586          0.469372   \n",
      "12       0.003769         0.000200         0.403733          0.477933   \n",
      "13       0.003897         0.000195         0.413751          0.475547   \n",
      "14       0.003986         0.000196         0.403597          0.475052   \n",
      "15       0.004694         0.000283         0.411755          0.458943   \n",
      "16       0.005017         0.000263         0.375363          0.462701   \n",
      "17       0.004650         0.000238         0.427560          0.476736   \n",
      "0        0.024528         0.005535         0.386108          0.891667   \n",
      "1        0.064410         0.011874         0.411488          0.912841   \n",
      "2        0.102493         0.019003         0.419298          0.918743   \n",
      "3        0.133738         0.026777         0.444938          0.915177   \n",
      "4        0.051778         0.004877         0.427890          0.897547   \n",
      "5        0.131375         0.011934         0.459303          0.910718   \n",
      "6        0.206936         0.018571         0.438172          0.920084   \n",
      "7        0.289170         0.025996         0.444466          0.924568   \n",
      "0        0.022605         0.004982         0.422705          0.999991   \n",
      "1        0.057766         0.013530         0.387522          0.999991   \n",
      "2        0.091327         0.018997         0.393227          0.999991   \n",
      "3        0.127417         0.032315         0.416780          0.999991   \n",
      "4        0.055872         0.005513         0.369715          0.999991   \n",
      "5        0.140371         0.014914         0.408119          0.999991   \n",
      "6        0.220550         0.019577         0.416559          0.999991   \n",
      "7        0.308043         0.027769         0.413040          0.999991   \n",
      "\n",
      "    method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 6]     NaN           1             NaN                True   \n",
      "1    [0, 0, 6]     NaN           1             NaN                True   \n",
      "2    [0, 0, 6]     NaN           1             NaN               False   \n",
      "3    [0, 0, 6]     NaN           1             NaN               False   \n",
      "4    [0, 0, 6]     NaN           2             NaN                True   \n",
      "5    [0, 0, 6]     NaN           2             NaN                True   \n",
      "..         ...     ...         ...             ...                 ...   \n",
      "4   [0, 0, 12]     0.3         NaN             NaN                 NaN   \n",
      "5   [0, 0, 12]    0.35         NaN             NaN                 NaN   \n",
      "6   [0, 0, 12]     0.4         NaN             NaN                 NaN   \n",
      "7   [0, 0, 12]    0.45         NaN             NaN                 NaN   \n",
      "8   [0, 0, 12]     0.5         NaN             NaN                 NaN   \n",
      "9   [0, 0, 12]    0.55         NaN             NaN                 NaN   \n",
      "10  [0, 0, 12]     0.6         NaN             NaN                 NaN   \n",
      "11  [0, 0, 12]    0.65         NaN             NaN                 NaN   \n",
      "12  [0, 0, 12]     0.7         NaN             NaN                 NaN   \n",
      "13  [0, 0, 12]    0.75         NaN             NaN                 NaN   \n",
      "14  [0, 0, 12]     0.8         NaN             NaN                 NaN   \n",
      "15  [0, 0, 12]    0.85         NaN             NaN                 NaN   \n",
      "16  [0, 0, 12]     0.9         NaN             NaN                 NaN   \n",
      "17  [0, 0, 12]    0.95         NaN             NaN                 NaN   \n",
      "0    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "\n",
      "   param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0             NaN       ...                 0.405206           0.900516   \n",
      "1             NaN       ...                 0.494276           0.923324   \n",
      "2             NaN       ...                 0.467146           0.929035   \n",
      "3             NaN       ...                 0.442689           0.925783   \n",
      "4             NaN       ...                 0.439291           0.894901   \n",
      "5             NaN       ...                 0.443830           0.926134   \n",
      "6             NaN       ...                 0.483314           0.930272   \n",
      "7             NaN       ...                 0.496800           0.929725   \n",
      "0             NaN       ...                 0.283188           0.999999   \n",
      "1             NaN       ...                 0.320898           0.999999   \n",
      "2             NaN       ...                 0.309973           0.999999   \n",
      "3             NaN       ...                 0.320773           0.999999   \n",
      "4             NaN       ...                 0.334430           0.999999   \n",
      "5             NaN       ...                 0.328006           0.999999   \n",
      "6             NaN       ...                 0.319685           0.999999   \n",
      "7             NaN       ...                 0.311854           0.999999   \n",
      "0             NaN       ...                 0.477591           0.463774   \n",
      "1             NaN       ...                 0.477591           0.463774   \n",
      "2             NaN       ...                 0.474478           0.463719   \n",
      "3             NaN       ...                 0.474478           0.463719   \n",
      "0             NaN       ...                 0.346831           0.382781   \n",
      "1             NaN       ...                 0.481139           0.463593   \n",
      "2             NaN       ...                 0.478163           0.463543   \n",
      "3             NaN       ...                 0.478163           0.463543   \n",
      "0             NaN       ...                -0.139219           0.000000   \n",
      "1             NaN       ...                 0.256755           0.292910   \n",
      "2             NaN       ...                 0.327736           0.304397   \n",
      "3             NaN       ...                 0.327736           0.304397   \n",
      "4             NaN       ...                -0.139219           0.000000   \n",
      "5             NaN       ...                 0.126198           0.204443   \n",
      "..            ...       ...                      ...                ...   \n",
      "4             NaN       ...                 0.422066           0.422271   \n",
      "5             NaN       ...                 0.438977           0.425143   \n",
      "6             NaN       ...                 0.447264           0.423504   \n",
      "7             NaN       ...                 0.455550           0.433231   \n",
      "8             NaN       ...                 0.450244           0.429696   \n",
      "9             NaN       ...                 0.416073           0.432025   \n",
      "10            NaN       ...                 0.383885           0.417086   \n",
      "11            NaN       ...                 0.428562           0.439883   \n",
      "12            NaN       ...                 0.474578           0.445554   \n",
      "13            NaN       ...                 0.460773           0.444344   \n",
      "14            NaN       ...                 0.432984           0.434689   \n",
      "15            NaN       ...                 0.436425           0.427840   \n",
      "16            NaN       ...                 0.372872           0.404919   \n",
      "17            NaN       ...                 0.478121           0.442253   \n",
      "0             NaN       ...                 0.249945           0.901259   \n",
      "1             NaN       ...                 0.298759           0.926966   \n",
      "2             NaN       ...                 0.322480           0.929455   \n",
      "3             NaN       ...                 0.345600           0.921293   \n",
      "4             NaN       ...                 0.378931           0.895731   \n",
      "5             NaN       ...                 0.366067           0.913207   \n",
      "6             NaN       ...                 0.358965           0.918982   \n",
      "7             NaN       ...                 0.349797           0.933519   \n",
      "0             NaN       ...                 0.295707           0.999999   \n",
      "1             NaN       ...                 0.278782           0.999999   \n",
      "2             NaN       ...                 0.262381           0.999999   \n",
      "3             NaN       ...                 0.294472           0.999999   \n",
      "4             NaN       ...                 0.244110           0.999999   \n",
      "5             NaN       ...                 0.268920           0.999999   \n",
      "6             NaN       ...                 0.268551           0.999999   \n",
      "7             NaN       ...                 0.268896           0.999999   \n",
      "\n",
      "   split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0           0.437757           0.918215          0.559186           0.919755   \n",
      "1           0.405933           0.925264          0.576696           0.903296   \n",
      "2           0.452861           0.933157          0.593257           0.907558   \n",
      "3           0.432066           0.937133          0.599064           0.915997   \n",
      "4           0.397995           0.919887          0.579476           0.913330   \n",
      "5           0.450539           0.935124          0.570797           0.913528   \n",
      "6           0.425251           0.936006          0.583939           0.917249   \n",
      "7           0.390728           0.934790          0.616149           0.920845   \n",
      "0           0.395423           0.999976          0.558876           0.999767   \n",
      "1           0.389036           0.999976          0.555985           0.999767   \n",
      "2           0.397995           0.999976          0.575444           0.999767   \n",
      "3           0.421707           0.999976          0.605744           0.999767   \n",
      "4           0.358684           0.999976          0.581883           0.999767   \n",
      "5           0.448363           0.999976          0.627892           0.999767   \n",
      "6           0.411515           0.999976          0.625488           0.999767   \n",
      "7           0.392534           0.999976          0.606243           0.999767   \n",
      "0           0.293992           0.567092          0.524410           0.452518   \n",
      "1           0.293992           0.567092          0.524410           0.452518   \n",
      "2           0.307132           0.556049          0.523140           0.448383   \n",
      "3           0.307132           0.556049          0.523140           0.448383   \n",
      "0           0.316059           0.465484          0.411636           0.381077   \n",
      "1           0.294810           0.567061          0.527691           0.452229   \n",
      "2           0.308503           0.555975          0.527068           0.448002   \n",
      "3           0.308503           0.555975          0.527068           0.448002   \n",
      "0          -0.032746           0.000000         -0.025697           0.000000   \n",
      "1           0.234169           0.345164          0.293167           0.282289   \n",
      "2           0.233787           0.346858          0.298829           0.290443   \n",
      "3           0.233787           0.346858          0.298829           0.290443   \n",
      "4          -0.032746           0.000000         -0.025697           0.000000   \n",
      "5           0.178140           0.257960          0.203580           0.199404   \n",
      "..               ...                ...               ...                ...   \n",
      "4           0.267493           0.549262          0.515525           0.437488   \n",
      "5           0.259471           0.547958          0.512178           0.440771   \n",
      "6           0.262739           0.548221          0.495848           0.440443   \n",
      "7           0.262935           0.544917          0.519265           0.441527   \n",
      "8           0.268747           0.536105          0.495214           0.438075   \n",
      "9           0.237335           0.548862          0.531876           0.438862   \n",
      "10          0.252377           0.544865          0.526066           0.438941   \n",
      "11          0.222890           0.536913          0.501834           0.431321   \n",
      "12          0.240942           0.546829          0.494918           0.441416   \n",
      "13          0.266815           0.536029          0.513159           0.446267   \n",
      "14          0.262770           0.544459          0.514721           0.446006   \n",
      "15          0.271270           0.501898          0.527307           0.447091   \n",
      "16          0.273006           0.549342          0.480239           0.433841   \n",
      "17          0.280917           0.546973          0.523099           0.440983   \n",
      "0           0.363762           0.892187          0.546082           0.881554   \n",
      "1           0.343352           0.912603          0.593565           0.898954   \n",
      "2           0.420893           0.919988          0.515562           0.906785   \n",
      "3           0.414799           0.917573          0.575484           0.906665   \n",
      "4           0.398372           0.900096          0.506894           0.896812   \n",
      "5           0.459015           0.913909          0.553828           0.905039   \n",
      "6           0.438007           0.923851          0.518395           0.917418   \n",
      "7           0.419239           0.922288          0.565382           0.917897   \n",
      "0           0.461337           0.999976          0.512436           0.999999   \n",
      "1           0.373238           0.999976          0.511713           0.999999   \n",
      "2           0.432841           0.999976          0.485867           0.999999   \n",
      "3           0.426488           0.999976          0.530695           0.999999   \n",
      "4           0.362474           0.999976          0.503912           0.999999   \n",
      "5           0.397706           0.999976          0.559226           0.999999   \n",
      "6           0.419472           0.999976          0.563246           0.999999   \n",
      "7           0.420156           0.999976          0.551619           0.999999   \n",
      "\n",
      "    std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0       0.000108    1.590706e-04        0.066246         0.008729  \n",
      "1       0.003123    1.511005e-03        0.069603         0.009930  \n",
      "2       0.003846    7.039070e-04        0.063013         0.011223  \n",
      "3       0.000581    2.888158e-04        0.076261         0.008637  \n",
      "4       0.001840    3.442162e-04        0.077556         0.010577  \n",
      "5       0.001926    1.717574e-04        0.058292         0.008858  \n",
      "6       0.000627    4.681239e-04        0.065445         0.007848  \n",
      "7       0.008962    3.297199e-04        0.091917         0.005764  \n",
      "0       0.000651    1.492319e-04        0.113255         0.000104  \n",
      "1       0.001725    1.890016e-03        0.098767         0.000104  \n",
      "2       0.003258    7.548232e-04        0.110435         0.000104  \n",
      "3       0.007825    2.299589e-03        0.118016         0.000104  \n",
      "4       0.002929    9.104882e-04        0.111307         0.000104  \n",
      "5       0.002699    8.946138e-05        0.123283         0.000104  \n",
      "6       0.008569    1.126224e-04        0.128135         0.000104  \n",
      "7       0.002479    1.238335e-03        0.124209         0.000104  \n",
      "0       0.001449    7.148106e-05        0.099298         0.051562  \n",
      "1       0.000012    5.857317e-06        0.099298         0.051562  \n",
      "2       0.000006    0.000000e+00        0.092380         0.047554  \n",
      "3       0.000009    9.602742e-07        0.092380         0.047554  \n",
      "0       0.000077    7.636841e-06        0.039770         0.039394  \n",
      "1       0.000004    5.150430e-07        0.100478         0.051662  \n",
      "2       0.000002    7.786718e-07        0.093522         0.047660  \n",
      "3       0.000006    7.786718e-07        0.093522         0.047660  \n",
      "0       0.000057    6.575629e-06        0.052024         0.000000  \n",
      "1       0.000002    1.189441e-06        0.024263         0.027481  \n",
      "2       0.000005    4.052337e-07        0.039295         0.023991  \n",
      "3       0.000004    4.052337e-07        0.039295         0.023991  \n",
      "4       0.000061    2.051724e-05        0.052024         0.000000  \n",
      "5       0.000135    1.253002e-05        0.032248         0.026496  \n",
      "..           ...             ...             ...              ...  \n",
      "4       0.000182    2.170642e-06        0.102103         0.056619  \n",
      "5       0.000076    4.495664e-07        0.105998         0.054586  \n",
      "6       0.000034    7.786718e-07        0.100273         0.055234  \n",
      "7       0.000091    1.700793e-06        0.108803         0.050807  \n",
      "8       0.000117    4.495664e-07        0.097758         0.048308  \n",
      "9       0.000073    2.685665e-06        0.120947         0.053539  \n",
      "10      0.000129    2.281882e-05        0.111562         0.055802  \n",
      "11      0.000090    1.620935e-06        0.117895         0.047886  \n",
      "12      0.000040    5.170013e-06        0.115103         0.048746  \n",
      "13      0.000007    7.786718e-07        0.105809         0.042775  \n",
      "14      0.000065    4.495664e-07        0.104779         0.049296  \n",
      "15      0.000479    9.876792e-05        0.105803         0.031374  \n",
      "16      0.000695    5.045031e-05        0.084470         0.062392  \n",
      "17      0.000260    3.228927e-05        0.105035         0.049667  \n",
      "0       0.000329    1.021421e-03        0.122025         0.008053  \n",
      "1       0.005248    6.018053e-05        0.129689         0.011437  \n",
      "2       0.004745    3.540041e-04        0.078903         0.009297  \n",
      "3       0.000828    1.949261e-03        0.096307         0.006207  \n",
      "4       0.001086    8.092499e-05        0.056278         0.001856  \n",
      "5       0.002834    2.007287e-04        0.076721         0.004026  \n",
      "6       0.001478    2.855920e-04        0.065144         0.002739  \n",
      "7       0.006004    7.027910e-04        0.089868         0.006578  \n",
      "0       0.001078    9.592259e-05        0.092653         0.000011  \n",
      "1       0.002636    1.307711e-03        0.095709         0.000011  \n",
      "2       0.002295    3.706593e-04        0.095495         0.000011  \n",
      "3       0.003828    1.655535e-03        0.096765         0.000011  \n",
      "4       0.003500    8.892264e-05        0.106280         0.000011  \n",
      "5       0.006008    7.830758e-04        0.118848         0.000011  \n",
      "6       0.016258    5.475426e-04        0.120432         0.000011  \n",
      "7       0.011855    1.834935e-03        0.115632         0.000011  \n",
      "\n",
      "[922 rows x 27 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "8        0.003215         0.000197         0.404897          0.467959   \n",
      "9        0.003415         0.000197         0.395170          0.473250   \n",
      "10       0.003473         0.000213         0.387430          0.466964   \n",
      "11       0.003575         0.000198         0.384586          0.469372   \n",
      "12       0.003769         0.000200         0.403733          0.477933   \n",
      "13       0.003897         0.000195         0.413751          0.475547   \n",
      "14       0.003986         0.000196         0.403597          0.475052   \n",
      "15       0.004694         0.000283         0.411755          0.458943   \n",
      "16       0.005017         0.000263         0.375363          0.462701   \n",
      "17       0.004650         0.000238         0.427560          0.476736   \n",
      "0        0.024528         0.005535         0.386108          0.891667   \n",
      "1        0.064410         0.011874         0.411488          0.912841   \n",
      "2        0.102493         0.019003         0.419298          0.918743   \n",
      "3        0.133738         0.026777         0.444938          0.915177   \n",
      "4        0.051778         0.004877         0.427890          0.897547   \n",
      "5        0.131375         0.011934         0.459303          0.910718   \n",
      "6        0.206936         0.018571         0.438172          0.920084   \n",
      "7        0.289170         0.025996         0.444466          0.924568   \n",
      "0        0.022605         0.004982         0.422705          0.999991   \n",
      "1        0.057766         0.013530         0.387522          0.999991   \n",
      "2        0.091327         0.018997         0.393227          0.999991   \n",
      "3        0.127417         0.032315         0.416780          0.999991   \n",
      "4        0.055872         0.005513         0.369715          0.999991   \n",
      "5        0.140371         0.014914         0.408119          0.999991   \n",
      "6        0.220550         0.019577         0.416559          0.999991   \n",
      "7        0.308043         0.027769         0.413040          0.999991   \n",
      "0        0.000539         0.000190         0.382487          0.471654   \n",
      "1        0.000395         0.000182         0.382487          0.471654   \n",
      "2        0.000366         0.000183         0.362164          0.458363   \n",
      "3        0.000354         0.000182         0.362164          0.458363   \n",
      "\n",
      "    method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 6]     NaN           1             NaN                True   \n",
      "1    [0, 0, 6]     NaN           1             NaN                True   \n",
      "2    [0, 0, 6]     NaN           1             NaN               False   \n",
      "3    [0, 0, 6]     NaN           1             NaN               False   \n",
      "4    [0, 0, 6]     NaN           2             NaN                True   \n",
      "5    [0, 0, 6]     NaN           2             NaN                True   \n",
      "..         ...     ...         ...             ...                 ...   \n",
      "8   [0, 0, 12]     0.5         NaN             NaN                 NaN   \n",
      "9   [0, 0, 12]    0.55         NaN             NaN                 NaN   \n",
      "10  [0, 0, 12]     0.6         NaN             NaN                 NaN   \n",
      "11  [0, 0, 12]    0.65         NaN             NaN                 NaN   \n",
      "12  [0, 0, 12]     0.7         NaN             NaN                 NaN   \n",
      "13  [0, 0, 12]    0.75         NaN             NaN                 NaN   \n",
      "14  [0, 0, 12]     0.8         NaN             NaN                 NaN   \n",
      "15  [0, 0, 12]    0.85         NaN             NaN                 NaN   \n",
      "16  [0, 0, 12]     0.9         NaN             NaN                 NaN   \n",
      "17  [0, 0, 12]    0.95         NaN             NaN                 NaN   \n",
      "0    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 1, 3]     NaN         NaN             NaN                True   \n",
      "1    [0, 1, 3]     NaN         NaN             NaN                True   \n",
      "2    [0, 1, 3]     NaN         NaN             NaN               False   \n",
      "3    [0, 1, 3]     NaN         NaN             NaN               False   \n",
      "\n",
      "   param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0             NaN       ...                 0.405206           0.900516   \n",
      "1             NaN       ...                 0.494276           0.923324   \n",
      "2             NaN       ...                 0.467146           0.929035   \n",
      "3             NaN       ...                 0.442689           0.925783   \n",
      "4             NaN       ...                 0.439291           0.894901   \n",
      "5             NaN       ...                 0.443830           0.926134   \n",
      "6             NaN       ...                 0.483314           0.930272   \n",
      "7             NaN       ...                 0.496800           0.929725   \n",
      "0             NaN       ...                 0.283188           0.999999   \n",
      "1             NaN       ...                 0.320898           0.999999   \n",
      "2             NaN       ...                 0.309973           0.999999   \n",
      "3             NaN       ...                 0.320773           0.999999   \n",
      "4             NaN       ...                 0.334430           0.999999   \n",
      "5             NaN       ...                 0.328006           0.999999   \n",
      "6             NaN       ...                 0.319685           0.999999   \n",
      "7             NaN       ...                 0.311854           0.999999   \n",
      "0             NaN       ...                 0.477591           0.463774   \n",
      "1             NaN       ...                 0.477591           0.463774   \n",
      "2             NaN       ...                 0.474478           0.463719   \n",
      "3             NaN       ...                 0.474478           0.463719   \n",
      "0             NaN       ...                 0.346831           0.382781   \n",
      "1             NaN       ...                 0.481139           0.463593   \n",
      "2             NaN       ...                 0.478163           0.463543   \n",
      "3             NaN       ...                 0.478163           0.463543   \n",
      "0             NaN       ...                -0.139219           0.000000   \n",
      "1             NaN       ...                 0.256755           0.292910   \n",
      "2             NaN       ...                 0.327736           0.304397   \n",
      "3             NaN       ...                 0.327736           0.304397   \n",
      "4             NaN       ...                -0.139219           0.000000   \n",
      "5             NaN       ...                 0.126198           0.204443   \n",
      "..            ...       ...                      ...                ...   \n",
      "8             NaN       ...                 0.450244           0.429696   \n",
      "9             NaN       ...                 0.416073           0.432025   \n",
      "10            NaN       ...                 0.383885           0.417086   \n",
      "11            NaN       ...                 0.428562           0.439883   \n",
      "12            NaN       ...                 0.474578           0.445554   \n",
      "13            NaN       ...                 0.460773           0.444344   \n",
      "14            NaN       ...                 0.432984           0.434689   \n",
      "15            NaN       ...                 0.436425           0.427840   \n",
      "16            NaN       ...                 0.372872           0.404919   \n",
      "17            NaN       ...                 0.478121           0.442253   \n",
      "0             NaN       ...                 0.249945           0.901259   \n",
      "1             NaN       ...                 0.298759           0.926966   \n",
      "2             NaN       ...                 0.322480           0.929455   \n",
      "3             NaN       ...                 0.345600           0.921293   \n",
      "4             NaN       ...                 0.378931           0.895731   \n",
      "5             NaN       ...                 0.366067           0.913207   \n",
      "6             NaN       ...                 0.358965           0.918982   \n",
      "7             NaN       ...                 0.349797           0.933519   \n",
      "0             NaN       ...                 0.295707           0.999999   \n",
      "1             NaN       ...                 0.278782           0.999999   \n",
      "2             NaN       ...                 0.262381           0.999999   \n",
      "3             NaN       ...                 0.294472           0.999999   \n",
      "4             NaN       ...                 0.244110           0.999999   \n",
      "5             NaN       ...                 0.268920           0.999999   \n",
      "6             NaN       ...                 0.268551           0.999999   \n",
      "7             NaN       ...                 0.268896           0.999999   \n",
      "0             NaN       ...                 0.418737           0.439281   \n",
      "1             NaN       ...                 0.418737           0.439281   \n",
      "2             NaN       ...                 0.392938           0.427893   \n",
      "3             NaN       ...                 0.392938           0.427893   \n",
      "\n",
      "   split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0           0.437757           0.918215          0.559186           0.919755   \n",
      "1           0.405933           0.925264          0.576696           0.903296   \n",
      "2           0.452861           0.933157          0.593257           0.907558   \n",
      "3           0.432066           0.937133          0.599064           0.915997   \n",
      "4           0.397995           0.919887          0.579476           0.913330   \n",
      "5           0.450539           0.935124          0.570797           0.913528   \n",
      "6           0.425251           0.936006          0.583939           0.917249   \n",
      "7           0.390728           0.934790          0.616149           0.920845   \n",
      "0           0.395423           0.999976          0.558876           0.999767   \n",
      "1           0.389036           0.999976          0.555985           0.999767   \n",
      "2           0.397995           0.999976          0.575444           0.999767   \n",
      "3           0.421707           0.999976          0.605744           0.999767   \n",
      "4           0.358684           0.999976          0.581883           0.999767   \n",
      "5           0.448363           0.999976          0.627892           0.999767   \n",
      "6           0.411515           0.999976          0.625488           0.999767   \n",
      "7           0.392534           0.999976          0.606243           0.999767   \n",
      "0           0.293992           0.567092          0.524410           0.452518   \n",
      "1           0.293992           0.567092          0.524410           0.452518   \n",
      "2           0.307132           0.556049          0.523140           0.448383   \n",
      "3           0.307132           0.556049          0.523140           0.448383   \n",
      "0           0.316059           0.465484          0.411636           0.381077   \n",
      "1           0.294810           0.567061          0.527691           0.452229   \n",
      "2           0.308503           0.555975          0.527068           0.448002   \n",
      "3           0.308503           0.555975          0.527068           0.448002   \n",
      "0          -0.032746           0.000000         -0.025697           0.000000   \n",
      "1           0.234169           0.345164          0.293167           0.282289   \n",
      "2           0.233787           0.346858          0.298829           0.290443   \n",
      "3           0.233787           0.346858          0.298829           0.290443   \n",
      "4          -0.032746           0.000000         -0.025697           0.000000   \n",
      "5           0.178140           0.257960          0.203580           0.199404   \n",
      "..               ...                ...               ...                ...   \n",
      "8           0.268747           0.536105          0.495214           0.438075   \n",
      "9           0.237335           0.548862          0.531876           0.438862   \n",
      "10          0.252377           0.544865          0.526066           0.438941   \n",
      "11          0.222890           0.536913          0.501834           0.431321   \n",
      "12          0.240942           0.546829          0.494918           0.441416   \n",
      "13          0.266815           0.536029          0.513159           0.446267   \n",
      "14          0.262770           0.544459          0.514721           0.446006   \n",
      "15          0.271270           0.501898          0.527307           0.447091   \n",
      "16          0.273006           0.549342          0.480239           0.433841   \n",
      "17          0.280917           0.546973          0.523099           0.440983   \n",
      "0           0.363762           0.892187          0.546082           0.881554   \n",
      "1           0.343352           0.912603          0.593565           0.898954   \n",
      "2           0.420893           0.919988          0.515562           0.906785   \n",
      "3           0.414799           0.917573          0.575484           0.906665   \n",
      "4           0.398372           0.900096          0.506894           0.896812   \n",
      "5           0.459015           0.913909          0.553828           0.905039   \n",
      "6           0.438007           0.923851          0.518395           0.917418   \n",
      "7           0.419239           0.922288          0.565382           0.917897   \n",
      "0           0.461337           0.999976          0.512436           0.999999   \n",
      "1           0.373238           0.999976          0.511713           0.999999   \n",
      "2           0.432841           0.999976          0.485867           0.999999   \n",
      "3           0.426488           0.999976          0.530695           0.999999   \n",
      "4           0.362474           0.999976          0.503912           0.999999   \n",
      "5           0.397706           0.999976          0.559226           0.999999   \n",
      "6           0.419472           0.999976          0.563246           0.999999   \n",
      "7           0.420156           0.999976          0.551619           0.999999   \n",
      "0           0.271300           0.539604          0.457032           0.436078   \n",
      "1           0.271300           0.539604          0.457032           0.436078   \n",
      "2           0.259509           0.521040          0.433714           0.426155   \n",
      "3           0.259509           0.521040          0.433714           0.426155   \n",
      "\n",
      "    std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0       0.000108    1.590706e-04        0.066246         0.008729  \n",
      "1       0.003123    1.511005e-03        0.069603         0.009930  \n",
      "2       0.003846    7.039070e-04        0.063013         0.011223  \n",
      "3       0.000581    2.888158e-04        0.076261         0.008637  \n",
      "4       0.001840    3.442162e-04        0.077556         0.010577  \n",
      "5       0.001926    1.717574e-04        0.058292         0.008858  \n",
      "6       0.000627    4.681239e-04        0.065445         0.007848  \n",
      "7       0.008962    3.297199e-04        0.091917         0.005764  \n",
      "0       0.000651    1.492319e-04        0.113255         0.000104  \n",
      "1       0.001725    1.890016e-03        0.098767         0.000104  \n",
      "2       0.003258    7.548232e-04        0.110435         0.000104  \n",
      "3       0.007825    2.299589e-03        0.118016         0.000104  \n",
      "4       0.002929    9.104882e-04        0.111307         0.000104  \n",
      "5       0.002699    8.946138e-05        0.123283         0.000104  \n",
      "6       0.008569    1.126224e-04        0.128135         0.000104  \n",
      "7       0.002479    1.238335e-03        0.124209         0.000104  \n",
      "0       0.001449    7.148106e-05        0.099298         0.051562  \n",
      "1       0.000012    5.857317e-06        0.099298         0.051562  \n",
      "2       0.000006    0.000000e+00        0.092380         0.047554  \n",
      "3       0.000009    9.602742e-07        0.092380         0.047554  \n",
      "0       0.000077    7.636841e-06        0.039770         0.039394  \n",
      "1       0.000004    5.150430e-07        0.100478         0.051662  \n",
      "2       0.000002    7.786718e-07        0.093522         0.047660  \n",
      "3       0.000006    7.786718e-07        0.093522         0.047660  \n",
      "0       0.000057    6.575629e-06        0.052024         0.000000  \n",
      "1       0.000002    1.189441e-06        0.024263         0.027481  \n",
      "2       0.000005    4.052337e-07        0.039295         0.023991  \n",
      "3       0.000004    4.052337e-07        0.039295         0.023991  \n",
      "4       0.000061    2.051724e-05        0.052024         0.000000  \n",
      "5       0.000135    1.253002e-05        0.032248         0.026496  \n",
      "..           ...             ...             ...              ...  \n",
      "8       0.000117    4.495664e-07        0.097758         0.048308  \n",
      "9       0.000073    2.685665e-06        0.120947         0.053539  \n",
      "10      0.000129    2.281882e-05        0.111562         0.055802  \n",
      "11      0.000090    1.620935e-06        0.117895         0.047886  \n",
      "12      0.000040    5.170013e-06        0.115103         0.048746  \n",
      "13      0.000007    7.786718e-07        0.105809         0.042775  \n",
      "14      0.000065    4.495664e-07        0.104779         0.049296  \n",
      "15      0.000479    9.876792e-05        0.105803         0.031374  \n",
      "16      0.000695    5.045031e-05        0.084470         0.062392  \n",
      "17      0.000260    3.228927e-05        0.105035         0.049667  \n",
      "0       0.000329    1.021421e-03        0.122025         0.008053  \n",
      "1       0.005248    6.018053e-05        0.129689         0.011437  \n",
      "2       0.004745    3.540041e-04        0.078903         0.009297  \n",
      "3       0.000828    1.949261e-03        0.096307         0.006207  \n",
      "4       0.001086    8.092499e-05        0.056278         0.001856  \n",
      "5       0.002834    2.007287e-04        0.076721         0.004026  \n",
      "6       0.001478    2.855920e-04        0.065144         0.002739  \n",
      "7       0.006004    7.027910e-04        0.089868         0.006578  \n",
      "0       0.001078    9.592259e-05        0.092653         0.000011  \n",
      "1       0.002636    1.307711e-03        0.095709         0.000011  \n",
      "2       0.002295    3.706593e-04        0.095495         0.000011  \n",
      "3       0.003828    1.655535e-03        0.096765         0.000011  \n",
      "4       0.003500    8.892264e-05        0.106280         0.000011  \n",
      "5       0.006008    7.830758e-04        0.118848         0.000011  \n",
      "6       0.016258    5.475426e-04        0.120432         0.000011  \n",
      "7       0.011855    1.834935e-03        0.115632         0.000011  \n",
      "0       0.000096    9.224331e-06        0.079956         0.048065  \n",
      "1       0.000007    1.655632e-06        0.079956         0.048065  \n",
      "2       0.000013    2.050954e-06        0.074286         0.044325  \n",
      "3       0.000001    8.778064e-07        0.074286         0.044325  \n",
      "\n",
      "[926 rows x 27 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "12       0.003769         0.000200         0.403733          0.477933   \n",
      "13       0.003897         0.000195         0.413751          0.475547   \n",
      "14       0.003986         0.000196         0.403597          0.475052   \n",
      "15       0.004694         0.000283         0.411755          0.458943   \n",
      "16       0.005017         0.000263         0.375363          0.462701   \n",
      "17       0.004650         0.000238         0.427560          0.476736   \n",
      "0        0.024528         0.005535         0.386108          0.891667   \n",
      "1        0.064410         0.011874         0.411488          0.912841   \n",
      "2        0.102493         0.019003         0.419298          0.918743   \n",
      "3        0.133738         0.026777         0.444938          0.915177   \n",
      "4        0.051778         0.004877         0.427890          0.897547   \n",
      "5        0.131375         0.011934         0.459303          0.910718   \n",
      "6        0.206936         0.018571         0.438172          0.920084   \n",
      "7        0.289170         0.025996         0.444466          0.924568   \n",
      "0        0.022605         0.004982         0.422705          0.999991   \n",
      "1        0.057766         0.013530         0.387522          0.999991   \n",
      "2        0.091327         0.018997         0.393227          0.999991   \n",
      "3        0.127417         0.032315         0.416780          0.999991   \n",
      "4        0.055872         0.005513         0.369715          0.999991   \n",
      "5        0.140371         0.014914         0.408119          0.999991   \n",
      "6        0.220550         0.019577         0.416559          0.999991   \n",
      "7        0.308043         0.027769         0.413040          0.999991   \n",
      "0        0.000539         0.000190         0.382487          0.471654   \n",
      "1        0.000395         0.000182         0.382487          0.471654   \n",
      "2        0.000366         0.000183         0.362164          0.458363   \n",
      "3        0.000354         0.000182         0.362164          0.458363   \n",
      "0        0.000609         0.000203         0.386295          0.435334   \n",
      "1        0.000400         0.000174         0.389866          0.471150   \n",
      "2        0.000387         0.000184         0.369650          0.457944   \n",
      "3        0.000368         0.000187         0.369650          0.457944   \n",
      "\n",
      "    method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1    [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3    [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0    [0, 0, 6]     NaN           1             NaN                True   \n",
      "1    [0, 0, 6]     NaN           1             NaN                True   \n",
      "2    [0, 0, 6]     NaN           1             NaN               False   \n",
      "3    [0, 0, 6]     NaN           1             NaN               False   \n",
      "4    [0, 0, 6]     NaN           2             NaN                True   \n",
      "5    [0, 0, 6]     NaN           2             NaN                True   \n",
      "..         ...     ...         ...             ...                 ...   \n",
      "12  [0, 0, 12]     0.7         NaN             NaN                 NaN   \n",
      "13  [0, 0, 12]    0.75         NaN             NaN                 NaN   \n",
      "14  [0, 0, 12]     0.8         NaN             NaN                 NaN   \n",
      "15  [0, 0, 12]    0.85         NaN             NaN                 NaN   \n",
      "16  [0, 0, 12]     0.9         NaN             NaN                 NaN   \n",
      "17  [0, 0, 12]    0.95         NaN             NaN                 NaN   \n",
      "0    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 1]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 1]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "1    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "2    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "3    [0, 1, 2]     NaN         NaN             mse                 NaN   \n",
      "4    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "5    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "6    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "7    [0, 1, 2]     NaN         NaN             mae                 NaN   \n",
      "0    [0, 1, 3]     NaN         NaN             NaN                True   \n",
      "1    [0, 1, 3]     NaN         NaN             NaN                True   \n",
      "2    [0, 1, 3]     NaN         NaN             NaN               False   \n",
      "3    [0, 1, 3]     NaN         NaN             NaN               False   \n",
      "0    [0, 1, 4]     NaN         NaN             NaN                True   \n",
      "1    [0, 1, 4]     NaN         NaN             NaN                True   \n",
      "2    [0, 1, 4]     NaN         NaN             NaN               False   \n",
      "3    [0, 1, 4]     NaN         NaN             NaN               False   \n",
      "\n",
      "   param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0             NaN       ...                 0.405206           0.900516   \n",
      "1             NaN       ...                 0.494276           0.923324   \n",
      "2             NaN       ...                 0.467146           0.929035   \n",
      "3             NaN       ...                 0.442689           0.925783   \n",
      "4             NaN       ...                 0.439291           0.894901   \n",
      "5             NaN       ...                 0.443830           0.926134   \n",
      "6             NaN       ...                 0.483314           0.930272   \n",
      "7             NaN       ...                 0.496800           0.929725   \n",
      "0             NaN       ...                 0.283188           0.999999   \n",
      "1             NaN       ...                 0.320898           0.999999   \n",
      "2             NaN       ...                 0.309973           0.999999   \n",
      "3             NaN       ...                 0.320773           0.999999   \n",
      "4             NaN       ...                 0.334430           0.999999   \n",
      "5             NaN       ...                 0.328006           0.999999   \n",
      "6             NaN       ...                 0.319685           0.999999   \n",
      "7             NaN       ...                 0.311854           0.999999   \n",
      "0             NaN       ...                 0.477591           0.463774   \n",
      "1             NaN       ...                 0.477591           0.463774   \n",
      "2             NaN       ...                 0.474478           0.463719   \n",
      "3             NaN       ...                 0.474478           0.463719   \n",
      "0             NaN       ...                 0.346831           0.382781   \n",
      "1             NaN       ...                 0.481139           0.463593   \n",
      "2             NaN       ...                 0.478163           0.463543   \n",
      "3             NaN       ...                 0.478163           0.463543   \n",
      "0             NaN       ...                -0.139219           0.000000   \n",
      "1             NaN       ...                 0.256755           0.292910   \n",
      "2             NaN       ...                 0.327736           0.304397   \n",
      "3             NaN       ...                 0.327736           0.304397   \n",
      "4             NaN       ...                -0.139219           0.000000   \n",
      "5             NaN       ...                 0.126198           0.204443   \n",
      "..            ...       ...                      ...                ...   \n",
      "12            NaN       ...                 0.474578           0.445554   \n",
      "13            NaN       ...                 0.460773           0.444344   \n",
      "14            NaN       ...                 0.432984           0.434689   \n",
      "15            NaN       ...                 0.436425           0.427840   \n",
      "16            NaN       ...                 0.372872           0.404919   \n",
      "17            NaN       ...                 0.478121           0.442253   \n",
      "0             NaN       ...                 0.249945           0.901259   \n",
      "1             NaN       ...                 0.298759           0.926966   \n",
      "2             NaN       ...                 0.322480           0.929455   \n",
      "3             NaN       ...                 0.345600           0.921293   \n",
      "4             NaN       ...                 0.378931           0.895731   \n",
      "5             NaN       ...                 0.366067           0.913207   \n",
      "6             NaN       ...                 0.358965           0.918982   \n",
      "7             NaN       ...                 0.349797           0.933519   \n",
      "0             NaN       ...                 0.295707           0.999999   \n",
      "1             NaN       ...                 0.278782           0.999999   \n",
      "2             NaN       ...                 0.262381           0.999999   \n",
      "3             NaN       ...                 0.294472           0.999999   \n",
      "4             NaN       ...                 0.244110           0.999999   \n",
      "5             NaN       ...                 0.268920           0.999999   \n",
      "6             NaN       ...                 0.268551           0.999999   \n",
      "7             NaN       ...                 0.268896           0.999999   \n",
      "0             NaN       ...                 0.418737           0.439281   \n",
      "1             NaN       ...                 0.418737           0.439281   \n",
      "2             NaN       ...                 0.392938           0.427893   \n",
      "3             NaN       ...                 0.392938           0.427893   \n",
      "0             NaN       ...                 0.393950           0.399648   \n",
      "1             NaN       ...                 0.435842           0.437994   \n",
      "2             NaN       ...                 0.407698           0.426982   \n",
      "3             NaN       ...                 0.407698           0.426982   \n",
      "\n",
      "   split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0           0.437757           0.918215          0.559186           0.919755   \n",
      "1           0.405933           0.925264          0.576696           0.903296   \n",
      "2           0.452861           0.933157          0.593257           0.907558   \n",
      "3           0.432066           0.937133          0.599064           0.915997   \n",
      "4           0.397995           0.919887          0.579476           0.913330   \n",
      "5           0.450539           0.935124          0.570797           0.913528   \n",
      "6           0.425251           0.936006          0.583939           0.917249   \n",
      "7           0.390728           0.934790          0.616149           0.920845   \n",
      "0           0.395423           0.999976          0.558876           0.999767   \n",
      "1           0.389036           0.999976          0.555985           0.999767   \n",
      "2           0.397995           0.999976          0.575444           0.999767   \n",
      "3           0.421707           0.999976          0.605744           0.999767   \n",
      "4           0.358684           0.999976          0.581883           0.999767   \n",
      "5           0.448363           0.999976          0.627892           0.999767   \n",
      "6           0.411515           0.999976          0.625488           0.999767   \n",
      "7           0.392534           0.999976          0.606243           0.999767   \n",
      "0           0.293992           0.567092          0.524410           0.452518   \n",
      "1           0.293992           0.567092          0.524410           0.452518   \n",
      "2           0.307132           0.556049          0.523140           0.448383   \n",
      "3           0.307132           0.556049          0.523140           0.448383   \n",
      "0           0.316059           0.465484          0.411636           0.381077   \n",
      "1           0.294810           0.567061          0.527691           0.452229   \n",
      "2           0.308503           0.555975          0.527068           0.448002   \n",
      "3           0.308503           0.555975          0.527068           0.448002   \n",
      "0          -0.032746           0.000000         -0.025697           0.000000   \n",
      "1           0.234169           0.345164          0.293167           0.282289   \n",
      "2           0.233787           0.346858          0.298829           0.290443   \n",
      "3           0.233787           0.346858          0.298829           0.290443   \n",
      "4          -0.032746           0.000000         -0.025697           0.000000   \n",
      "5           0.178140           0.257960          0.203580           0.199404   \n",
      "..               ...                ...               ...                ...   \n",
      "12          0.240942           0.546829          0.494918           0.441416   \n",
      "13          0.266815           0.536029          0.513159           0.446267   \n",
      "14          0.262770           0.544459          0.514721           0.446006   \n",
      "15          0.271270           0.501898          0.527307           0.447091   \n",
      "16          0.273006           0.549342          0.480239           0.433841   \n",
      "17          0.280917           0.546973          0.523099           0.440983   \n",
      "0           0.363762           0.892187          0.546082           0.881554   \n",
      "1           0.343352           0.912603          0.593565           0.898954   \n",
      "2           0.420893           0.919988          0.515562           0.906785   \n",
      "3           0.414799           0.917573          0.575484           0.906665   \n",
      "4           0.398372           0.900096          0.506894           0.896812   \n",
      "5           0.459015           0.913909          0.553828           0.905039   \n",
      "6           0.438007           0.923851          0.518395           0.917418   \n",
      "7           0.419239           0.922288          0.565382           0.917897   \n",
      "0           0.461337           0.999976          0.512436           0.999999   \n",
      "1           0.373238           0.999976          0.511713           0.999999   \n",
      "2           0.432841           0.999976          0.485867           0.999999   \n",
      "3           0.426488           0.999976          0.530695           0.999999   \n",
      "4           0.362474           0.999976          0.503912           0.999999   \n",
      "5           0.397706           0.999976          0.559226           0.999999   \n",
      "6           0.419472           0.999976          0.563246           0.999999   \n",
      "7           0.420156           0.999976          0.551619           0.999999   \n",
      "0           0.271300           0.539604          0.457032           0.436078   \n",
      "1           0.271300           0.539604          0.457032           0.436078   \n",
      "2           0.259509           0.521040          0.433714           0.426155   \n",
      "3           0.259509           0.521040          0.433714           0.426155   \n",
      "0           0.340762           0.493042          0.424091           0.413312   \n",
      "1           0.271268           0.539577          0.461994           0.435878   \n",
      "2           0.260536           0.521033          0.440306           0.425817   \n",
      "3           0.260536           0.521033          0.440306           0.425817   \n",
      "\n",
      "    std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0       0.000108    1.590706e-04        0.066246         0.008729  \n",
      "1       0.003123    1.511005e-03        0.069603         0.009930  \n",
      "2       0.003846    7.039070e-04        0.063013         0.011223  \n",
      "3       0.000581    2.888158e-04        0.076261         0.008637  \n",
      "4       0.001840    3.442162e-04        0.077556         0.010577  \n",
      "5       0.001926    1.717574e-04        0.058292         0.008858  \n",
      "6       0.000627    4.681239e-04        0.065445         0.007848  \n",
      "7       0.008962    3.297199e-04        0.091917         0.005764  \n",
      "0       0.000651    1.492319e-04        0.113255         0.000104  \n",
      "1       0.001725    1.890016e-03        0.098767         0.000104  \n",
      "2       0.003258    7.548232e-04        0.110435         0.000104  \n",
      "3       0.007825    2.299589e-03        0.118016         0.000104  \n",
      "4       0.002929    9.104882e-04        0.111307         0.000104  \n",
      "5       0.002699    8.946138e-05        0.123283         0.000104  \n",
      "6       0.008569    1.126224e-04        0.128135         0.000104  \n",
      "7       0.002479    1.238335e-03        0.124209         0.000104  \n",
      "0       0.001449    7.148106e-05        0.099298         0.051562  \n",
      "1       0.000012    5.857317e-06        0.099298         0.051562  \n",
      "2       0.000006    0.000000e+00        0.092380         0.047554  \n",
      "3       0.000009    9.602742e-07        0.092380         0.047554  \n",
      "0       0.000077    7.636841e-06        0.039770         0.039394  \n",
      "1       0.000004    5.150430e-07        0.100478         0.051662  \n",
      "2       0.000002    7.786718e-07        0.093522         0.047660  \n",
      "3       0.000006    7.786718e-07        0.093522         0.047660  \n",
      "0       0.000057    6.575629e-06        0.052024         0.000000  \n",
      "1       0.000002    1.189441e-06        0.024263         0.027481  \n",
      "2       0.000005    4.052337e-07        0.039295         0.023991  \n",
      "3       0.000004    4.052337e-07        0.039295         0.023991  \n",
      "4       0.000061    2.051724e-05        0.052024         0.000000  \n",
      "5       0.000135    1.253002e-05        0.032248         0.026496  \n",
      "..           ...             ...             ...              ...  \n",
      "12      0.000040    5.170013e-06        0.115103         0.048746  \n",
      "13      0.000007    7.786718e-07        0.105809         0.042775  \n",
      "14      0.000065    4.495664e-07        0.104779         0.049296  \n",
      "15      0.000479    9.876792e-05        0.105803         0.031374  \n",
      "16      0.000695    5.045031e-05        0.084470         0.062392  \n",
      "17      0.000260    3.228927e-05        0.105035         0.049667  \n",
      "0       0.000329    1.021421e-03        0.122025         0.008053  \n",
      "1       0.005248    6.018053e-05        0.129689         0.011437  \n",
      "2       0.004745    3.540041e-04        0.078903         0.009297  \n",
      "3       0.000828    1.949261e-03        0.096307         0.006207  \n",
      "4       0.001086    8.092499e-05        0.056278         0.001856  \n",
      "5       0.002834    2.007287e-04        0.076721         0.004026  \n",
      "6       0.001478    2.855920e-04        0.065144         0.002739  \n",
      "7       0.006004    7.027910e-04        0.089868         0.006578  \n",
      "0       0.001078    9.592259e-05        0.092653         0.000011  \n",
      "1       0.002636    1.307711e-03        0.095709         0.000011  \n",
      "2       0.002295    3.706593e-04        0.095495         0.000011  \n",
      "3       0.003828    1.655535e-03        0.096765         0.000011  \n",
      "4       0.003500    8.892264e-05        0.106280         0.000011  \n",
      "5       0.006008    7.830758e-04        0.118848         0.000011  \n",
      "6       0.016258    5.475426e-04        0.120432         0.000011  \n",
      "7       0.011855    1.834935e-03        0.115632         0.000011  \n",
      "0       0.000096    9.224331e-06        0.079956         0.048065  \n",
      "1       0.000007    1.655632e-06        0.079956         0.048065  \n",
      "2       0.000013    2.050954e-06        0.074286         0.044325  \n",
      "3       0.000001    8.778064e-07        0.074286         0.044325  \n",
      "0       0.000218    3.909112e-05        0.034391         0.041185  \n",
      "1       0.000004    4.495664e-07        0.084317         0.048393  \n",
      "2       0.000040    1.225068e-05        0.078094         0.044613  \n",
      "3       0.000027    1.368365e-05        0.078094         0.044613  \n",
      "\n",
      "[930 rows x 27 columns]\n",
      "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0        0.024165         0.004869         0.467161          0.912829   \n",
      "1        0.063295         0.013676         0.492309          0.917294   \n",
      "2        0.097568         0.019110         0.504288          0.923250   \n",
      "3        0.135895         0.025160         0.491100          0.926304   \n",
      "4        0.052104         0.005416         0.472136          0.909373   \n",
      "5        0.133042         0.012063         0.488230          0.924929   \n",
      "6        0.213503         0.019280         0.497451          0.927842   \n",
      "7        0.297030         0.025394         0.501210          0.928453   \n",
      "0        0.022245         0.004949         0.412033          0.999914   \n",
      "1        0.055298         0.013039         0.421612          0.999914   \n",
      "2        0.090962         0.019441         0.427383          0.999914   \n",
      "3        0.123162         0.027238         0.448949          0.999914   \n",
      "4        0.053856         0.006441         0.424675          0.999914   \n",
      "5        0.131324         0.011887         0.467587          0.999914   \n",
      "6        0.210276         0.018615         0.451756          0.999914   \n",
      "7        0.283644         0.029835         0.436431          0.999914   \n",
      "0        0.001577         0.000264         0.432161          0.494461   \n",
      "1        0.000414         0.000186         0.432161          0.494461   \n",
      "2        0.000347         0.000180         0.435058          0.489384   \n",
      "3        0.000346         0.000180         0.435058          0.489384   \n",
      "0        0.000483         0.000179         0.358135          0.409781   \n",
      "1        0.000383         0.000173         0.434713          0.494294   \n",
      "2        0.000341         0.000176         0.438055          0.489173   \n",
      "3        0.000336         0.000175         0.438055          0.489173   \n",
      "0        0.000454         0.000188        -0.066149          0.000000   \n",
      "1        0.000361         0.000185         0.261347          0.306788   \n",
      "2        0.000422         0.000187         0.286931          0.313899   \n",
      "3        0.000423         0.000188         0.286931          0.313899   \n",
      "4        0.000453         0.000199        -0.066149          0.000000   \n",
      "5        0.000574         0.000238         0.169152          0.220602   \n",
      "..            ...              ...              ...               ...   \n",
      "10       0.000359         0.000194         0.348454          0.368449   \n",
      "11       0.000316         0.000187         0.348454          0.368449   \n",
      "12       0.000415         0.000187        -0.066149          0.000000   \n",
      "13       0.000352         0.000185         0.359648          0.390221   \n",
      "14       0.000308         0.000185         0.348388          0.368395   \n",
      "15       0.000302         0.000185         0.348388          0.368395   \n",
      "16       0.000462         0.000196        -0.066149          0.000000   \n",
      "17       0.000373         0.000189         0.359034          0.389770   \n",
      "18       0.000339         0.000197         0.348306          0.368325   \n",
      "19       0.000399         0.000205         0.348306          0.368325   \n",
      "20       0.000472         0.000185        -0.066149          0.000000   \n",
      "21       0.000371         0.000189         0.358313          0.389219   \n",
      "22       0.000312         0.000186         0.348208          0.368240   \n",
      "23       0.000329         0.000186         0.348208          0.368240   \n",
      "24       0.000385         0.000182        -0.066149          0.000000   \n",
      "25       0.000358         0.000183         0.357486          0.388567   \n",
      "26       0.000338         0.000194         0.348095          0.368140   \n",
      "27       0.000305         0.000186         0.348095          0.368140   \n",
      "28       0.000406         0.000183        -0.066149          0.000000   \n",
      "29       0.000361         0.000184         0.356553          0.387815   \n",
      "30       0.000304         0.000184         0.347966          0.368025   \n",
      "31       0.000443         0.000201         0.347966          0.368025   \n",
      "32       0.000425         0.000186        -0.066149          0.000000   \n",
      "33       0.000350         0.000183         0.355513          0.386963   \n",
      "34       0.000310         0.000186         0.347821          0.367894   \n",
      "35       0.000310         0.000185         0.347821          0.367894   \n",
      "36       0.000398         0.000191        -0.066149          0.000000   \n",
      "37       0.000419         0.000189         0.354367          0.386011   \n",
      "38       0.000333         0.000191         0.347660          0.367747   \n",
      "39       0.000313         0.000185         0.347660          0.367747   \n",
      "\n",
      "   method_ids param_C param_alpha param_criterion param_fit_intercept  \\\n",
      "0   [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "1   [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "2   [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "3   [0, 0, 1]     NaN         NaN             mse                 NaN   \n",
      "4   [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "5   [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "6   [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "7   [0, 0, 1]     NaN         NaN             mae                 NaN   \n",
      "0   [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "1   [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "2   [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "3   [0, 0, 2]     NaN         NaN             mse                 NaN   \n",
      "4   [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "5   [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "6   [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "7   [0, 0, 2]     NaN         NaN             mae                 NaN   \n",
      "0   [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "1   [0, 0, 3]     NaN         NaN             NaN                True   \n",
      "2   [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "3   [0, 0, 3]     NaN         NaN             NaN               False   \n",
      "0   [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "1   [0, 0, 4]     NaN         NaN             NaN                True   \n",
      "2   [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "3   [0, 0, 4]     NaN         NaN             NaN               False   \n",
      "0   [0, 0, 6]     NaN           1             NaN                True   \n",
      "1   [0, 0, 6]     NaN           1             NaN                True   \n",
      "2   [0, 0, 6]     NaN           1             NaN               False   \n",
      "3   [0, 0, 6]     NaN           1             NaN               False   \n",
      "4   [0, 0, 6]     NaN           2             NaN                True   \n",
      "5   [0, 0, 6]     NaN           2             NaN                True   \n",
      "..        ...     ...         ...             ...                 ...   \n",
      "10  [0, 1, 6]     NaN           3             NaN               False   \n",
      "11  [0, 1, 6]     NaN           3             NaN               False   \n",
      "12  [0, 1, 6]     NaN           4             NaN                True   \n",
      "13  [0, 1, 6]     NaN           4             NaN                True   \n",
      "14  [0, 1, 6]     NaN           4             NaN               False   \n",
      "15  [0, 1, 6]     NaN           4             NaN               False   \n",
      "16  [0, 1, 6]     NaN           5             NaN                True   \n",
      "17  [0, 1, 6]     NaN           5             NaN                True   \n",
      "18  [0, 1, 6]     NaN           5             NaN               False   \n",
      "19  [0, 1, 6]     NaN           5             NaN               False   \n",
      "20  [0, 1, 6]     NaN           6             NaN                True   \n",
      "21  [0, 1, 6]     NaN           6             NaN                True   \n",
      "22  [0, 1, 6]     NaN           6             NaN               False   \n",
      "23  [0, 1, 6]     NaN           6             NaN               False   \n",
      "24  [0, 1, 6]     NaN           7             NaN                True   \n",
      "25  [0, 1, 6]     NaN           7             NaN                True   \n",
      "26  [0, 1, 6]     NaN           7             NaN               False   \n",
      "27  [0, 1, 6]     NaN           7             NaN               False   \n",
      "28  [0, 1, 6]     NaN           8             NaN                True   \n",
      "29  [0, 1, 6]     NaN           8             NaN                True   \n",
      "30  [0, 1, 6]     NaN           8             NaN               False   \n",
      "31  [0, 1, 6]     NaN           8             NaN               False   \n",
      "32  [0, 1, 6]     NaN           9             NaN                True   \n",
      "33  [0, 1, 6]     NaN           9             NaN                True   \n",
      "34  [0, 1, 6]     NaN           9             NaN               False   \n",
      "35  [0, 1, 6]     NaN           9             NaN               False   \n",
      "36  [0, 1, 6]     NaN          10             NaN                True   \n",
      "37  [0, 1, 6]     NaN          10             NaN                True   \n",
      "38  [0, 1, 6]     NaN          10             NaN               False   \n",
      "39  [0, 1, 6]     NaN          10             NaN               False   \n",
      "\n",
      "   param_l1_ratio       ...        split0_test_score split0_train_score  \\\n",
      "0             NaN       ...                 0.405206           0.900516   \n",
      "1             NaN       ...                 0.494276           0.923324   \n",
      "2             NaN       ...                 0.467146           0.929035   \n",
      "3             NaN       ...                 0.442689           0.925783   \n",
      "4             NaN       ...                 0.439291           0.894901   \n",
      "5             NaN       ...                 0.443830           0.926134   \n",
      "6             NaN       ...                 0.483314           0.930272   \n",
      "7             NaN       ...                 0.496800           0.929725   \n",
      "0             NaN       ...                 0.283188           0.999999   \n",
      "1             NaN       ...                 0.320898           0.999999   \n",
      "2             NaN       ...                 0.309973           0.999999   \n",
      "3             NaN       ...                 0.320773           0.999999   \n",
      "4             NaN       ...                 0.334430           0.999999   \n",
      "5             NaN       ...                 0.328006           0.999999   \n",
      "6             NaN       ...                 0.319685           0.999999   \n",
      "7             NaN       ...                 0.311854           0.999999   \n",
      "0             NaN       ...                 0.477591           0.463774   \n",
      "1             NaN       ...                 0.477591           0.463774   \n",
      "2             NaN       ...                 0.474478           0.463719   \n",
      "3             NaN       ...                 0.474478           0.463719   \n",
      "0             NaN       ...                 0.346831           0.382781   \n",
      "1             NaN       ...                 0.481139           0.463593   \n",
      "2             NaN       ...                 0.478163           0.463543   \n",
      "3             NaN       ...                 0.478163           0.463543   \n",
      "0             NaN       ...                -0.139219           0.000000   \n",
      "1             NaN       ...                 0.256755           0.292910   \n",
      "2             NaN       ...                 0.327736           0.304397   \n",
      "3             NaN       ...                 0.327736           0.304397   \n",
      "4             NaN       ...                -0.139219           0.000000   \n",
      "5             NaN       ...                 0.126198           0.204443   \n",
      "..            ...       ...                      ...                ...   \n",
      "10            NaN       ...                 0.360449           0.349717   \n",
      "11            NaN       ...                 0.360449           0.349717   \n",
      "12            NaN       ...                -0.139219           0.000000   \n",
      "13            NaN       ...                 0.392216           0.354863   \n",
      "14            NaN       ...                 0.361155           0.349670   \n",
      "15            NaN       ...                 0.361155           0.349670   \n",
      "16            NaN       ...                -0.139219           0.000000   \n",
      "17            NaN       ...                 0.389534           0.354472   \n",
      "18            NaN       ...                 0.361848           0.349609   \n",
      "19            NaN       ...                 0.361848           0.349609   \n",
      "20            NaN       ...                -0.139219           0.000000   \n",
      "21            NaN       ...                 0.386770           0.353993   \n",
      "22            NaN       ...                 0.362528           0.349535   \n",
      "23            NaN       ...                 0.362528           0.349535   \n",
      "24            NaN       ...                -0.139219           0.000000   \n",
      "25            NaN       ...                 0.383925           0.353428   \n",
      "26            NaN       ...                 0.363197           0.349447   \n",
      "27            NaN       ...                 0.363197           0.349447   \n",
      "28            NaN       ...                -0.139219           0.000000   \n",
      "29            NaN       ...                 0.380999           0.352775   \n",
      "30            NaN       ...                 0.363853           0.349345   \n",
      "31            NaN       ...                 0.363853           0.349345   \n",
      "32            NaN       ...                -0.139219           0.000000   \n",
      "33            NaN       ...                 0.377991           0.352036   \n",
      "34            NaN       ...                 0.364496           0.349230   \n",
      "35            NaN       ...                 0.364496           0.349230   \n",
      "36            NaN       ...                -0.139219           0.000000   \n",
      "37            NaN       ...                 0.374903           0.351209   \n",
      "38            NaN       ...                 0.365128           0.349102   \n",
      "39            NaN       ...                 0.365128           0.349102   \n",
      "\n",
      "   split1_test_score split1_train_score split2_test_score split2_train_score  \\\n",
      "0           0.437757           0.918215          0.559186           0.919755   \n",
      "1           0.405933           0.925264          0.576696           0.903296   \n",
      "2           0.452861           0.933157          0.593257           0.907558   \n",
      "3           0.432066           0.937133          0.599064           0.915997   \n",
      "4           0.397995           0.919887          0.579476           0.913330   \n",
      "5           0.450539           0.935124          0.570797           0.913528   \n",
      "6           0.425251           0.936006          0.583939           0.917249   \n",
      "7           0.390728           0.934790          0.616149           0.920845   \n",
      "0           0.395423           0.999976          0.558876           0.999767   \n",
      "1           0.389036           0.999976          0.555985           0.999767   \n",
      "2           0.397995           0.999976          0.575444           0.999767   \n",
      "3           0.421707           0.999976          0.605744           0.999767   \n",
      "4           0.358684           0.999976          0.581883           0.999767   \n",
      "5           0.448363           0.999976          0.627892           0.999767   \n",
      "6           0.411515           0.999976          0.625488           0.999767   \n",
      "7           0.392534           0.999976          0.606243           0.999767   \n",
      "0           0.293992           0.567092          0.524410           0.452518   \n",
      "1           0.293992           0.567092          0.524410           0.452518   \n",
      "2           0.307132           0.556049          0.523140           0.448383   \n",
      "3           0.307132           0.556049          0.523140           0.448383   \n",
      "0           0.316059           0.465484          0.411636           0.381077   \n",
      "1           0.294810           0.567061          0.527691           0.452229   \n",
      "2           0.308503           0.555975          0.527068           0.448002   \n",
      "3           0.308503           0.555975          0.527068           0.448002   \n",
      "0          -0.032746           0.000000         -0.025697           0.000000   \n",
      "1           0.234169           0.345164          0.293167           0.282289   \n",
      "2           0.233787           0.346858          0.298829           0.290443   \n",
      "3           0.233787           0.346858          0.298829           0.290443   \n",
      "4          -0.032746           0.000000         -0.025697           0.000000   \n",
      "5           0.178140           0.257960          0.203580           0.199404   \n",
      "..               ...                ...               ...                ...   \n",
      "10          0.303504           0.399004          0.381279           0.356625   \n",
      "11          0.303504           0.399004          0.381279           0.356625   \n",
      "12         -0.032746           0.000000         -0.025697           0.000000   \n",
      "13          0.288807           0.436937          0.397569           0.378863   \n",
      "14          0.303296           0.398946          0.380576           0.356568   \n",
      "15          0.303296           0.398946          0.380576           0.356568   \n",
      "16         -0.032746           0.000000         -0.025697           0.000000   \n",
      "17          0.290459           0.436451          0.396780           0.378387   \n",
      "18          0.303068           0.398871          0.379856           0.356496   \n",
      "19          0.303068           0.398871          0.379856           0.356496   \n",
      "20         -0.032746           0.000000         -0.025697           0.000000   \n",
      "21          0.291984           0.435858          0.395880           0.377805   \n",
      "22          0.302823           0.398780          0.379120           0.356407   \n",
      "23          0.302823           0.398780          0.379120           0.356407   \n",
      "24         -0.032746           0.000000         -0.025697           0.000000   \n",
      "25          0.293382           0.435156          0.394867           0.377118   \n",
      "26          0.302558           0.398673          0.378367           0.356301   \n",
      "27          0.302558           0.398673          0.378367           0.356301   \n",
      "28         -0.032746           0.000000         -0.025697           0.000000   \n",
      "29          0.294653           0.434346          0.393743           0.376324   \n",
      "30          0.302275           0.398549          0.377598           0.356180   \n",
      "31          0.302275           0.398549          0.377598           0.356180   \n",
      "32         -0.032746           0.000000         -0.025697           0.000000   \n",
      "33          0.295798           0.433429          0.392507           0.375425   \n",
      "34          0.301973           0.398408          0.376813           0.356043   \n",
      "35          0.301973           0.398408          0.376813           0.356043   \n",
      "36         -0.032746           0.000000         -0.025697           0.000000   \n",
      "37          0.296816           0.432403          0.391160           0.374420   \n",
      "38          0.301653           0.398251          0.376011           0.355889   \n",
      "39          0.301653           0.398251          0.376011           0.355889   \n",
      "\n",
      "    std_fit_time  std_score_time  std_test_score  std_train_score  \n",
      "0   1.081822e-04    1.590706e-04        0.066246         0.008729  \n",
      "1   3.123020e-03    1.511005e-03        0.069603         0.009930  \n",
      "2   3.845929e-03    7.039070e-04        0.063013         0.011223  \n",
      "3   5.805324e-04    2.888158e-04        0.076261         0.008637  \n",
      "4   1.840379e-03    3.442162e-04        0.077556         0.010577  \n",
      "5   1.925634e-03    1.717574e-04        0.058292         0.008858  \n",
      "6   6.273349e-04    4.681239e-04        0.065445         0.007848  \n",
      "7   8.961997e-03    3.297199e-04        0.091917         0.005764  \n",
      "0   6.513482e-04    1.492319e-04        0.113255         0.000104  \n",
      "1   1.725471e-03    1.890016e-03        0.098767         0.000104  \n",
      "2   3.258254e-03    7.548232e-04        0.110435         0.000104  \n",
      "3   7.824721e-03    2.299589e-03        0.118016         0.000104  \n",
      "4   2.928583e-03    9.104882e-04        0.111307         0.000104  \n",
      "5   2.698708e-03    8.946138e-05        0.123283         0.000104  \n",
      "6   8.568505e-03    1.126224e-04        0.128135         0.000104  \n",
      "7   2.479396e-03    1.238335e-03        0.124209         0.000104  \n",
      "0   1.449068e-03    7.148106e-05        0.099298         0.051562  \n",
      "1   1.248154e-05    5.857317e-06        0.099298         0.051562  \n",
      "2   6.344901e-06    0.000000e+00        0.092380         0.047554  \n",
      "3   8.878936e-06    9.602742e-07        0.092380         0.047554  \n",
      "0   7.674025e-05    7.636841e-06        0.039770         0.039394  \n",
      "1   4.103448e-06    5.150430e-07        0.100478         0.051662  \n",
      "2   2.063235e-06    7.786718e-07        0.093522         0.047660  \n",
      "3   5.629686e-06    7.786718e-07        0.093522         0.047660  \n",
      "0   5.744530e-05    6.575629e-06        0.052024         0.000000  \n",
      "1   2.153113e-06    1.189441e-06        0.024263         0.027481  \n",
      "2   5.081294e-06    4.052337e-07        0.039295         0.023991  \n",
      "3   3.552357e-06    4.052337e-07        0.039295         0.023991  \n",
      "4   6.122516e-05    2.051724e-05        0.052024         0.000000  \n",
      "5   1.353791e-04    1.253002e-05        0.032248         0.026496  \n",
      "..           ...             ...             ...              ...  \n",
      "10  3.875327e-05    7.078886e-06        0.032822         0.021789  \n",
      "11  5.170013e-06    5.619580e-07        0.032822         0.021789  \n",
      "12  3.259763e-05    3.821314e-06        0.052024         0.000000  \n",
      "13  1.590371e-05    1.808772e-06        0.050005         0.034456  \n",
      "14  9.873204e-06    8.991328e-07        0.032775         0.021786  \n",
      "15  8.991328e-07    0.000000e+00        0.032775         0.021786  \n",
      "16  5.235313e-05    7.826362e-06        0.052024         0.000000  \n",
      "17  1.652156e-05    2.485354e-06        0.048450         0.034422  \n",
      "18  6.695559e-06    1.857014e-06        0.032740         0.021781  \n",
      "19  6.496225e-05    9.934425e-06        0.032740         0.021781  \n",
      "20  9.146577e-05    2.614162e-06        0.052024         0.000000  \n",
      "21  1.500459e-05    5.731971e-06        0.046924         0.034382  \n",
      "22  1.077964e-05    7.786718e-07        0.032717         0.021776  \n",
      "23  2.713669e-05    1.296163e-06        0.032717         0.021776  \n",
      "24  1.620935e-06    4.495664e-07        0.052024         0.000000  \n",
      "25  1.083865e-06    9.602742e-07        0.045428         0.034334  \n",
      "26  2.696157e-05    1.085321e-05        0.032706         0.021770  \n",
      "27  6.575629e-06    1.910657e-06        0.032706         0.021770  \n",
      "28  2.728293e-05    1.215701e-06        0.052024         0.000000  \n",
      "29  2.872941e-05    1.839930e-06        0.043962         0.034278  \n",
      "30  4.960533e-06    1.655632e-06        0.032707         0.021763  \n",
      "31  1.823748e-04    1.913135e-05        0.032707         0.021763  \n",
      "32  1.472330e-05    1.520405e-06        0.052024         0.000000  \n",
      "33  1.276523e-06    8.991328e-07        0.042527         0.034216  \n",
      "34  2.867638e-06    2.170642e-06        0.032721         0.021755  \n",
      "35  7.642629e-06    8.991328e-07        0.032721         0.021755  \n",
      "36  2.141259e-05    1.373387e-05        0.052024         0.000000  \n",
      "37  3.295489e-05    4.960533e-06        0.041125         0.034146  \n",
      "38  1.296163e-06    6.590979e-06        0.032748         0.021746  \n",
      "39  6.439754e-06    2.153113e-06        0.032748         0.021746  \n",
      "\n",
      "[970 rows x 27 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./modules/run_grid.py:89: UserWarning: The current method (id: [0, 1, 7]) has failed...\n",
      "  warnings.warn(str('The current method (id: ' + str(method_id) + ') has failed...'))\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.318e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.750e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.352e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.863e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.386e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.064e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.813e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.406e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.928e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.941e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.730e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.365e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.806e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.550e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.275e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.732e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.321e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.909e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.255e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.297e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.649e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.164e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.529e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.952e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.181e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.984e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.962e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.717e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.324e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.917e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.197e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.754e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.023e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.981e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.277e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.701e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.399e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.397e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.556e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.225e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.031e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.570e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.421e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.326e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.603e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.949e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.933e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.596e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.218e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.546e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=3.348e-02, previous alpha=8.984e-03, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.999e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.255e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.105e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.819e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.217e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.723e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.547e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.735e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.891e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.767e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.653e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.243e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.742e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.709e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.697e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.011e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.005e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.004e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.210e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.638e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.191e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.977e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.258e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=2.190e-02, previous alpha=2.094e-02, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.038e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.255e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.098e-01, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.771e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.146e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.451e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.131e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.304e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.314e-01, previous alpha=1.314e-01, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.610e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.426e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.679e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.168e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "./modules/run_grid.py:89: UserWarning: The current method (id: [0, 1, 8]) has failed...\n",
      "  warnings.warn(str('The current method (id: ' + str(method_id) + ') has failed...'))\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.179e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.508e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.718e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.974e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.397e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.083e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.370e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.685e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.258e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.752e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/res3/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.544e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "./modules/run_grid.py:89: UserWarning: The current method (id: [0, 1, 10]) has failed...\n",
      "  warnings.warn(str('The current method (id: ' + str(method_id) + ') has failed...'))\n"
     ]
    }
   ],
   "source": [
    "X,y,labels = proj.set_input('./input_files/rdkit_descriptors.csv')\n",
    "results = rg.auto_grid(X, y, labels)\n",
    "proj.save_eval('./results_test',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
